From d69504bae22e23771b7e197b24864886280cf4d0 Mon Sep 17 00:00:00 2001
From: Udit kumar agarwal <dev.madaari@gmail.com>
Date: Tue, 3 Jul 2018 14:37:42 +0530
Subject: [PATCH] Initial MMCCAM port for RTEMS

- Currently driver is able to to Detect and initialize all sort of SD/MMC/SDIO cards. It's able to read and modify registers.
- However, several bugs exist in the part responsible for mounting the partitions using RTEMS media server.
- More details on this can be found here:http://uditagarwal.in/index.php/2018/08/01/rtems-sdio-driver-current-progress/
- This patch has been tested with rtems-libbsd commit id: commit id: 137250239e9e0b244eb924928e8d1ec7996dcfef
- For details on how to use this patch, visit here: http://uditagarwal.in/index.php/2018/08/03/gsoc-2018-final-report/#use_sdio_code
---
 buildset/default-mmccam.ini                        |   13 +
 freebsd-org                                        |    2 +-
 freebsd/sys/arm/include/md_var.h                   |   76 +
 freebsd/sys/arm/ti/ti_sdhci.c                      |   29 +-
 freebsd/sys/cam/cam.h                              |    2 +-
 freebsd/sys/cam/cam_ccb.h                          |   87 +-
 freebsd/sys/cam/cam_compat.c                       |  426 ++
 freebsd/sys/cam/cam_compat.h                       |  223 +
 freebsd/sys/cam/cam_debug.h                        |    2 +-
 freebsd/sys/cam/cam_periph.c                       | 2018 +++++++
 freebsd/sys/cam/cam_periph.h                       |    2 +-
 freebsd/sys/cam/cam_queue.c                        |  399 ++
 freebsd/sys/cam/cam_queue.h                        |  291 ++
 freebsd/sys/cam/cam_sim.h                          |   10 +-
 freebsd/sys/cam/cam_xpt.c                          | 5491 ++++++++++++++++++++
 freebsd/sys/cam/cam_xpt.h                          |   13 +
 freebsd/sys/cam/cam_xpt_sim.h                      |    4 -
 freebsd/sys/cam/mmc/mmc.h                          |  106 +
 freebsd/sys/cam/mmc/mmc_all.h                      |   70 +
 freebsd/sys/cam/mmc/mmc_bus.h                      |    5 +
 freebsd/sys/cam/mmc/mmc_da.c                       | 2187 ++++++++
 freebsd/sys/cam/mmc/mmc_xpt.c                      | 1110 ++++
 freebsd/sys/cam/scsi/scsi_all.c                    |    2 +-
 freebsd/sys/dev/mmc/bridge.h                       |    2 +
 freebsd/sys/dev/mmc/mmcbrvar.h                     |    1 -
 freebsd/sys/dev/mmc/mmcreg.h                       |  102 +
 freebsd/sys/dev/sdhci/fsl_sdhci.c                  |  998 ++++
 freebsd/sys/dev/sdhci/sdhci.c                      |  566 +-
 freebsd/sys/dev/sdhci/sdhci.h                      |   13 +
 freebsd/sys/sys/devicestat.h                       |  206 +
 libbsd.py                                          |   45 +
 rtemsbsd/include/bsp/nexus-devices.h               |    6 +-
 rtemsbsd/include/cam/cam_queue.h                   |  294 +-
 rtemsbsd/include/cam/cam_xpt_internal.h            |  212 +-
 rtemsbsd/include/cam/cam_xpt_periph.h              |  260 +-
 .../include/machine/rtems-bsd-kernel-namespace.h   |    3 -
 rtemsbsd/include/rtems/bsd/local/opt_cam.h         |    6 +
 rtemsbsd/include/rtems/bsd/local/opt_mmccam.h      |    4 +
 rtemsbsd/rtems/rtems-kernel-cam.c                  |  119 +-
 39 files changed, 15347 insertions(+), 58 deletions(-)
 create mode 100644 buildset/default-mmccam.ini
 create mode 100644 freebsd/sys/arm/include/md_var.h
 create mode 100644 freebsd/sys/cam/cam_compat.c
 create mode 100644 freebsd/sys/cam/cam_compat.h
 create mode 100644 freebsd/sys/cam/cam_periph.c
 create mode 100644 freebsd/sys/cam/cam_queue.c
 create mode 100644 freebsd/sys/cam/cam_queue.h
 create mode 100644 freebsd/sys/cam/cam_xpt.c
 create mode 100644 freebsd/sys/cam/mmc/mmc.h
 create mode 100644 freebsd/sys/cam/mmc/mmc_all.h
 create mode 100644 freebsd/sys/cam/mmc/mmc_bus.h
 create mode 100644 freebsd/sys/cam/mmc/mmc_da.c
 create mode 100644 freebsd/sys/cam/mmc/mmc_xpt.c
 create mode 100644 freebsd/sys/dev/sdhci/fsl_sdhci.c
 create mode 100644 freebsd/sys/sys/devicestat.h
 create mode 100644 rtemsbsd/include/rtems/bsd/local/opt_mmccam.h

diff --git a/buildset/default-mmccam.ini b/buildset/default-mmccam.ini
new file mode 100644
index 0000000..2cdce6a
--- /dev/null
+++ b/buildset/default-mmccam.ini
@@ -0,0 +1,13 @@
+#
+# Default configuration. Contains most features except for some big or slow ones
+# like WiFi or IPSec.
+#
+# At all developers: Please allways add all modules to this file and mark them
+# as explicitly "off" if they are not used.
+#
+
+[general]
+name = default-mmccam
+extends = default.ini
+[modules]
+mmccam = on
diff --git a/freebsd-org b/freebsd-org
index 642b174..f2ecfd4 160000
--- a/freebsd-org
+++ b/freebsd-org
@@ -1 +1 @@
-Subproject commit 642b174daddbd0efd9bb5f242c43f4ab4db6869f
+Subproject commit f2ecfd4517d49677df7d782b83c1b305dabe116a
diff --git a/freebsd/sys/arm/include/md_var.h b/freebsd/sys/arm/include/md_var.h
new file mode 100644
index 0000000..642124d
--- /dev/null
+++ b/freebsd/sys/arm/include/md_var.h
@@ -0,0 +1,76 @@
+/*-
+ * Copyright (c) 1995 Bruce D. Evans.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the name of the author nor the names of contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	from: FreeBSD: src/sys/i386/include/md_var.h,v 1.40 2001/07/12
+ * $FreeBSD$
+ */
+
+#ifndef	_MACHINE_MD_VAR_H_
+#define	_MACHINE_MD_VAR_H_
+
+extern long Maxmem;
+extern char sigcode[];
+extern int szsigcode;
+extern uint32_t *vm_page_dump;
+extern int vm_page_dump_size;
+
+extern int (*_arm_memcpy)(void *, void *, int, int);
+extern int (*_arm_bzero)(void *, int, int);
+
+extern int _min_memcpy_size;
+extern int _min_bzero_size;
+
+#define DST_IS_USER	0x1
+#define SRC_IS_USER	0x2
+#define IS_PHYSICAL	0x4
+
+enum cpu_class {
+	CPU_CLASS_NONE,
+	CPU_CLASS_ARM9TDMI,
+	CPU_CLASS_ARM9ES,
+	CPU_CLASS_ARM9EJS,
+	CPU_CLASS_ARM10E,
+	CPU_CLASS_ARM10EJ,
+	CPU_CLASS_CORTEXA,
+	CPU_CLASS_KRAIT,
+	CPU_CLASS_XSCALE,
+	CPU_CLASS_ARM11J,
+	CPU_CLASS_MARVELL
+};
+extern enum cpu_class cpu_class;
+
+struct dumperinfo;
+extern int busdma_swi_pending;
+void busdma_swi(void);
+void dump_add_page(vm_paddr_t);
+void dump_drop_page(vm_paddr_t);
+int minidumpsys(struct dumperinfo *);
+
+extern uint32_t initial_fpscr;
+
+#endif /* !_MACHINE_MD_VAR_H_ */
diff --git a/freebsd/sys/arm/ti/ti_sdhci.c b/freebsd/sys/arm/ti/ti_sdhci.c
index 94096fd..58d178a 100644
--- a/freebsd/sys/arm/ti/ti_sdhci.c
+++ b/freebsd/sys/arm/ti/ti_sdhci.c
@@ -41,6 +41,8 @@ __FBSDID("$FreeBSD$");
 #include <sys/rman.h>
 #include <sys/sysctl.h>
 #include <sys/taskqueue.h>
+#include <sys/lock.h>
+#include <sys/mutex.h>
 
 #include <machine/bus.h>
 #include <machine/resource.h>
@@ -62,6 +64,8 @@ __FBSDID("$FreeBSD$");
 #include <arm/ti/ti_hwmods.h>
 #include <rtems/bsd/local/gpio_if.h>
 
+#include <rtems/bsd/local/opt_mmccam.h>
+
 struct ti_sdhci_softc {
 	device_t		dev;
 	struct sdhci_fdt_gpio * gpio;
@@ -124,6 +128,11 @@ static struct ofw_compat_data compat_data[] = {
 #define	  MMCHS_SD_CAPA_VS30		  (1 << 25)
 #define	  MMCHS_SD_CAPA_VS33		  (1 << 24)
 
+/* Forward declarations, CAM-relataed */
+// static void ti_sdhci_cam_poll(struct cam_sim *);
+// static void ti_sdhci_cam_action(struct cam_sim *, union ccb *);
+// static int ti_sdhci_cam_settran_settings(struct ti_sdhci_softc *sc, union ccb *);
+
 static inline uint32_t
 ti_mmchs_read_4(struct ti_sdhci_softc *sc, bus_size_t off)
 {
@@ -243,6 +252,22 @@ ti_sdhci_write_1(device_t dev, struct sdhci_slot *slot, bus_size_t off,
 	struct ti_sdhci_softc *sc = device_get_softc(dev);
 	uint32_t val32;
 
+#ifdef MMCCAM
+	uint32_t newval32;
+	if (off == SDHCI_HOST_CONTROL) {
+		val32 = ti_mmchs_read_4(sc, MMCHS_CON);
+		newval32  = val32;
+		if (val & SDHCI_CTRL_8BITBUS) {
+			device_printf(dev, "Custom-enabling 8-bit bus\n");
+			newval32 |= MMCHS_CON_DW8;
+		} else {
+			device_printf(dev, "Custom-disabling 8-bit bus\n");
+			newval32 &= ~MMCHS_CON_DW8;
+		}
+		if (newval32 != val32)
+			ti_mmchs_write_4(sc, MMCHS_CON, newval32);
+	}
+#endif
 	val32 = RD4(sc, off & ~3);
 	val32 &= ~(0xff << (off & 3) * 8);
 	val32 |= (val << (off & 3) * 8);
@@ -661,7 +686,6 @@ ti_sdhci_attach(device_t dev)
 	bus_generic_attach(dev);
 
 	sdhci_start_slot(&sc->slot);
-
 	return (0);
 
 fail:
@@ -732,4 +756,7 @@ static driver_t ti_sdhci_driver = {
 DRIVER_MODULE(sdhci_ti, simplebus, ti_sdhci_driver, ti_sdhci_devclass, NULL,
     NULL);
 MODULE_DEPEND(sdhci_ti, sdhci, 1, 1, 1);
+
+#ifndef MMCCAM
 MMC_DECLARE_BRIDGE(sdhci_ti);
+#endif
diff --git a/freebsd/sys/cam/cam.h b/freebsd/sys/cam/cam.h
index 23feb50..b39c625 100644
--- a/freebsd/sys/cam/cam.h
+++ b/freebsd/sys/cam/cam.h
@@ -33,7 +33,7 @@
 
 #ifdef _KERNEL
 #ifndef __rtems__
-#include <opt_cam.h>
+#include <rtems/bsd/local/opt_cam.h>
 #else /* __rtems__ */
 #include <rtems/bsd/local/opt_cam.h>
 #endif /* __rtems__ */
diff --git a/freebsd/sys/cam/cam_ccb.h b/freebsd/sys/cam/cam_ccb.h
index 99249f4..9f9b0cc 100644
--- a/freebsd/sys/cam/cam_ccb.h
+++ b/freebsd/sys/cam/cam_ccb.h
@@ -42,6 +42,7 @@
 #include <cam/scsi/scsi_all.h>
 #include <cam/ata/ata_all.h>
 #include <cam/nvme/nvme_all.h>
+#include <cam/mmc/mmc_all.h>
 #ifdef __rtems__
 #include <rtems/blkdev.h>
 #endif /* __rtems__ */
@@ -211,10 +212,10 @@ typedef enum {
 	XPT_NVME_IO		= 0x1c | XPT_FC_DEV_QUEUED,
 				/* Execiute the requestred NVMe I/O operation */
 
-	XPT_MMCSD_IO		= 0x1d | XPT_FC_DEV_QUEUED,
+	XPT_MMC_IO		= 0x1d | XPT_FC_DEV_QUEUED,
 				/* Placeholder for MMC / SD / SDIO I/O stuff */
 
-	XPT_SCAN_TGT		= 0x1E | XPT_FC_QUEUED | XPT_FC_USER_CCB
+	XPT_SCAN_TGT		= 0x1e | XPT_FC_QUEUED | XPT_FC_USER_CCB
 				       | XPT_FC_XPT_ONLY,
 				/* Scan Target */
 
@@ -270,6 +271,7 @@ typedef enum {
 	PROTO_SATAPM,	/* SATA Port Multiplier */
 	PROTO_SEMB,	/* SATA Enclosure Management Bridge */
 	PROTO_NVME,	/* NVME */
+	PROTO_MMCSD,	/* MMC, SD, SDIO */
 } cam_proto;
 
 typedef enum {
@@ -286,6 +288,7 @@ typedef enum {
 	XPORT_ISCSI,	/* iSCSI */
 	XPORT_SRP,	/* SCSI RDMA Protocol */
 	XPORT_NVME,	/* NVMe over PCIe */
+	XPORT_MMCSD,	/* MMC, SD, SDIO card */
 } cam_xport;
 
 #define XPORT_IS_NVME(t)	((t) == XPORT_NVME)
@@ -332,36 +335,27 @@ typedef struct {
 } ccb_qos_area;
 
 struct ccb_hdr {
-#ifndef __rtems__
 	cam_pinfo	pinfo;		/* Info for priority scheduling */
 	camq_entry	xpt_links;	/* For chaining in the XPT layer */	
 	camq_entry	sim_links;	/* For chaining in the SIM layer */	
 	camq_entry	periph_links;	/* For chaining in the type driver */
-#else /* __rtems__ */
 	struct cam_sim	*sim;
-#endif /* __rtems__ */
 	u_int32_t	retry_count;
 	void		(*cbfcnp)(struct cam_periph *, union ccb *);
 					/* Callback on completion function */
 	xpt_opcode	func_code;	/* XPT function code */
 	u_int32_t	status;		/* Status returned by CAM subsystem */
-#ifndef __rtems__
 	struct		cam_path *path;	/* Compiled path for this ccb */
 	path_id_t	path_id;	/* Path ID for the request */
-#endif /* __rtems__ */
 	target_id_t	target_id;	/* Target device ID */
 	lun_id_t	target_lun;	/* Target LUN number */
 	u_int32_t	flags;		/* ccb_flags */
 	u_int32_t	xflags;		/* Extended flags */
-#ifndef __rtems__
 	ccb_ppriv_area	periph_priv;
 	ccb_spriv_area	sim_priv;
 	ccb_qos_area	qos;
-#endif /* __rtems__ */
 	u_int32_t	timeout;	/* Hard timeout value in mseconds */
-#ifndef __rtems__
 	struct timeval	softtimeout;	/* Soft timeout value in sec + usec */
-#endif /* __rtems__ */
 };
 
 /* Get Device Information CCB */
@@ -792,6 +786,16 @@ struct ccb_ataio {
 	uint32_t   unused;
 };
 
+/*
+ * MMC I/O Request CCB used for the XPT_MMC_IO function code.
+ */
+struct ccb_mmcio {
+	struct	   ccb_hdr ccb_h;
+	union	   ccb *next_ccb;	/* Ptr for next CCB for action */
+	struct mmc_command cmd;
+        struct mmc_command stop;
+};
+
 struct ccb_accept_tio {
 	struct	   ccb_hdr ccb_h;
 	cdb_t	   cdb_io;		/* Union for CDB bytes/pointer */
@@ -1028,7 +1032,28 @@ struct ccb_trans_settings_nvme
 	u_int		max_xfer;	/* Max transfer size (0 -> unlimited */
 	u_int		caps;
 };
-	
+
+#include <cam/mmc/mmc_bus.h>
+struct ccb_trans_settings_mmc {
+	struct mmc_ios ios;
+#define MMC_CLK		(1 << 1)
+#define MMC_VDD		(1 << 2)
+#define MMC_CS		(1 << 3)
+#define MMC_BW		(1 << 4)
+#define MMC_PM		(1 << 5)
+#define MMC_BT		(1 << 6)
+#define MMC_BM		(1 << 7)
+	uint32_t ios_valid;
+/* The folowing is used only for GET_TRAN_SETTINGS */
+	uint32_t	host_ocr;
+	int host_f_min;
+	int host_f_max;
+#define MMC_CAP_4_BIT_DATA	(1 << 0) /* Can do 4-bit data transfers */
+#define MMC_CAP_8_BIT_DATA	(1 << 1) /* Can do 8-bit data transfers */
+#define MMC_CAP_HSPEED		(1 << 2) /* Can do High Speed transfers */
+	uint32_t host_caps;
+};
+
 /* Get/Set transfer rate/width/disconnection/tag queueing settings */
 struct ccb_trans_settings {
 	struct	  ccb_hdr ccb_h;
@@ -1042,6 +1067,7 @@ struct ccb_trans_settings {
 		struct ccb_trans_settings_ata ata;
 		struct ccb_trans_settings_scsi scsi;
 		struct ccb_trans_settings_nvme nvme;
+		struct ccb_trans_settings_mmc mmc;
 	} proto_specific;
 	union {
 		u_int  valid;	/* Which fields to honor */
@@ -1250,6 +1276,7 @@ struct ccb_dev_advinfo {
 #define	CDAI_TYPE_PHYS_PATH	3
 #define	CDAI_TYPE_RCAPLONG	4
 #define	CDAI_TYPE_EXT_INQ	5
+#define CDAI_TYPE_MMC_PARAMS	8
 	off_t bufsiz;			/* IN: Size of external buffer */
 #define	CAM_SCSI_DEVID_MAXLEN	65536	/* length in buffer is an uint16_t */
 	off_t provsiz;			/* OUT: Size required/used */
@@ -1307,6 +1334,7 @@ union ccb {
 	struct	ccb_dev_advinfo		cdai;
 	struct	ccb_async		casync;
 	struct	ccb_nvmeio		nvmeio;
+	struct	ccb_mmcio		mmcio;
 };
 
 #define CCB_CLEAR_ALL_EXCEPT_HDR(ccbp)			\
@@ -1350,6 +1378,13 @@ cam_fill_smpio(struct ccb_smpio *smpio, uint32_t retries,
 	       uint32_t timeout);
 
 static __inline void
+cam_fill_mmcio(struct ccb_mmcio *mmcio, uint32_t retries,
+	       void (*cbfcnp)(struct cam_periph *, union ccb *), uint32_t flags,
+               uint32_t mmc_opcode, uint32_t mmc_arg, uint32_t mmc_flags,
+	       struct mmc_data *mmc_d,
+	       uint32_t timeout);
+
+static __inline void
 cam_fill_csio(struct ccb_scsiio *csio, u_int32_t retries,
 	      void (*cbfcnp)(struct cam_periph *, union ccb *),
 	      u_int32_t flags, u_int8_t tag_action,
@@ -1438,6 +1473,34 @@ cam_fill_smpio(struct ccb_smpio *smpio, uint32_t retries,
 }
 
 static __inline void
+cam_fill_mmcio(struct ccb_mmcio *mmcio, uint32_t retries,
+	       void (*cbfcnp)(struct cam_periph *, union ccb *), uint32_t flags,
+	       uint32_t mmc_opcode, uint32_t mmc_arg, uint32_t mmc_flags,
+	       struct mmc_data *mmc_d,
+	       uint32_t timeout)
+{
+	mmcio->ccb_h.func_code = XPT_MMC_IO;
+	mmcio->ccb_h.flags = flags;
+	mmcio->ccb_h.retry_count = retries;
+	mmcio->ccb_h.cbfcnp = cbfcnp;
+	mmcio->ccb_h.timeout = timeout;
+	mmcio->cmd.opcode = mmc_opcode;
+	mmcio->cmd.arg = mmc_arg;
+	mmcio->cmd.flags = mmc_flags;
+	mmcio->stop.opcode = 0;
+	mmcio->stop.arg = 0;
+	mmcio->stop.flags = 0;
+	if (mmc_d != NULL) {
+		mmcio->cmd.data = mmc_d;
+	} else
+		mmcio->cmd.data = NULL;
+	mmcio->cmd.resp[0] = 0;
+	mmcio->cmd.resp[1] = 0;
+	mmcio->cmd.resp[2] = 0;
+	mmcio->cmd.resp[3] = 0;
+}
+
+static __inline void
 cam_set_ccbstatus(union ccb *ccb, cam_status status)
 {
 	ccb->ccb_h.status &= ~CAM_STATUS_MASK;
diff --git a/freebsd/sys/cam/cam_compat.c b/freebsd/sys/cam/cam_compat.c
new file mode 100644
index 0000000..7a77631
--- /dev/null
+++ b/freebsd/sys/cam/cam_compat.c
@@ -0,0 +1,426 @@
+#include <machine/rtems-bsd-kernel-space.h>
+
+/*-
+ * CAM ioctl compatibility shims
+ *
+ * Copyright (c) 2013 Scott Long
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/types.h>
+#include <sys/kernel.h>
+#include <sys/conf.h>
+#include <sys/fcntl.h>
+
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/sysctl.h>
+#include <sys/kthread.h>
+
+#include <cam/cam.h>
+#include <cam/cam_ccb.h>
+#include <cam/cam_xpt.h>
+#include <cam/cam_compat.h>
+#include <cam/cam_periph.h>
+#ifndef __rtems__
+#include <cam/scsi/scsi_pass.h>
+#else
+#define CAMIOCOMMAND	_IOWR(CAM_VERSION, 2, union ccb)
+#define CAMGETPASSTHRU	_IOWR(CAM_VERSION, 3, union ccb)
+#endif
+#include <rtems/bsd/local/opt_cam.h>
+
+static int cam_compat_handle_0x17(struct cdev *dev, u_long cmd, caddr_t addr,
+    int flag, struct thread *td, d_ioctl_t *cbfnp);
+static int cam_compat_handle_0x18(struct cdev *dev, u_long cmd, caddr_t addr,
+    int flag, struct thread *td, d_ioctl_t *cbfnp);
+static int cam_compat_translate_dev_match_0x18(union ccb *ccb);
+
+int
+cam_compat_ioctl(struct cdev *dev, u_long cmd, caddr_t addr, int flag,
+    struct thread *td, d_ioctl_t *cbfnp)
+{
+	int error;
+
+	switch (cmd) {
+	case CAMIOCOMMAND_0x16:
+	{
+		struct ccb_hdr_0x17 *hdr17;
+
+		hdr17 = (struct ccb_hdr_0x17 *)addr;
+		if (hdr17->flags & CAM_SG_LIST_PHYS_0x16) {
+			hdr17->flags &= ~CAM_SG_LIST_PHYS_0x16;
+			hdr17->flags |= CAM_DATA_SG_PADDR;
+		}
+		if (hdr17->flags & CAM_DATA_PHYS_0x16) {
+			hdr17->flags &= ~CAM_DATA_PHYS_0x16;
+			hdr17->flags |= CAM_DATA_PADDR;
+		}
+		if (hdr17->flags & CAM_SCATTER_VALID_0x16) {
+			hdr17->flags &= CAM_SCATTER_VALID_0x16;
+			hdr17->flags |= CAM_DATA_SG;
+		}
+		cmd = CAMIOCOMMAND;
+		error = cam_compat_handle_0x17(dev, cmd, addr, flag, td, cbfnp);
+		break;
+	}
+	case CAMGETPASSTHRU_0x16:
+		cmd = CAMGETPASSTHRU;
+		error = cam_compat_handle_0x17(dev, cmd, addr, flag, td, cbfnp);
+		break;
+	case CAMIOCOMMAND_0x17:
+		cmd = CAMIOCOMMAND;
+		error = cam_compat_handle_0x17(dev, cmd, addr, flag, td, cbfnp);
+		break;
+	case CAMGETPASSTHRU_0x17:
+		cmd = CAMGETPASSTHRU;
+		error = cam_compat_handle_0x17(dev, cmd, addr, flag, td, cbfnp);
+		break;
+	case CAMIOCOMMAND_0x18:
+		cmd = CAMIOCOMMAND;
+		error = cam_compat_handle_0x18(dev, cmd, addr, flag, td, cbfnp);
+		break;
+	case CAMGETPASSTHRU_0x18:
+		cmd = CAMGETPASSTHRU;
+		error = cam_compat_handle_0x18(dev, cmd, addr, flag, td, cbfnp);
+		break;
+	default:
+		error = ENOTTY;
+	}
+
+	return (error);
+}
+
+static int
+cam_compat_handle_0x17(struct cdev *dev, u_long cmd, caddr_t addr, int flag,
+    struct thread *td, d_ioctl_t *cbfnp)
+{
+	union ccb		*ccb;
+	struct ccb_hdr		*hdr;
+	struct ccb_hdr_0x17	*hdr17;
+	uint8_t			*ccbb, *ccbb17;
+	u_int			error;
+
+	hdr17 = (struct ccb_hdr_0x17 *)addr;
+	ccb = xpt_alloc_ccb();
+	hdr = &ccb->ccb_h;
+
+	hdr->pinfo = hdr17->pinfo;
+	hdr->xpt_links = hdr17->xpt_links;
+	hdr->sim_links = hdr17->sim_links;
+	hdr->periph_links = hdr17->periph_links;
+	hdr->retry_count = hdr17->retry_count;
+	hdr->cbfcnp = hdr17->cbfcnp;
+	hdr->func_code = hdr17->func_code;
+	hdr->status = hdr17->status;
+	hdr->path = hdr17->path;
+	hdr->path_id = hdr17->path_id;
+	hdr->target_id = hdr17->target_id;
+	hdr->target_lun = hdr17->target_lun;
+	hdr->flags = hdr17->flags;
+	hdr->xflags = 0;
+	hdr->periph_priv = hdr17->periph_priv;
+	hdr->sim_priv = hdr17->sim_priv;
+	hdr->timeout = hdr17->timeout;
+	hdr->softtimeout.tv_sec = 0;
+	hdr->softtimeout.tv_usec = 0;
+
+	ccbb = (uint8_t *)&hdr[1];
+	ccbb17 = (uint8_t *)&hdr17[1];
+	if (ccb->ccb_h.func_code == XPT_SET_TRAN_SETTINGS) {
+		struct ccb_trans_settings *cts;
+		struct ccb_trans_settings_0x17 *cts17;
+
+		cts = &ccb->cts;
+		cts17 = (struct ccb_trans_settings_0x17 *)hdr17;
+		cts->type = cts17->type;
+		cts->protocol = cts17->protocol;
+		cts->protocol_version = cts17->protocol_version;
+		cts->transport = cts17->transport;
+		cts->transport_version = cts17->transport_version;
+		bcopy(&cts17->proto_specific, &cts->proto_specific,
+		    sizeof(cts17->proto_specific));
+		bcopy(&cts17->xport_specific, &cts->xport_specific,
+		    sizeof(cts17->xport_specific));
+	} else {
+		bcopy(ccbb17, ccbb, CAM_0X17_DATA_LEN);
+	}
+
+	error = (cbfnp)(dev, cmd, (caddr_t)ccb, flag, td);
+
+	hdr17->pinfo = hdr->pinfo;
+	hdr17->xpt_links = hdr->xpt_links;
+	hdr17->sim_links = hdr->sim_links;
+	hdr17->periph_links = hdr->periph_links;
+	hdr17->retry_count = hdr->retry_count;
+	hdr17->cbfcnp = hdr->cbfcnp;
+	hdr17->func_code = hdr->func_code;
+	hdr17->status = hdr->status;
+	hdr17->path = hdr->path;
+	hdr17->path_id = hdr->path_id;
+	hdr17->target_id = hdr->target_id;
+	hdr17->target_lun = hdr->target_lun;
+	hdr17->flags = hdr->flags;
+	hdr17->periph_priv = hdr->periph_priv;
+	hdr17->sim_priv = hdr->sim_priv;
+	hdr17->timeout = hdr->timeout;
+
+	if (ccb->ccb_h.func_code == XPT_PATH_INQ) {
+		struct ccb_pathinq	*cpi;
+		struct ccb_pathinq_0x17 *cpi17;
+
+		/* The PATH_INQ only needs special handling on the way out */
+		cpi = &ccb->cpi;
+		cpi17 = (struct ccb_pathinq_0x17 *)hdr17;
+		cpi17->version_num = cpi->version_num;
+		cpi17->hba_inquiry = cpi->hba_inquiry;
+		cpi17->target_sprt = (u_int8_t)cpi->target_sprt;
+		cpi17->hba_misc = (u_int8_t)cpi->hba_misc;
+		cpi17->hba_eng_cnt = cpi->hba_eng_cnt;
+		bcopy(&cpi->vuhba_flags[0], &cpi17->vuhba_flags[0], VUHBALEN);
+		cpi17->max_target = cpi->max_target;
+		cpi17->max_lun = cpi->max_lun;
+		cpi17->async_flags = cpi->async_flags;
+		cpi17->hpath_id = cpi->hpath_id;
+		cpi17->initiator_id = cpi->initiator_id;
+		bcopy(&cpi->sim_vid[0], &cpi17->sim_vid[0], SIM_IDLEN);
+		bcopy(&cpi->hba_vid[0], &cpi17->hba_vid[0], HBA_IDLEN);
+		bcopy(&cpi->dev_name[0], &cpi17->dev_name[0], DEV_IDLEN);
+		cpi17->unit_number = cpi->unit_number;
+		cpi17->bus_id = cpi->bus_id;
+		cpi17->base_transfer_speed = cpi->base_transfer_speed;
+		cpi17->protocol = cpi->protocol;
+		cpi17->protocol_version = cpi->protocol_version;
+		cpi17->transport = cpi->transport;
+		cpi17->transport_version = cpi->transport_version;
+		bcopy(&cpi->xport_specific, &cpi17->xport_specific,
+		    PATHINQ_SETTINGS_SIZE);
+		cpi17->maxio = cpi->maxio;
+		cpi17->hba_vendor = cpi->hba_vendor;
+		cpi17->hba_device = cpi->hba_device;
+		cpi17->hba_subvendor = cpi->hba_subvendor;
+		cpi17->hba_subdevice = cpi->hba_subdevice;
+	} else if (ccb->ccb_h.func_code == XPT_GET_TRAN_SETTINGS) {
+		struct ccb_trans_settings *cts;
+		struct ccb_trans_settings_0x17 *cts17;
+
+		cts = &ccb->cts;
+		cts17 = (struct ccb_trans_settings_0x17 *)hdr17;
+		cts17->type = cts->type;
+		cts17->protocol = cts->protocol;
+		cts17->protocol_version = cts->protocol_version;
+		cts17->transport = cts->transport;
+		cts17->transport_version = cts->transport_version;
+		bcopy(&cts->proto_specific, &cts17->proto_specific,
+		    sizeof(cts17->proto_specific));
+		bcopy(&cts->xport_specific, &cts17->xport_specific,
+		    sizeof(cts17->xport_specific));
+	} else if (ccb->ccb_h.func_code == XPT_DEV_MATCH) {
+		/* Copy the rest of the header over */
+		bcopy(ccbb, ccbb17, CAM_0X17_DATA_LEN);
+
+		cam_compat_translate_dev_match_0x18(ccb);
+	} else {
+		bcopy(ccbb, ccbb17, CAM_0X17_DATA_LEN);
+	}
+
+	xpt_free_ccb(ccb);
+
+	return (error);
+}
+
+static int
+cam_compat_handle_0x18(struct cdev *dev, u_long cmd, caddr_t addr, int flag,
+    struct thread *td, d_ioctl_t *cbfnp)
+{
+	union ccb		*ccb;
+	struct ccb_hdr		*hdr;
+	struct ccb_hdr_0x18	*hdr18;
+	uint8_t			*ccbb, *ccbb18;
+	u_int			error;
+
+	hdr18 = (struct ccb_hdr_0x18 *)addr;
+	ccb = xpt_alloc_ccb();
+	hdr = &ccb->ccb_h;
+
+	hdr->pinfo = hdr18->pinfo;
+	hdr->xpt_links = hdr18->xpt_links;
+	hdr->sim_links = hdr18->sim_links;
+	hdr->periph_links = hdr18->periph_links;
+	hdr->retry_count = hdr18->retry_count;
+	hdr->cbfcnp = hdr18->cbfcnp;
+	hdr->func_code = hdr18->func_code;
+	hdr->status = hdr18->status;
+	hdr->path = hdr18->path;
+	hdr->path_id = hdr18->path_id;
+	hdr->target_id = hdr18->target_id;
+	hdr->target_lun = hdr18->target_lun;
+	if (hdr18->xflags & CAM_EXTLUN_VALID_0x18)
+		hdr->target_lun = hdr18->ext_lun;
+	hdr->flags = hdr18->flags;
+	hdr->xflags = hdr18->xflags;
+	hdr->periph_priv = hdr18->periph_priv;
+	hdr->sim_priv = hdr18->sim_priv;
+	hdr->timeout = hdr18->timeout;
+	hdr->softtimeout.tv_sec = 0;
+	hdr->softtimeout.tv_usec = 0;
+
+	ccbb = (uint8_t *)&hdr[1];
+	ccbb18 = (uint8_t *)&hdr18[1];
+	if (ccb->ccb_h.func_code == XPT_SET_TRAN_SETTINGS) {
+		struct ccb_trans_settings *cts;
+		struct ccb_trans_settings_0x18 *cts18;
+
+		cts = &ccb->cts;
+		cts18 = (struct ccb_trans_settings_0x18 *)hdr18;
+		cts->type = cts18->type;
+		cts->protocol = cts18->protocol;
+		cts->protocol_version = cts18->protocol_version;
+		cts->transport = cts18->transport;
+		cts->transport_version = cts18->transport_version;
+		bcopy(&cts18->proto_specific, &cts->proto_specific,
+		    sizeof(cts18->proto_specific));
+		bcopy(&cts18->xport_specific, &cts->xport_specific,
+		    sizeof(cts18->xport_specific));
+	} else {
+		bcopy(ccbb18, ccbb, CAM_0X18_DATA_LEN);
+	}
+
+	error = (cbfnp)(dev, cmd, (caddr_t)ccb, flag, td);
+
+	hdr18->pinfo = hdr->pinfo;
+	hdr18->xpt_links = hdr->xpt_links;
+	hdr18->sim_links = hdr->sim_links;
+	hdr18->periph_links = hdr->periph_links;
+	hdr18->retry_count = hdr->retry_count;
+	hdr18->cbfcnp = hdr->cbfcnp;
+	hdr18->func_code = hdr->func_code;
+	hdr18->status = hdr->status;
+	hdr18->path = hdr->path;
+	hdr18->path_id = hdr->path_id;
+	hdr18->target_id = hdr->target_id;
+	hdr18->target_lun = hdr->target_lun;
+	hdr18->ext_lun = hdr->target_lun;
+	hdr18->flags = hdr->flags;
+	hdr18->xflags = hdr->xflags | CAM_EXTLUN_VALID_0x18;
+	hdr18->periph_priv = hdr->periph_priv;
+	hdr18->sim_priv = hdr->sim_priv;
+	hdr18->timeout = hdr->timeout;
+
+	if (ccb->ccb_h.func_code == XPT_GET_TRAN_SETTINGS) {
+		struct ccb_trans_settings *cts;
+		struct ccb_trans_settings_0x18 *cts18;
+
+		cts = &ccb->cts;
+		cts18 = (struct ccb_trans_settings_0x18 *)hdr18;
+		cts18->type = cts->type;
+		cts18->protocol = cts->protocol;
+		cts18->protocol_version = cts->protocol_version;
+		cts18->transport = cts->transport;
+		cts18->transport_version = cts->transport_version;
+		bcopy(&cts->proto_specific, &cts18->proto_specific,
+		    sizeof(cts18->proto_specific));
+		bcopy(&cts->xport_specific, &cts18->xport_specific,
+		    sizeof(cts18->xport_specific));
+	} else if (ccb->ccb_h.func_code == XPT_DEV_MATCH) {
+		bcopy(ccbb, ccbb18, CAM_0X18_DATA_LEN);
+		cam_compat_translate_dev_match_0x18(ccb);
+	} else {
+		bcopy(ccbb, ccbb18, CAM_0X18_DATA_LEN);
+	}
+
+	xpt_free_ccb(ccb);
+
+	return (error);
+}
+
+static int
+cam_compat_translate_dev_match_0x18(union ccb *ccb)
+{
+	struct dev_match_result		*dm;
+	struct dev_match_result_0x18	*dm18;
+	struct cam_periph_map_info	mapinfo;
+	int i;
+#ifndef __rtems__
+	/* Remap the CCB into kernel address space */
+	bzero(&mapinfo, sizeof(mapinfo));
+	cam_periph_mapmem(ccb, &mapinfo, MAXPHYS);
+#endif
+	dm = ccb->cdm.matches;
+	/* Translate in-place: old fields are smaller */
+	dm18 = (struct dev_match_result_0x18 *)(dm);
+	
+	for (i = 0; i < ccb->cdm.num_matches; i++) {
+		dm18[i].type = dm[i].type;
+		switch (dm[i].type) {
+		case DEV_MATCH_PERIPH:
+			memcpy(&dm18[i].result.periph_result.periph_name, 
+			    &dm[i].result.periph_result.periph_name,
+			    DEV_IDLEN);
+			dm18[i].result.periph_result.unit_number =
+			   dm[i].result.periph_result.unit_number;
+			dm18[i].result.periph_result.path_id =
+			   dm[i].result.periph_result.path_id;
+			dm18[i].result.periph_result.target_id =
+			   dm[i].result.periph_result.target_id;
+			dm18[i].result.periph_result.target_lun =
+			   dm[i].result.periph_result.target_lun;
+			break;
+		case DEV_MATCH_DEVICE:
+			dm18[i].result.device_result.path_id =
+			   dm[i].result.device_result.path_id;
+			dm18[i].result.device_result.target_id =
+			   dm[i].result.device_result.target_id;
+			dm18[i].result.device_result.target_lun =
+			   dm[i].result.device_result.target_lun;
+			dm18[i].result.device_result.protocol =
+			   dm[i].result.device_result.protocol;
+			memcpy(&dm18[i].result.device_result.inq_data, 
+			    &dm[i].result.device_result.inq_data,
+			    sizeof(struct scsi_inquiry_data));
+			memcpy(&dm18[i].result.device_result.ident_data,
+			    &dm[i].result.device_result.ident_data,
+			    sizeof(struct ata_params));
+			dm18[i].result.device_result.flags =
+			   dm[i].result.device_result.flags;
+			break;
+		case DEV_MATCH_BUS:
+			memcpy(&dm18[i].result.bus_result, 
+			    &dm[i].result.bus_result,
+			    sizeof(struct bus_match_result));
+			break;
+		}
+	}
+#ifndef __rtems__
+	cam_periph_unmapmem(ccb, &mapinfo);
+#endif
+	return (0);
+}
+
diff --git a/freebsd/sys/cam/cam_compat.h b/freebsd/sys/cam/cam_compat.h
new file mode 100644
index 0000000..a939087
--- /dev/null
+++ b/freebsd/sys/cam/cam_compat.h
@@ -0,0 +1,223 @@
+/*-
+ * CAM ioctl compatibility shims
+ *
+ * Copyright (c) 2013 Scott Long
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _CAM_CAM_COMPAT_H
+#define _CAM_CAM_COMPAT_H
+
+/* No user-serviceable parts in here. */
+#ifdef _KERNEL
+
+int cam_compat_ioctl(struct cdev *dev, u_long cmd, caddr_t addr, int flag,
+    struct thread *td, int(*cbfnp)(struct cdev *, u_long, caddr_t, int,
+    struct thread *));
+
+
+/* Version 0x16 compatibility */
+#define CAM_VERSION_0x16	0x16
+
+/* The size of the union ccb didn't change when going to 0x17 */
+#define	CAMIOCOMMAND_0x16	_IOC(IOC_INOUT, CAM_VERSION_0x16, 2, CAM_0X17_LEN)
+#define	CAMGETPASSTHRU_0x16	_IOC(IOC_INOUT, CAM_VERSION_0x16, 3, CAM_0X17_LEN)
+
+#define CAM_SCATTER_VALID_0x16	0x00000010
+#define CAM_SG_LIST_PHYS_0x16	0x00040000
+#define CAM_DATA_PHYS_0x16	0x00200000
+
+/* Version 0x17 compatibility */
+#define CAM_VERSION_0x17	0x17
+
+struct ccb_hdr_0x17 {
+	cam_pinfo	pinfo;		/* Info for priority scheduling */
+	camq_entry	xpt_links;	/* For chaining in the XPT layer */	
+	camq_entry	sim_links;	/* For chaining in the SIM layer */	
+	camq_entry	periph_links;	/* For chaining in the type driver */
+	u_int32_t	retry_count;
+	void		(*cbfcnp)(struct cam_periph *, union ccb *);
+	xpt_opcode	func_code;	/* XPT function code */
+	u_int32_t	status;		/* Status returned by CAM subsystem */
+	struct		cam_path *path;	/* Compiled path for this ccb */
+	path_id_t	path_id;	/* Path ID for the request */
+	target_id_t	target_id;	/* Target device ID */
+	u_int		target_lun;	/* Target LUN number */
+	u_int32_t	flags;		/* ccb_flags */
+	ccb_ppriv_area	periph_priv;
+	ccb_spriv_area	sim_priv;
+	u_int32_t	timeout;	/* Hard timeout value in seconds */
+	struct callout_handle timeout_ch;
+};
+
+struct ccb_pathinq_0x17 {
+	struct ccb_hdr_0x17 ccb_h;
+	u_int8_t    version_num;	/* Version number for the SIM/HBA */
+	u_int8_t    hba_inquiry;	/* Mimic of INQ byte 7 for the HBA */
+	u_int8_t    target_sprt;	/* Flags for target mode support */
+	u_int8_t    hba_misc;		/* Misc HBA features */
+	u_int16_t   hba_eng_cnt;	/* HBA engine count */
+					/* Vendor Unique capabilities */
+	u_int8_t    vuhba_flags[VUHBALEN];
+	u_int32_t   max_target;		/* Maximum supported Target */
+	u_int32_t   max_lun;		/* Maximum supported Lun */
+	u_int32_t   async_flags;	/* Installed Async handlers */
+	path_id_t   hpath_id;		/* Highest Path ID in the subsystem */
+	target_id_t initiator_id;	/* ID of the HBA on the SCSI bus */
+	char	    sim_vid[SIM_IDLEN];	/* Vendor ID of the SIM */
+	char	    hba_vid[HBA_IDLEN];	/* Vendor ID of the HBA */
+	char 	    dev_name[DEV_IDLEN];/* Device name for SIM */
+	u_int32_t   unit_number;	/* Unit number for SIM */
+	u_int32_t   bus_id;		/* Bus ID for SIM */
+	u_int32_t   base_transfer_speed;/* Base bus speed in KB/sec */
+	cam_proto   protocol;
+	u_int	    protocol_version;
+	cam_xport   transport;
+	u_int	    transport_version;
+	union {
+		struct ccb_pathinq_settings_spi spi;
+		struct ccb_pathinq_settings_fc fc;
+		struct ccb_pathinq_settings_sas sas;
+		char ccb_pathinq_settings_opaque[PATHINQ_SETTINGS_SIZE];
+	} xport_specific;
+	u_int		maxio;		/* Max supported I/O size, in bytes. */
+	u_int16_t	hba_vendor;	/* HBA vendor ID */
+	u_int16_t	hba_device;	/* HBA device ID */
+	u_int16_t	hba_subvendor;	/* HBA subvendor ID */
+	u_int16_t	hba_subdevice;	/* HBA subdevice ID */
+};
+
+struct ccb_trans_settings_0x17 {
+	struct	  ccb_hdr_0x17 ccb_h;
+	cts_type  type;		/* Current or User settings */
+	cam_proto protocol;
+	u_int	  protocol_version;
+	cam_xport transport;
+	u_int	  transport_version;
+	union {
+		u_int  valid;	/* Which fields to honor */
+		struct ccb_trans_settings_ata ata;
+		struct ccb_trans_settings_scsi scsi;
+	} proto_specific;
+	union {
+		u_int  valid;	/* Which fields to honor */
+		struct ccb_trans_settings_spi spi;
+		struct ccb_trans_settings_fc fc;
+		struct ccb_trans_settings_sas sas;
+		struct ccb_trans_settings_pata ata;
+		struct ccb_trans_settings_sata sata;
+	} xport_specific;
+};
+
+#define CAM_0X17_DATA_LEN	CAM_0X18_DATA_LEN
+#define CAM_0X17_LEN		(sizeof(struct ccb_hdr_0x17) + CAM_0X17_DATA_LEN)
+
+#define	CAMIOCOMMAND_0x17	_IOC(IOC_INOUT, CAM_VERSION_0x17, 2, CAM_0X17_LEN)
+#define CAMGETPASSTHRU_0x17	_IOC(IOC_INOUT, CAM_VERSION_0x17, 3, CAM_0X17_LEN)
+
+/* Version 0x18 compatibility */
+#define CAM_VERSION_0x18	0x18
+
+struct ccb_hdr_0x18 {
+	cam_pinfo	pinfo;		/* Info for priority scheduling */
+	camq_entry	xpt_links;	/* For chaining in the XPT layer */	
+	camq_entry	sim_links;	/* For chaining in the SIM layer */	
+	camq_entry	periph_links;	/* For chaining in the type driver */
+	u_int32_t	retry_count;
+	void		(*cbfcnp)(struct cam_periph *, union ccb *);
+	xpt_opcode	func_code;	/* XPT function code */
+	u_int32_t	status;		/* Status returned by CAM subsystem */
+	struct		cam_path *path;	/* Compiled path for this ccb */
+	path_id_t	path_id;	/* Path ID for the request */
+	target_id_t	target_id;	/* Target device ID */
+	u_int		target_lun;	/* Target LUN number */
+	u_int64_t	ext_lun;	/* 64-bit LUN, more or less */
+	u_int32_t	flags;		/* ccb_flags */
+	u_int32_t	xflags;		/* extended ccb_flags */
+	ccb_ppriv_area	periph_priv;
+	ccb_spriv_area	sim_priv;
+	ccb_qos_area	qos;
+	u_int32_t	timeout;	/* Hard timeout value in seconds */
+	struct timeval	softtimeout;	/* Soft timeout value in sec + usec */
+};
+
+typedef enum {
+	CAM_EXTLUN_VALID_0x18	= 0x00000001,/* 64bit lun field is valid      */
+} ccb_xflags_0x18;
+
+struct ccb_trans_settings_0x18 {
+	struct	  ccb_hdr_0x18 ccb_h;
+	cts_type  type;		/* Current or User settings */
+	cam_proto protocol;
+	u_int	  protocol_version;
+	cam_xport transport;
+	u_int	  transport_version;
+	union {
+		u_int  valid;	/* Which fields to honor */
+		struct ccb_trans_settings_ata ata;
+		struct ccb_trans_settings_scsi scsi;
+	} proto_specific;
+	union {
+		u_int  valid;	/* Which fields to honor */
+		struct ccb_trans_settings_spi spi;
+		struct ccb_trans_settings_fc fc;
+		struct ccb_trans_settings_sas sas;
+		struct ccb_trans_settings_pata ata;
+		struct ccb_trans_settings_sata sata;
+	} xport_specific;
+};
+
+struct dev_match_result_0x18 {
+        dev_match_type          type;
+        union {
+		struct {
+			char periph_name[DEV_IDLEN];
+			u_int32_t unit_number;
+			path_id_t path_id;
+			target_id_t target_id;
+			u_int target_lun;
+		} periph_result;
+		struct {
+			path_id_t	path_id;
+			target_id_t	target_id;
+			u_int		target_lun;
+			cam_proto	protocol;
+			struct scsi_inquiry_data inq_data;
+			struct ata_params ident_data;
+			dev_result_flags flags;
+		} device_result;
+		struct bus_match_result	bus_result;
+	} result;
+};
+
+#define CAM_0X18_DATA_LEN	(sizeof(union ccb) - 2*sizeof(void *) - sizeof(struct ccb_hdr))
+#define CAM_0X18_LEN		(sizeof(struct ccb_hdr_0x18) + CAM_0X18_DATA_LEN)
+
+#define	CAMIOCOMMAND_0x18	_IOC(IOC_INOUT, CAM_VERSION_0x18, 2, CAM_0X18_LEN)
+#define CAMGETPASSTHRU_0x18	_IOC(IOC_INOUT, CAM_VERSION_0x18, 3, CAM_0X18_LEN)
+
+#endif
+#endif
diff --git a/freebsd/sys/cam/cam_debug.h b/freebsd/sys/cam/cam_debug.h
index 7b619a2..c13d7c1 100644
--- a/freebsd/sys/cam/cam_debug.h
+++ b/freebsd/sys/cam/cam_debug.h
@@ -44,7 +44,7 @@ typedef enum {
 	CAM_DEBUG_PROBE		= 0x40  /* print out probe actions */
 } cam_debug_flags;
 
-#if defined(_KERNEL) && !defined(__rtems__)
+#if defined(_KERNEL) && defined(__rtems__)
 
 #ifndef CAM_DEBUG_FLAGS
 #define CAM_DEBUG_FLAGS		CAM_DEBUG_NONE
diff --git a/freebsd/sys/cam/cam_periph.c b/freebsd/sys/cam/cam_periph.c
new file mode 100644
index 0000000..4986639
--- /dev/null
+++ b/freebsd/sys/cam/cam_periph.c
@@ -0,0 +1,2018 @@
+#include <machine/rtems-bsd-kernel-space.h>
+
+/*-
+ * Common functions for CAM "type" (peripheral) drivers.
+ *
+ * Copyright (c) 1997, 1998 Justin T. Gibbs.
+ * Copyright (c) 1997, 1998, 1999, 2000 Kenneth D. Merry.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/types.h>
+#include <sys/malloc.h>
+#include <sys/kernel.h>
+#include <sys/bio.h>
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/buf.h>
+#include <sys/proc.h>
+#include <sys/devicestat.h>
+#include <sys/bus.h>
+#include <sys/sbuf.h>
+#include <vm/vm.h>
+#include <vm/vm_extern.h>
+
+#include <cam/cam.h>
+#include <cam/cam_ccb.h>
+#include <cam/cam_queue.h>
+#include <cam/cam_xpt_periph.h>
+#include <cam/cam_periph.h>
+#include <cam/cam_debug.h>
+#include <cam/cam_sim.h>
+
+#include <cam/scsi/scsi_all.h>
+#ifndef __rtems__
+#include <cam/scsi/scsi_message.h>
+#include <cam/scsi/scsi_pass.h>
+#endif /* __rtems__ */
+//Extracted from scsi
+#define CAMIOCOMMAND	_IOWR(CAM_VERSION, 2, union ccb)
+#define CAMGETPASSTHRU	_IOWR(CAM_VERSION, 3, union ccb)
+
+static	u_int		camperiphnextunit(struct periph_driver *p_drv,
+					  u_int newunit, int wired,
+					  path_id_t pathid, target_id_t target,
+					  lun_id_t lun);
+static	u_int		camperiphunit(struct periph_driver *p_drv,
+				      path_id_t pathid, target_id_t target,
+				      lun_id_t lun); 
+static	void		camperiphdone(struct cam_periph *periph, 
+					union ccb *done_ccb);
+static  void		camperiphfree(struct cam_periph *periph);
+static int		camperiphscsistatuserror(union ccb *ccb,
+					        union ccb **orig_ccb,
+						 cam_flags camflags,
+						 u_int32_t sense_flags,
+						 int *openings,
+						 u_int32_t *relsim_flags,
+						 u_int32_t *timeout,
+						 u_int32_t  *action,
+						 const char **action_string);
+static	int		camperiphscsisenseerror(union ccb *ccb,
+					        union ccb **orig_ccb,
+					        cam_flags camflags,
+					        u_int32_t sense_flags,
+					        int *openings,
+					        u_int32_t *relsim_flags,
+					        u_int32_t *timeout,
+					        u_int32_t *action,
+					        const char **action_string);
+static void		cam_periph_devctl_notify(union ccb *ccb);
+
+static int nperiph_drivers;
+static int initialized = 0;
+struct periph_driver **periph_drivers;
+
+static MALLOC_DEFINE(M_CAMPERIPH, "CAM periph", "CAM peripheral buffers");
+
+static int periph_selto_delay = 1000;
+TUNABLE_INT("kern.cam.periph_selto_delay", &periph_selto_delay);
+static int periph_noresrc_delay = 500;
+TUNABLE_INT("kern.cam.periph_noresrc_delay", &periph_noresrc_delay);
+static int periph_busy_delay = 500;
+TUNABLE_INT("kern.cam.periph_busy_delay", &periph_busy_delay);
+
+
+void
+periphdriver_register(void *data)
+{
+	struct periph_driver *drv = (struct periph_driver *)data;
+	struct periph_driver **newdrivers, **old;
+	int ndrivers;
+
+again:
+	ndrivers = nperiph_drivers + 2;
+	newdrivers = malloc(sizeof(*newdrivers) * ndrivers, M_CAMPERIPH,
+			    M_WAITOK);
+	xpt_lock_buses();
+	if (ndrivers != nperiph_drivers + 2) {
+		/*
+		 * Lost race against itself; go around.
+		 */
+		xpt_unlock_buses();
+		free(newdrivers, M_CAMPERIPH);
+		goto again;
+	}
+	if (periph_drivers)
+		bcopy(periph_drivers, newdrivers,
+		      sizeof(*newdrivers) * nperiph_drivers);
+	newdrivers[nperiph_drivers] = drv;
+	newdrivers[nperiph_drivers + 1] = NULL;
+	old = periph_drivers;
+	periph_drivers = newdrivers;
+	nperiph_drivers++;
+	xpt_unlock_buses();
+	if (old)
+		free(old, M_CAMPERIPH);
+	/* If driver marked as early or it is late now, initialize it. */
+	if (((drv->flags & CAM_PERIPH_DRV_EARLY) != 0 && initialized > 0) ||
+	    initialized > 1)
+		(*drv->init)();
+}
+
+int
+periphdriver_unregister(void *data)
+{
+	struct periph_driver *drv = (struct periph_driver *)data;
+	int error, n;
+
+	/* If driver marked as early or it is late now, deinitialize it. */
+	if (((drv->flags & CAM_PERIPH_DRV_EARLY) != 0 && initialized > 0) ||
+	    initialized > 1) {
+		if (drv->deinit == NULL) {
+			printf("CAM periph driver '%s' doesn't have deinit.\n",
+			    drv->driver_name);
+			return (EOPNOTSUPP);
+		}
+		error = drv->deinit();
+		if (error != 0)
+			return (error);
+	}
+
+	xpt_lock_buses();
+	for (n = 0; n < nperiph_drivers && periph_drivers[n] != drv; n++)
+		;
+	KASSERT(n < nperiph_drivers,
+	    ("Periph driver '%s' was not registered", drv->driver_name));
+	for (; n + 1 < nperiph_drivers; n++)
+		periph_drivers[n] = periph_drivers[n + 1];
+	periph_drivers[n + 1] = NULL;
+	nperiph_drivers--;
+	xpt_unlock_buses();
+	return (0);
+}
+
+void
+periphdriver_init(int level)
+{
+	int	i, early;
+
+	initialized = max(initialized, level);
+	for (i = 0; periph_drivers[i] != NULL; i++) {
+		early = (periph_drivers[i]->flags & CAM_PERIPH_DRV_EARLY) ? 1 : 2;
+		if (early == initialized)
+			(*periph_drivers[i]->init)();
+	}
+}
+
+cam_status
+cam_periph_alloc(periph_ctor_t *periph_ctor,
+		 periph_oninv_t *periph_oninvalidate,
+		 periph_dtor_t *periph_dtor, periph_start_t *periph_start,
+		 char *name, cam_periph_type type, struct cam_path *path,
+		 ac_callback_t *ac_callback, ac_code code, void *arg)
+{
+	struct		periph_driver **p_drv;
+	struct		cam_sim *sim;
+	struct		cam_periph *periph;
+	struct		cam_periph *cur_periph;
+	path_id_t	path_id;
+	target_id_t	target_id;
+	lun_id_t	lun_id;
+	cam_status	status;
+	u_int		init_level;
+
+	init_level = 0;
+	/*
+	 * Handle Hot-Plug scenarios.  If there is already a peripheral
+	 * of our type assigned to this path, we are likely waiting for
+	 * final close on an old, invalidated, peripheral.  If this is
+	 * the case, queue up a deferred call to the peripheral's async
+	 * handler.  If it looks like a mistaken re-allocation, complain.
+	 */
+	if ((periph = cam_periph_find(path, name)) != NULL) {
+
+		if ((periph->flags & CAM_PERIPH_INVALID) != 0
+		 && (periph->flags & CAM_PERIPH_NEW_DEV_FOUND) == 0) {
+			periph->flags |= CAM_PERIPH_NEW_DEV_FOUND;
+			periph->deferred_callback = ac_callback;
+			periph->deferred_ac = code;
+			return (CAM_REQ_INPROG);
+		} else {
+			printf("cam_periph_alloc: attempt to re-allocate "
+			       "valid device %s%d rejected flags %#x "
+			       "refcount %d\n", periph->periph_name,
+			       periph->unit_number, periph->flags,
+			       periph->refcount);
+		}
+		return (CAM_REQ_INVALID);
+	}
+	
+	periph = (struct cam_periph *)malloc(sizeof(*periph), M_CAMPERIPH,
+					     M_NOWAIT|M_ZERO);
+
+	if (periph == NULL)
+		return (CAM_RESRC_UNAVAIL);
+	
+	init_level++;
+
+
+	sim = xpt_path_sim(path);
+	path_id = xpt_path_path_id(path);
+	target_id = xpt_path_target_id(path);
+	lun_id = xpt_path_lun_id(path);
+	periph->periph_start = periph_start;
+	periph->periph_dtor = periph_dtor;
+	periph->periph_oninval = periph_oninvalidate;
+	periph->type = type;
+	periph->periph_name = name;
+	periph->scheduled_priority = CAM_PRIORITY_NONE;
+	periph->immediate_priority = CAM_PRIORITY_NONE;
+	periph->refcount = 1;		/* Dropped by invalidation. */
+	periph->sim = sim;
+	SLIST_INIT(&periph->ccb_list);
+	status = xpt_create_path(&path, periph, path_id, target_id, lun_id);
+	if (status != CAM_REQ_CMP)
+		goto failure;
+	periph->path = path;
+
+	xpt_lock_buses();
+	for (p_drv = periph_drivers; *p_drv != NULL; p_drv++) {
+		if (strcmp((*p_drv)->driver_name, name) == 0)
+			break;
+	}
+	if (*p_drv == NULL) {
+		printf("cam_periph_alloc: invalid periph name '%s'\n", name);
+		xpt_unlock_buses();
+		xpt_free_path(periph->path);
+		free(periph, M_CAMPERIPH);
+		return (CAM_REQ_INVALID);
+	}
+	periph->unit_number = camperiphunit(*p_drv, path_id, target_id, lun_id);
+	cur_periph = TAILQ_FIRST(&(*p_drv)->units);
+	while (cur_periph != NULL
+	    && cur_periph->unit_number < periph->unit_number)
+		cur_periph = TAILQ_NEXT(cur_periph, unit_links);
+	if (cur_periph != NULL) {
+		KASSERT(cur_periph->unit_number != periph->unit_number, ("duplicate units on periph list"));
+		TAILQ_INSERT_BEFORE(cur_periph, periph, unit_links);
+	} else {
+		TAILQ_INSERT_TAIL(&(*p_drv)->units, periph, unit_links);
+		(*p_drv)->generation++;
+	}
+	xpt_unlock_buses();
+
+	init_level++;
+
+	status = xpt_add_periph(periph);
+	if (status != CAM_REQ_CMP)
+		goto failure;
+
+	init_level++;
+	CAM_DEBUG(periph->path, CAM_DEBUG_INFO, ("Periph created\n"));
+
+	status = periph_ctor(periph, arg);
+
+	if (status == CAM_REQ_CMP)
+		init_level++;
+
+failure:
+	switch (init_level) {
+	case 4:
+		/* Initialized successfully */
+		break;
+	case 3:
+		CAM_DEBUG(periph->path, CAM_DEBUG_INFO, ("Periph destroyed\n"));
+		xpt_remove_periph(periph);
+		/* FALLTHROUGH */
+	case 2:
+		xpt_lock_buses();
+		TAILQ_REMOVE(&(*p_drv)->units, periph, unit_links);
+		xpt_unlock_buses();
+		xpt_free_path(periph->path);
+		/* FALLTHROUGH */
+	case 1:
+		free(periph, M_CAMPERIPH);
+		/* FALLTHROUGH */
+	case 0:
+		/* No cleanup to perform. */
+		break;
+	default:
+		panic("%s: Unknown init level", __func__);
+	}
+	return(status);
+}
+
+/*
+ * Find a peripheral structure with the specified path, target, lun, 
+ * and (optionally) type.  If the name is NULL, this function will return
+ * the first peripheral driver that matches the specified path.
+ */
+struct cam_periph *
+cam_periph_find(struct cam_path *path, char *name)
+{
+	struct periph_driver **p_drv;
+	struct cam_periph *periph;
+
+	xpt_lock_buses();
+	for (p_drv = periph_drivers; *p_drv != NULL; p_drv++) {
+
+		if (name != NULL && (strcmp((*p_drv)->driver_name, name) != 0))
+			continue;
+
+		TAILQ_FOREACH(periph, &(*p_drv)->units, unit_links) {
+			if (xpt_path_comp(periph->path, path) == 0) {
+				xpt_unlock_buses();
+				cam_periph_assert(periph, MA_OWNED);
+				return(periph);
+			}
+		}
+		if (name != NULL) {
+			xpt_unlock_buses();
+			return(NULL);
+		}
+	}
+	xpt_unlock_buses();
+	return(NULL);
+}
+
+/*
+ * Find peripheral driver instances attached to the specified path.
+ */
+int
+cam_periph_list(struct cam_path *path, struct sbuf *sb)
+{
+	struct sbuf local_sb;
+	struct periph_driver **p_drv;
+	struct cam_periph *periph;
+	int count;
+	int sbuf_alloc_len;
+
+	sbuf_alloc_len = 16;
+retry:
+	sbuf_new(&local_sb, NULL, sbuf_alloc_len, SBUF_FIXEDLEN);
+	count = 0;
+	xpt_lock_buses();
+	for (p_drv = periph_drivers; *p_drv != NULL; p_drv++) {
+
+		TAILQ_FOREACH(periph, &(*p_drv)->units, unit_links) {
+			if (xpt_path_comp(periph->path, path) != 0)
+				continue;
+
+			if (sbuf_len(&local_sb) != 0)
+				sbuf_cat(&local_sb, ",");
+
+			sbuf_printf(&local_sb, "%s%d", periph->periph_name,
+				    periph->unit_number);
+
+			if (sbuf_error(&local_sb) == ENOMEM) {
+				sbuf_alloc_len *= 2;
+				xpt_unlock_buses();
+				sbuf_delete(&local_sb);
+				goto retry;
+			}
+			count++;
+		}
+	}
+	xpt_unlock_buses();
+	sbuf_finish(&local_sb);
+	sbuf_cpy(sb, sbuf_data(&local_sb));
+	sbuf_delete(&local_sb);
+	return (count);
+}
+
+cam_status
+cam_periph_acquire(struct cam_periph *periph)
+{
+	cam_status status;
+
+	status = CAM_REQ_CMP_ERR;
+	if (periph == NULL)
+		return (status);
+
+	xpt_lock_buses();
+	if ((periph->flags & CAM_PERIPH_INVALID) == 0) {
+		periph->refcount++;
+		status = CAM_REQ_CMP;
+	}
+	xpt_unlock_buses();
+
+	return (status);
+}
+
+void
+cam_periph_doacquire(struct cam_periph *periph)
+{
+
+	xpt_lock_buses();
+	KASSERT(periph->refcount >= 1,
+	    ("cam_periph_doacquire() with refcount == %d", periph->refcount));
+	periph->refcount++;
+	xpt_unlock_buses();
+}
+
+void
+cam_periph_release_locked_buses(struct cam_periph *periph)
+{
+
+	cam_periph_assert(periph, MA_OWNED);
+	KASSERT(periph->refcount >= 1, ("periph->refcount >= 1"));
+	if (--periph->refcount == 0)
+		camperiphfree(periph);
+}
+
+void
+cam_periph_release_locked(struct cam_periph *periph)
+{
+
+	if (periph == NULL)
+		return;
+
+	xpt_lock_buses();
+	cam_periph_release_locked_buses(periph);
+	xpt_unlock_buses();
+}
+
+void
+cam_periph_release(struct cam_periph *periph)
+{
+	struct mtx *mtx;
+
+	if (periph == NULL)
+		return;
+	
+	cam_periph_assert(periph, MA_NOTOWNED);
+	mtx = cam_periph_mtx(periph);
+	mtx_lock(mtx);
+	cam_periph_release_locked(periph);
+	mtx_unlock(mtx);
+}
+
+int
+cam_periph_hold(struct cam_periph *periph, int priority)
+{
+	int error;
+
+	/*
+	 * Increment the reference count on the peripheral
+	 * while we wait for our lock attempt to succeed
+	 * to ensure the peripheral doesn't disappear out
+	 * from user us while we sleep.
+	 */
+
+	if (cam_periph_acquire(periph) != CAM_REQ_CMP)
+		return (ENXIO);
+
+	cam_periph_assert(periph, MA_OWNED);
+	while ((periph->flags & CAM_PERIPH_LOCKED) != 0) {
+		periph->flags |= CAM_PERIPH_LOCK_WANTED;
+		if ((error = cam_periph_sleep(periph, periph, priority,
+		    "caplck", 0)) != 0) {
+			cam_periph_release_locked(periph);
+			return (error);
+		}
+		if (periph->flags & CAM_PERIPH_INVALID) {
+			cam_periph_release_locked(periph);
+			return (ENXIO);
+		}
+	}
+
+	periph->flags |= CAM_PERIPH_LOCKED;
+	return (0);
+}
+
+void
+cam_periph_unhold(struct cam_periph *periph)
+{
+
+	cam_periph_assert(periph, MA_OWNED);
+
+	periph->flags &= ~CAM_PERIPH_LOCKED;
+	if ((periph->flags & CAM_PERIPH_LOCK_WANTED) != 0) {
+		periph->flags &= ~CAM_PERIPH_LOCK_WANTED;
+		wakeup(periph);
+	}
+
+	cam_periph_release_locked(periph);
+}
+
+/*
+ * Look for the next unit number that is not currently in use for this
+ * peripheral type starting at "newunit".  Also exclude unit numbers that
+ * are reserved by for future "hardwiring" unless we already know that this
+ * is a potential wired device.  Only assume that the device is "wired" the
+ * first time through the loop since after that we'll be looking at unit
+ * numbers that did not match a wiring entry.
+ */
+static u_int
+camperiphnextunit(struct periph_driver *p_drv, u_int newunit, int wired,
+		  path_id_t pathid, target_id_t target, lun_id_t lun)
+{
+	struct	cam_periph *periph;
+	char	*periph_name;
+	int	i, val, dunit, r;
+	const char *dname, *strval;
+
+	periph_name = p_drv->driver_name;
+	for (;;newunit++) {
+
+		for (periph = TAILQ_FIRST(&p_drv->units);
+		     periph != NULL && periph->unit_number != newunit;
+		     periph = TAILQ_NEXT(periph, unit_links))
+			;
+
+		if (periph != NULL && periph->unit_number == newunit) {
+			if (wired != 0) {
+				xpt_print(periph->path, "Duplicate Wired "
+				    "Device entry!\n");
+				xpt_print(periph->path, "Second device (%s "
+				    "device at scbus%d target %d lun %d) will "
+				    "not be wired\n", periph_name, pathid,
+				    target, lun);
+				wired = 0;
+			}
+			continue;
+		}
+		if (wired)
+			break;
+
+		/*
+		 * Don't match entries like "da 4" as a wired down
+		 * device, but do match entries like "da 4 target 5"
+		 * or even "da 4 scbus 1". 
+		 */
+		i = 0;
+		dname = periph_name;
+#ifndef __rtems__
+		for (;;) {
+			r = resource_find_dev(&i, dname, &dunit, NULL, NULL);
+			if (r != 0)
+				break;
+			/* if no "target" and no specific scbus, skip */
+			if (resource_int_value(dname, dunit, "target", &val) &&
+			    (resource_string_value(dname, dunit, "at",&strval)||
+			     strcmp(strval, "scbus") == 0))
+				continue;
+			if (newunit == dunit)
+				break;
+		}
+#else
+        r = 1;
+#endif
+		if (r != 0)
+			break;
+	}
+	return (newunit);
+}
+
+static u_int
+camperiphunit(struct periph_driver *p_drv, path_id_t pathid,
+	      target_id_t target, lun_id_t lun)
+{
+	u_int	unit;
+	int	wired, i, val, dunit;
+	const char *dname, *strval;
+	char	pathbuf[32], *periph_name;
+
+	periph_name = p_drv->driver_name;
+	snprintf(pathbuf, sizeof(pathbuf), "scbus%d", pathid);
+	unit = 0;
+	i = 0;
+	dname = periph_name;
+#ifndef __rtems__
+	for (wired = 0; resource_find_dev(&i, dname, &dunit, NULL, NULL) == 0;
+	     wired = 0) {
+		if (resource_string_value(dname, dunit, "at", &strval) == 0) {
+			if (strcmp(strval, pathbuf) != 0)
+				continue;
+			wired++;
+		}
+		if (resource_int_value(dname, dunit, "target", &val) == 0) {
+			if (val != target)
+				continue;
+			wired++;
+		}
+		if (resource_int_value(dname, dunit, "lun", &val) == 0) {
+			if (val != lun)
+				continue;
+			wired++;
+		}
+		if (wired != 0) {
+			unit = dunit;
+			break;
+		}
+	}
+#endif
+	/*
+	 * Either start from 0 looking for the next unit or from
+	 * the unit number given in the resource config.  This way,
+	 * if we have wildcard matches, we don't return the same
+	 * unit number twice.
+	 */
+	unit = camperiphnextunit(p_drv, unit, wired, pathid, target, lun);
+
+	return (unit);
+}
+
+void
+cam_periph_invalidate(struct cam_periph *periph)
+{
+
+	cam_periph_assert(periph, MA_OWNED);
+	/*
+	 * We only call this routine the first time a peripheral is
+	 * invalidated.
+	 */
+	if ((periph->flags & CAM_PERIPH_INVALID) != 0)
+		return;
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_INFO, ("Periph invalidated\n"));
+	if ((periph->flags & CAM_PERIPH_ANNOUNCED) && !rebooting)
+		xpt_denounce_periph(periph);
+	periph->flags |= CAM_PERIPH_INVALID;
+	periph->flags &= ~CAM_PERIPH_NEW_DEV_FOUND;
+	if (periph->periph_oninval != NULL)
+		periph->periph_oninval(periph);
+	cam_periph_release_locked(periph);
+}
+
+static void
+camperiphfree(struct cam_periph *periph)
+{
+	struct periph_driver **p_drv;
+
+	cam_periph_assert(periph, MA_OWNED);
+	KASSERT(periph->periph_allocating == 0, ("%s%d: freed while allocating",
+	    periph->periph_name, periph->unit_number));
+	for (p_drv = periph_drivers; *p_drv != NULL; p_drv++) {
+		if (strcmp((*p_drv)->driver_name, periph->periph_name) == 0)
+			break;
+	}
+	if (*p_drv == NULL) {
+		printf("camperiphfree: attempt to free non-existant periph\n");
+		return;
+	}
+
+	/*
+	 * We need to set this flag before dropping the topology lock, to
+	 * let anyone who is traversing the list that this peripheral is
+	 * about to be freed, and there will be no more reference count
+	 * checks.
+	 */
+	periph->flags |= CAM_PERIPH_FREE;
+
+	/*
+	 * The peripheral destructor semantics dictate calling with only the
+	 * SIM mutex held.  Since it might sleep, it should not be called
+	 * with the topology lock held.
+	 */
+	xpt_unlock_buses();
+
+	/*
+	 * We need to call the peripheral destructor prior to removing the
+	 * peripheral from the list.  Otherwise, we risk running into a
+	 * scenario where the peripheral unit number may get reused
+	 * (because it has been removed from the list), but some resources
+	 * used by the peripheral are still hanging around.  In particular,
+	 * the devfs nodes used by some peripherals like the pass(4) driver
+	 * aren't fully cleaned up until the destructor is run.  If the
+	 * unit number is reused before the devfs instance is fully gone,
+	 * devfs will panic.
+	 */
+	if (periph->periph_dtor != NULL)
+		periph->periph_dtor(periph);
+
+	/*
+	 * The peripheral list is protected by the topology lock.
+	 */
+	xpt_lock_buses();
+
+	TAILQ_REMOVE(&(*p_drv)->units, periph, unit_links);
+	(*p_drv)->generation++;
+
+	xpt_remove_periph(periph);
+
+	xpt_unlock_buses();
+	if ((periph->flags & CAM_PERIPH_ANNOUNCED) && !rebooting)
+		xpt_print(periph->path, "Periph destroyed\n");
+	else
+		CAM_DEBUG(periph->path, CAM_DEBUG_INFO, ("Periph destroyed\n"));
+
+	if (periph->flags & CAM_PERIPH_NEW_DEV_FOUND) {
+		union ccb ccb;
+		void *arg;
+
+		switch (periph->deferred_ac) {
+		case AC_FOUND_DEVICE:
+			ccb.ccb_h.func_code = XPT_GDEV_TYPE;
+			xpt_setup_ccb(&ccb.ccb_h, periph->path, CAM_PRIORITY_NORMAL);
+			xpt_action(&ccb);
+			arg = &ccb;
+			break;
+		case AC_PATH_REGISTERED:
+			ccb.ccb_h.func_code = XPT_PATH_INQ;
+			xpt_setup_ccb(&ccb.ccb_h, periph->path, CAM_PRIORITY_NORMAL);
+			xpt_action(&ccb);
+			arg = &ccb;
+			break;
+		default:
+			arg = NULL;
+			break;
+		}
+		periph->deferred_callback(NULL, periph->deferred_ac,
+					  periph->path, arg);
+	}
+	xpt_free_path(periph->path);
+	free(periph, M_CAMPERIPH);
+	xpt_lock_buses();
+}
+
+/*
+ * Map user virtual pointers into kernel virtual address space, so we can
+ * access the memory.  This is now a generic function that centralizes most
+ * of the sanity checks on the data flags, if any.
+ * This also only works for up to MAXPHYS memory.  Since we use
+ * buffers to map stuff in and out, we're limited to the buffer size.
+ */
+#ifndef __rtems__
+int
+cam_periph_mapmem(union ccb *ccb, struct cam_periph_map_info *mapinfo,
+    u_int maxmap)
+{
+	int numbufs, i, j;
+	int flags[CAM_PERIPH_MAXMAPS];
+	u_int8_t **data_ptrs[CAM_PERIPH_MAXMAPS];
+	u_int32_t lengths[CAM_PERIPH_MAXMAPS];
+	u_int32_t dirs[CAM_PERIPH_MAXMAPS];
+
+	if (maxmap == 0)
+		maxmap = DFLTPHYS;	/* traditional default */
+	else if (maxmap > MAXPHYS)
+		maxmap = MAXPHYS;	/* for safety */
+	switch(ccb->ccb_h.func_code) {
+	case XPT_DEV_MATCH:
+		if (ccb->cdm.match_buf_len == 0) {
+			printf("cam_periph_mapmem: invalid match buffer "
+			       "length 0\n");
+			return(EINVAL);
+		}
+		if (ccb->cdm.pattern_buf_len > 0) {
+			data_ptrs[0] = (u_int8_t **)&ccb->cdm.patterns;
+			lengths[0] = ccb->cdm.pattern_buf_len;
+			dirs[0] = CAM_DIR_OUT;
+			data_ptrs[1] = (u_int8_t **)&ccb->cdm.matches;
+			lengths[1] = ccb->cdm.match_buf_len;
+			dirs[1] = CAM_DIR_IN;
+			numbufs = 2;
+		} else {
+			data_ptrs[0] = (u_int8_t **)&ccb->cdm.matches;
+			lengths[0] = ccb->cdm.match_buf_len;
+			dirs[0] = CAM_DIR_IN;
+			numbufs = 1;
+		}
+		/*
+		 * This request will not go to the hardware, no reason
+		 * to be so strict. vmapbuf() is able to map up to MAXPHYS.
+		 */
+		maxmap = MAXPHYS;
+		break;
+	case XPT_SCSI_IO:
+	case XPT_CONT_TARGET_IO:
+		if ((ccb->ccb_h.flags & CAM_DIR_MASK) == CAM_DIR_NONE)
+			return(0);
+		if ((ccb->ccb_h.flags & CAM_DATA_MASK) != CAM_DATA_VADDR)
+			return (EINVAL);
+		data_ptrs[0] = &ccb->csio.data_ptr;
+		lengths[0] = ccb->csio.dxfer_len;
+		dirs[0] = ccb->ccb_h.flags & CAM_DIR_MASK;
+		numbufs = 1;
+		break;
+	case XPT_ATA_IO:
+		if ((ccb->ccb_h.flags & CAM_DIR_MASK) == CAM_DIR_NONE)
+			return(0);
+		if ((ccb->ccb_h.flags & CAM_DATA_MASK) != CAM_DATA_VADDR)
+			return (EINVAL);
+		data_ptrs[0] = &ccb->ataio.data_ptr;
+		lengths[0] = ccb->ataio.dxfer_len;
+		dirs[0] = ccb->ccb_h.flags & CAM_DIR_MASK;
+		numbufs = 1;
+		break;
+	case XPT_MMC_IO:
+		if ((ccb->ccb_h.flags & CAM_DIR_MASK) == CAM_DIR_NONE)
+			return(0);
+		/* Two mappings: one for cmd->data and one for cmd->data->data */
+		data_ptrs[0] = (unsigned char **)&ccb->mmcio.cmd.data;
+		lengths[0] = sizeof(struct mmc_data *);
+		dirs[0] = ccb->ccb_h.flags & CAM_DIR_MASK;
+		data_ptrs[1] = (unsigned char **)&ccb->mmcio.cmd.data->data;
+		lengths[1] = ccb->mmcio.cmd.data->len;
+		dirs[1] = ccb->ccb_h.flags & CAM_DIR_MASK;
+		numbufs = 2;
+		break;
+	case XPT_SMP_IO:
+		data_ptrs[0] = &ccb->smpio.smp_request;
+		lengths[0] = ccb->smpio.smp_request_len;
+		dirs[0] = CAM_DIR_OUT;
+		data_ptrs[1] = &ccb->smpio.smp_response;
+		lengths[1] = ccb->smpio.smp_response_len;
+		dirs[1] = CAM_DIR_IN;
+		numbufs = 2;
+		break;
+	case XPT_DEV_ADVINFO:
+		if (ccb->cdai.bufsiz == 0)
+			return (0);
+
+		data_ptrs[0] = (uint8_t **)&ccb->cdai.buf;
+		lengths[0] = ccb->cdai.bufsiz;
+		dirs[0] = CAM_DIR_IN;
+		numbufs = 1;
+
+		/*
+		 * This request will not go to the hardware, no reason
+		 * to be so strict. vmapbuf() is able to map up to MAXPHYS.
+		 */
+		maxmap = MAXPHYS;
+		break;
+	default:
+		return(EINVAL);
+		break; /* NOTREACHED */
+	}
+
+	/*
+	 * Check the transfer length and permissions first, so we don't
+	 * have to unmap any previously mapped buffers.
+	 */
+	for (i = 0; i < numbufs; i++) {
+
+		flags[i] = 0;
+
+		/*
+		 * The userland data pointer passed in may not be page
+		 * aligned.  vmapbuf() truncates the address to a page
+		 * boundary, so if the address isn't page aligned, we'll
+		 * need enough space for the given transfer length, plus
+		 * whatever extra space is necessary to make it to the page
+		 * boundary.
+		 */
+		if ((lengths[i] +
+		    (((vm_offset_t)(*data_ptrs[i])) & PAGE_MASK)) > maxmap){
+			printf("cam_periph_mapmem: attempt to map %lu bytes, "
+			       "which is greater than %lu\n",
+			       (long)(lengths[i] +
+			       (((vm_offset_t)(*data_ptrs[i])) & PAGE_MASK)),
+			       (u_long)maxmap);
+			return(E2BIG);
+		}
+
+		if (dirs[i] & CAM_DIR_OUT) {
+			flags[i] = BIO_WRITE;
+		}
+
+		if (dirs[i] & CAM_DIR_IN) {
+			flags[i] = BIO_READ;
+		}
+
+	}
+
+	/*
+	 * This keeps the kernel stack of current thread from getting
+	 * swapped.  In low-memory situations where the kernel stack might
+	 * otherwise get swapped out, this holds it and allows the thread
+	 * to make progress and release the kernel mapped pages sooner.
+	 *
+	 * XXX KDM should I use P_NOSWAP instead?
+	 */
+	PHOLD(curproc);
+
+	for (i = 0; i < numbufs; i++) {
+		/*
+		 * Get the buffer.
+		 */
+		mapinfo->bp[i] = getpbuf(NULL);
+
+		/* put our pointer in the data slot */
+		mapinfo->bp[i]->b_data = *data_ptrs[i];
+
+		/* save the user's data address */
+		mapinfo->bp[i]->b_caller1 = *data_ptrs[i];
+
+		/* set the transfer length, we know it's < MAXPHYS */
+		mapinfo->bp[i]->b_bufsize = lengths[i];
+
+		/* set the direction */
+		mapinfo->bp[i]->b_iocmd = flags[i];
+
+		/*
+		 * Map the buffer into kernel memory.
+		 *
+		 * Note that useracc() alone is not a  sufficient test.
+		 * vmapbuf() can still fail due to a smaller file mapped
+		 * into a larger area of VM, or if userland races against
+		 * vmapbuf() after the useracc() check.
+		 */
+		if (vmapbuf(mapinfo->bp[i], 1) < 0) {
+			for (j = 0; j < i; ++j) {
+				*data_ptrs[j] = mapinfo->bp[j]->b_caller1;
+				vunmapbuf(mapinfo->bp[j]);
+				relpbuf(mapinfo->bp[j], NULL);
+			}
+			relpbuf(mapinfo->bp[i], NULL);
+			PRELE(curproc);
+			return(EACCES);
+		}
+
+		/* set our pointer to the new mapped area */
+		*data_ptrs[i] = mapinfo->bp[i]->b_data;
+
+		mapinfo->num_bufs_used++;
+	}
+
+	/*
+	 * Now that we've gotten this far, change ownership to the kernel
+	 * of the buffers so that we don't run afoul of returning to user
+	 * space with locks (on the buffer) held.
+	 */
+	for (i = 0; i < numbufs; i++) {
+		BUF_KERNPROC(mapinfo->bp[i]);
+	}
+
+
+	return(0);
+}
+
+/*
+ * Unmap memory segments mapped into kernel virtual address space by
+ * cam_periph_mapmem().
+ */
+void
+cam_periph_unmapmem(union ccb *ccb, struct cam_periph_map_info *mapinfo)
+{
+	int numbufs, i;
+	u_int8_t **data_ptrs[CAM_PERIPH_MAXMAPS];
+
+	if (mapinfo->num_bufs_used <= 0) {
+		/* nothing to free and the process wasn't held. */
+		return;
+	}
+
+	switch (ccb->ccb_h.func_code) {
+	case XPT_DEV_MATCH:
+		numbufs = min(mapinfo->num_bufs_used, 2);
+
+		if (numbufs == 1) {
+			data_ptrs[0] = (u_int8_t **)&ccb->cdm.matches;
+		} else {
+			data_ptrs[0] = (u_int8_t **)&ccb->cdm.patterns;
+			data_ptrs[1] = (u_int8_t **)&ccb->cdm.matches;
+		}
+		break;
+	case XPT_SCSI_IO:
+	case XPT_CONT_TARGET_IO:
+		data_ptrs[0] = &ccb->csio.data_ptr;
+		numbufs = min(mapinfo->num_bufs_used, 1);
+		break;
+	case XPT_ATA_IO:
+		data_ptrs[0] = &ccb->ataio.data_ptr;
+		numbufs = min(mapinfo->num_bufs_used, 1);
+		break;
+	case XPT_SMP_IO:
+		numbufs = min(mapinfo->num_bufs_used, 2);
+		data_ptrs[0] = &ccb->smpio.smp_request;
+		data_ptrs[1] = &ccb->smpio.smp_response;
+		break;
+	case XPT_DEV_ADVINFO:
+		numbufs = min(mapinfo->num_bufs_used, 1);
+		data_ptrs[0] = (uint8_t **)&ccb->cdai.buf;
+		break;
+	default:
+		/* allow ourselves to be swapped once again */
+		PRELE(curproc);
+		return;
+		break; /* NOTREACHED */ 
+	}
+
+	for (i = 0; i < numbufs; i++) {
+		/* Set the user's pointer back to the original value */
+		*data_ptrs[i] = mapinfo->bp[i]->b_caller1;
+
+		/* unmap the buffer */
+		vunmapbuf(mapinfo->bp[i]);
+
+		/* release the buffer */
+		relpbuf(mapinfo->bp[i], NULL);
+	}
+
+	/* allow ourselves to be swapped once again */
+	PRELE(curproc);
+}
+
+#endif
+int
+cam_periph_ioctl(struct cam_periph *periph, u_long cmd, caddr_t addr,
+		 int (*error_routine)(union ccb *ccb, 
+				      cam_flags camflags,
+				      u_int32_t sense_flags))
+{
+	union ccb 	     *ccb;
+	int 		     error;
+	int		     found;
+
+	error = found = 0;
+
+	switch(cmd){
+	case CAMGETPASSTHRU:
+		ccb = cam_periph_getccb(periph, CAM_PRIORITY_NORMAL);
+		xpt_setup_ccb(&ccb->ccb_h,
+			      ccb->ccb_h.path,
+			      CAM_PRIORITY_NORMAL);
+		ccb->ccb_h.func_code = XPT_GDEVLIST;
+
+		/*
+		 * Basically, the point of this is that we go through
+		 * getting the list of devices, until we find a passthrough
+		 * device.  In the current version of the CAM code, the
+		 * only way to determine what type of device we're dealing
+		 * with is by its name.
+		 */
+		while (found == 0) {
+			ccb->cgdl.index = 0;
+			ccb->cgdl.status = CAM_GDEVLIST_MORE_DEVS;
+			while (ccb->cgdl.status == CAM_GDEVLIST_MORE_DEVS) {
+
+				/* we want the next device in the list */
+				xpt_action(ccb);
+				if (strncmp(ccb->cgdl.periph_name, 
+				    "pass", 4) == 0){
+					found = 1;
+					break;
+				}
+			}
+			if ((ccb->cgdl.status == CAM_GDEVLIST_LAST_DEVICE) &&
+			    (found == 0)) {
+				ccb->cgdl.periph_name[0] = '\0';
+				ccb->cgdl.unit_number = 0;
+				break;
+			}
+		}
+
+		/* copy the result back out */	
+		bcopy(ccb, addr, sizeof(union ccb));
+
+		/* and release the ccb */
+		xpt_release_ccb(ccb);
+
+		break;
+	default:
+		error = ENOTTY;
+		break;
+	}
+	return(error);
+}
+
+static void
+cam_periph_done_panic(struct cam_periph *periph, union ccb *done_ccb)
+{
+
+	panic("%s: already done with ccb %p", __func__, done_ccb);
+}
+
+static void
+cam_periph_done(struct cam_periph *periph, union ccb *done_ccb)
+{
+
+	/* Caller will release the CCB */
+	xpt_path_assert(done_ccb->ccb_h.path, MA_OWNED);
+	done_ccb->ccb_h.cbfcnp = cam_periph_done_panic;
+	wakeup(&done_ccb->ccb_h.cbfcnp);
+}
+
+static void
+cam_periph_ccbwait(union ccb *ccb)
+{
+
+	if ((ccb->ccb_h.func_code & XPT_FC_QUEUED) != 0) {
+		while (ccb->ccb_h.cbfcnp != cam_periph_done_panic)
+			xpt_path_sleep(ccb->ccb_h.path, &ccb->ccb_h.cbfcnp,
+			    PRIBIO, "cbwait", 0);
+	}
+	KASSERT(ccb->ccb_h.pinfo.index == CAM_UNQUEUED_INDEX &&
+	    (ccb->ccb_h.status & CAM_STATUS_MASK) != CAM_REQ_INPROG,
+	    ("%s: proceeding with incomplete ccb: ccb=%p, func_code=%#x, "
+	     "status=%#x, index=%d", __func__, ccb, ccb->ccb_h.func_code,
+	     ccb->ccb_h.status, ccb->ccb_h.pinfo.index));
+}
+
+int
+cam_periph_runccb(union ccb *ccb,
+		  int (*error_routine)(union ccb *ccb,
+				       cam_flags camflags,
+				       u_int32_t sense_flags),
+		  cam_flags camflags, u_int32_t sense_flags,
+		  struct devstat *ds)
+{
+	struct bintime *starttime;
+	struct bintime ltime;
+	int error;
+ 
+	starttime = NULL;
+	xpt_path_assert(ccb->ccb_h.path, MA_OWNED);
+	KASSERT((ccb->ccb_h.flags & CAM_UNLOCKED) == 0,
+	    ("%s: ccb=%p, func_code=%#x, flags=%#x", __func__, ccb,
+	     ccb->ccb_h.func_code, ccb->ccb_h.flags));
+#ifndef __rtems__
+	/*
+	 * If the user has supplied a stats structure, and if we understand
+	 * this particular type of ccb, record the transaction start.
+	 */
+	if ((ds != NULL) && (ccb->ccb_h.func_code == XPT_SCSI_IO ||
+	    ccb->ccb_h.func_code == XPT_ATA_IO)) {
+		starttime = &ltime;
+		binuptime(starttime);
+		devstat_start_transaction(ds, starttime);
+	}
+#endif
+	ccb->ccb_h.cbfcnp = cam_periph_done;
+	xpt_action(ccb);
+ 
+	do {
+		cam_periph_ccbwait(ccb);
+		if ((ccb->ccb_h.status & CAM_STATUS_MASK) == CAM_REQ_CMP)
+			error = 0;
+		else if (error_routine != NULL) {
+			ccb->ccb_h.cbfcnp = cam_periph_done;
+			error = (*error_routine)(ccb, camflags, sense_flags);
+		} else
+			error = 0;
+
+	} while (error == ERESTART);
+          
+	if ((ccb->ccb_h.status & CAM_DEV_QFRZN) != 0) {
+		cam_release_devq(ccb->ccb_h.path,
+				 /* relsim_flags */0,
+				 /* openings */0,
+				 /* timeout */0,
+				 /* getcount_only */ FALSE);
+		ccb->ccb_h.status &= ~CAM_DEV_QFRZN;
+	}
+#ifndef __rtems__
+	if (ds != NULL) {
+		if (ccb->ccb_h.func_code == XPT_SCSI_IO) {
+			devstat_end_transaction(ds,
+					ccb->csio.dxfer_len - ccb->csio.resid,
+					ccb->csio.tag_action & 0x3,
+					((ccb->ccb_h.flags & CAM_DIR_MASK) ==
+					CAM_DIR_NONE) ?  DEVSTAT_NO_DATA : 
+					(ccb->ccb_h.flags & CAM_DIR_OUT) ?
+					DEVSTAT_WRITE : 
+					DEVSTAT_READ, NULL, starttime);
+		} else if (ccb->ccb_h.func_code == XPT_ATA_IO) {
+			devstat_end_transaction(ds,
+					ccb->ataio.dxfer_len - ccb->ataio.resid,
+					0, /* Not used in ATA */
+					((ccb->ccb_h.flags & CAM_DIR_MASK) ==
+					CAM_DIR_NONE) ?  DEVSTAT_NO_DATA : 
+					(ccb->ccb_h.flags & CAM_DIR_OUT) ?
+					DEVSTAT_WRITE : 
+					DEVSTAT_READ, NULL, starttime);
+		}
+	}
+#endif
+	return(error);
+}
+
+void
+cam_freeze_devq(struct cam_path *path)
+{
+	struct ccb_hdr ccb_h;
+
+	CAM_DEBUG(path, CAM_DEBUG_TRACE, ("cam_freeze_devq\n"));
+	xpt_setup_ccb(&ccb_h, path, /*priority*/1);
+	ccb_h.func_code = XPT_NOOP;
+	ccb_h.flags = CAM_DEV_QFREEZE;
+	xpt_action((union ccb *)&ccb_h);
+}
+
+u_int32_t
+cam_release_devq(struct cam_path *path, u_int32_t relsim_flags,
+		 u_int32_t openings, u_int32_t arg,
+		 int getcount_only)
+{
+	struct ccb_relsim crs;
+
+	CAM_DEBUG(path, CAM_DEBUG_TRACE, ("cam_release_devq(%u, %u, %u, %d)\n",
+	    relsim_flags, openings, arg, getcount_only));
+	xpt_setup_ccb(&crs.ccb_h, path, CAM_PRIORITY_NORMAL);
+	crs.ccb_h.func_code = XPT_REL_SIMQ;
+	crs.ccb_h.flags = getcount_only ? CAM_DEV_QFREEZE : 0;
+	crs.release_flags = relsim_flags;
+	crs.openings = openings;
+	crs.release_timeout = arg;
+	xpt_action((union ccb *)&crs);
+	return (crs.qfrozen_cnt);
+}
+
+#define saved_ccb_ptr ppriv_ptr0
+static void
+camperiphdone(struct cam_periph *periph, union ccb *done_ccb)
+{
+	union ccb      *saved_ccb;
+	cam_status	status;
+	struct scsi_start_stop_unit *scsi_cmd;
+	int    error_code, sense_key, asc, ascq;
+
+	scsi_cmd = (struct scsi_start_stop_unit *)
+	    &done_ccb->csio.cdb_io.cdb_bytes;
+	status = done_ccb->ccb_h.status;
+
+	if ((status & CAM_STATUS_MASK) != CAM_REQ_CMP) {
+		if (scsi_extract_sense_ccb(done_ccb,
+		    &error_code, &sense_key, &asc, &ascq)) {
+			/*
+			 * If the error is "invalid field in CDB",
+			 * and the load/eject flag is set, turn the
+			 * flag off and try again.  This is just in
+			 * case the drive in question barfs on the
+			 * load eject flag.  The CAM code should set
+			 * the load/eject flag by default for
+			 * removable media.
+			 */
+			if ((scsi_cmd->opcode == START_STOP_UNIT) &&
+			    ((scsi_cmd->how & SSS_LOEJ) != 0) &&
+			     (asc == 0x24) && (ascq == 0x00)) {
+				scsi_cmd->how &= ~SSS_LOEJ;
+				if (status & CAM_DEV_QFRZN) {
+					cam_release_devq(done_ccb->ccb_h.path,
+					    0, 0, 0, 0);
+					done_ccb->ccb_h.status &=
+					    ~CAM_DEV_QFRZN;
+				}
+				xpt_action(done_ccb);
+				goto out;
+			}
+		}
+		if (cam_periph_error(done_ccb,
+		    0, SF_RETRY_UA | SF_NO_PRINT) == ERESTART)
+			goto out;
+		if (done_ccb->ccb_h.status & CAM_DEV_QFRZN) {
+			cam_release_devq(done_ccb->ccb_h.path, 0, 0, 0, 0);
+			done_ccb->ccb_h.status &= ~CAM_DEV_QFRZN;
+		}
+	} else {
+		/*
+		 * If we have successfully taken a device from the not
+		 * ready to ready state, re-scan the device and re-get
+		 * the inquiry information.  Many devices (mostly disks)
+		 * don't properly report their inquiry information unless
+		 * they are spun up.
+		 */
+		if (scsi_cmd->opcode == START_STOP_UNIT)
+			xpt_async(AC_INQ_CHANGED, done_ccb->ccb_h.path, NULL);
+	}
+
+	/*
+	 * Perform the final retry with the original CCB so that final
+	 * error processing is performed by the owner of the CCB.
+	 */
+	saved_ccb = (union ccb *)done_ccb->ccb_h.saved_ccb_ptr;
+	bcopy(saved_ccb, done_ccb, sizeof(*done_ccb));
+	xpt_free_ccb(saved_ccb);
+	if (done_ccb->ccb_h.cbfcnp != camperiphdone)
+		periph->flags &= ~CAM_PERIPH_RECOVERY_INPROG;
+	xpt_action(done_ccb);
+
+out:
+	/* Drop freeze taken due to CAM_DEV_QFREEZE flag set. */
+	cam_release_devq(done_ccb->ccb_h.path, 0, 0, 0, 0);
+}
+
+/*
+ * Generic Async Event handler.  Peripheral drivers usually
+ * filter out the events that require personal attention,
+ * and leave the rest to this function.
+ */
+void
+cam_periph_async(struct cam_periph *periph, u_int32_t code,
+		 struct cam_path *path, void *arg)
+{
+	switch (code) {
+	case AC_LOST_DEVICE:
+		cam_periph_invalidate(periph);
+		break; 
+	default:
+		break;
+	}
+}
+
+void
+cam_periph_bus_settle(struct cam_periph *periph, u_int bus_settle)
+{
+	struct ccb_getdevstats cgds;
+
+	xpt_setup_ccb(&cgds.ccb_h, periph->path, CAM_PRIORITY_NORMAL);
+	cgds.ccb_h.func_code = XPT_GDEV_STATS;
+	xpt_action((union ccb *)&cgds);
+	cam_periph_freeze_after_event(periph, &cgds.last_reset, bus_settle);
+}
+
+void
+cam_periph_freeze_after_event(struct cam_periph *periph,
+			      struct timeval* event_time, u_int duration_ms)
+{
+	struct timeval delta;
+	struct timeval duration_tv;
+
+	if (!timevalisset(event_time))
+		return;
+
+	microtime(&delta);
+	timevalsub(&delta, event_time);
+	duration_tv.tv_sec = duration_ms / 1000;
+	duration_tv.tv_usec = (duration_ms % 1000) * 1000;
+	if (timevalcmp(&delta, &duration_tv, <)) {
+		timevalsub(&duration_tv, &delta);
+
+		duration_ms = duration_tv.tv_sec * 1000;
+		duration_ms += duration_tv.tv_usec / 1000;
+		cam_freeze_devq(periph->path); 
+		cam_release_devq(periph->path,
+				RELSIM_RELEASE_AFTER_TIMEOUT,
+				/*reduction*/0,
+				/*timeout*/duration_ms,
+				/*getcount_only*/0);
+	}
+
+}
+
+static int
+camperiphscsistatuserror(union ccb *ccb, union ccb **orig_ccb,
+    cam_flags camflags, u_int32_t sense_flags,
+    int *openings, u_int32_t *relsim_flags,
+    u_int32_t *timeout, u_int32_t *action, const char **action_string)
+{
+#ifndef __rtems__
+	int error;
+
+	switch (ccb->csio.scsi_status) {
+	case SCSI_STATUS_OK:
+	case SCSI_STATUS_COND_MET:
+	case SCSI_STATUS_INTERMED:
+	case SCSI_STATUS_INTERMED_COND_MET:
+		error = 0;
+		break;
+	case SCSI_STATUS_CMD_TERMINATED:
+	case SCSI_STATUS_CHECK_COND:
+		error = camperiphscsisenseerror(ccb, orig_ccb,
+					        camflags,
+					        sense_flags,
+					        openings,
+					        relsim_flags,
+					        timeout,
+					        action,
+					        action_string);
+		break;
+	case SCSI_STATUS_QUEUE_FULL:
+	{
+		/* no decrement */
+		struct ccb_getdevstats cgds;
+
+		/*
+		 * First off, find out what the current
+		 * transaction counts are.
+		 */
+		xpt_setup_ccb(&cgds.ccb_h,
+			      ccb->ccb_h.path,
+			      CAM_PRIORITY_NORMAL);
+		cgds.ccb_h.func_code = XPT_GDEV_STATS;
+		xpt_action((union ccb *)&cgds);
+
+		/*
+		 * If we were the only transaction active, treat
+		 * the QUEUE FULL as if it were a BUSY condition.
+		 */
+		if (cgds.dev_active != 0) {
+			int total_openings;
+
+			/*
+		 	 * Reduce the number of openings to
+			 * be 1 less than the amount it took
+			 * to get a queue full bounded by the
+			 * minimum allowed tag count for this
+			 * device.
+		 	 */
+			total_openings = cgds.dev_active + cgds.dev_openings;
+			*openings = cgds.dev_active;
+			if (*openings < cgds.mintags)
+				*openings = cgds.mintags;
+			if (*openings < total_openings)
+				*relsim_flags = RELSIM_ADJUST_OPENINGS;
+			else {
+				/*
+				 * Some devices report queue full for
+				 * temporary resource shortages.  For
+				 * this reason, we allow a minimum
+				 * tag count to be entered via a
+				 * quirk entry to prevent the queue
+				 * count on these devices from falling
+				 * to a pessimisticly low value.  We
+				 * still wait for the next successful
+				 * completion, however, before queueing
+				 * more transactions to the device.
+				 */
+				*relsim_flags = RELSIM_RELEASE_AFTER_CMDCMPLT;
+			}
+			*timeout = 0;
+			error = ERESTART;
+			*action &= ~SSQ_PRINT_SENSE;
+			break;
+		}
+		/* FALLTHROUGH */
+	}
+	case SCSI_STATUS_BUSY:
+		/*
+		 * Restart the queue after either another
+		 * command completes or a 1 second timeout.
+		 */
+		if ((sense_flags & SF_RETRY_BUSY) != 0 ||
+		    (ccb->ccb_h.retry_count--) > 0) {
+			error = ERESTART;
+			*relsim_flags = RELSIM_RELEASE_AFTER_TIMEOUT
+				      | RELSIM_RELEASE_AFTER_CMDCMPLT;
+			*timeout = 1000;
+		} else {
+			error = EIO;
+		}
+		break;
+	case SCSI_STATUS_RESERV_CONFLICT:
+	default:
+		error = EIO;
+		break;
+	}
+	return (error);
+#else /* __rtems__ */
+    return 0;
+#endif /* __rtems__ */
+}
+#ifndef __rtems__
+static int
+camperiphscsisenseerror(union ccb *ccb, union ccb **orig,
+    cam_flags camflags, u_int32_t sense_flags,
+    int *openings, u_int32_t *relsim_flags,
+    u_int32_t *timeout, u_int32_t *action, const char **action_string)
+{
+	struct cam_periph *periph;
+	union ccb *orig_ccb = ccb;
+	int error, recoveryccb;
+
+#if defined(BUF_TRACKING) || defined(FULL_BUF_TRACKING)
+	if (ccb->ccb_h.func_code == XPT_SCSI_IO && ccb->csio.bio != NULL)
+		biotrack(ccb->csio.bio, __func__);
+#endif
+
+	periph = xpt_path_periph(ccb->ccb_h.path);
+	recoveryccb = (ccb->ccb_h.cbfcnp == camperiphdone);
+	if ((periph->flags & CAM_PERIPH_RECOVERY_INPROG) && !recoveryccb) {
+		/*
+		 * If error recovery is already in progress, don't attempt
+		 * to process this error, but requeue it unconditionally
+		 * and attempt to process it once error recovery has
+		 * completed.  This failed command is probably related to
+		 * the error that caused the currently active error recovery
+		 * action so our  current recovery efforts should also
+		 * address this command.  Be aware that the error recovery
+		 * code assumes that only one recovery action is in progress
+		 * on a particular peripheral instance at any given time
+		 * (e.g. only one saved CCB for error recovery) so it is
+		 * imperitive that we don't violate this assumption.
+		 */
+		error = ERESTART;
+		*action &= ~SSQ_PRINT_SENSE;
+	} else {
+		scsi_sense_action err_action;
+		struct ccb_getdev cgd;
+
+		/*
+		 * Grab the inquiry data for this device.
+		 */
+		xpt_setup_ccb(&cgd.ccb_h, ccb->ccb_h.path, CAM_PRIORITY_NORMAL);
+		cgd.ccb_h.func_code = XPT_GDEV_TYPE;
+		xpt_action((union ccb *)&cgd);
+
+		err_action = scsi_error_action(&ccb->csio, &cgd.inq_data,
+		    sense_flags);
+		error = err_action & SS_ERRMASK;
+
+		/*
+		 * Do not autostart sequential access devices
+		 * to avoid unexpected tape loading.
+		 */
+		if ((err_action & SS_MASK) == SS_START &&
+		    SID_TYPE(&cgd.inq_data) == T_SEQUENTIAL) {
+			*action_string = "Will not autostart a "
+			    "sequential access device";
+			goto sense_error_done;
+		}
+
+		/*
+		 * Avoid recovery recursion if recovery action is the same.
+		 */
+		if ((err_action & SS_MASK) >= SS_START && recoveryccb) {
+			if (((err_action & SS_MASK) == SS_START &&
+			     ccb->csio.cdb_io.cdb_bytes[0] == START_STOP_UNIT) ||
+			    ((err_action & SS_MASK) == SS_TUR &&
+			     (ccb->csio.cdb_io.cdb_bytes[0] == TEST_UNIT_READY))) {
+				err_action = SS_RETRY|SSQ_DECREMENT_COUNT|EIO;
+				*relsim_flags = RELSIM_RELEASE_AFTER_TIMEOUT;
+				*timeout = 500;
+			}
+		}
+
+		/*
+		 * If the recovery action will consume a retry,
+		 * make sure we actually have retries available.
+		 */
+		if ((err_action & SSQ_DECREMENT_COUNT) != 0) {
+		 	if (ccb->ccb_h.retry_count > 0 &&
+			    (periph->flags & CAM_PERIPH_INVALID) == 0)
+		 		ccb->ccb_h.retry_count--;
+			else {
+				*action_string = "Retries exhausted";
+				goto sense_error_done;
+			}
+		}
+
+		if ((err_action & SS_MASK) >= SS_START) {
+			/*
+			 * Do common portions of commands that
+			 * use recovery CCBs.
+			 */
+			orig_ccb = xpt_alloc_ccb_nowait();
+			if (orig_ccb == NULL) {
+				*action_string = "Can't allocate recovery CCB";
+				goto sense_error_done;
+			}
+			/*
+			 * Clear freeze flag for original request here, as
+			 * this freeze will be dropped as part of ERESTART.
+			 */
+			ccb->ccb_h.status &= ~CAM_DEV_QFRZN;
+			bcopy(ccb, orig_ccb, sizeof(*orig_ccb));
+		}
+
+		switch (err_action & SS_MASK) {
+		case SS_NOP:
+			*action_string = "No recovery action needed";
+			error = 0;
+			break;
+		case SS_RETRY:
+			*action_string = "Retrying command (per sense data)";
+			error = ERESTART;
+			break;
+		case SS_FAIL:
+			*action_string = "Unretryable error";
+			break;
+		case SS_START:
+		{
+			int le;
+
+			/*
+			 * Send a start unit command to the device, and
+			 * then retry the command.
+			 */
+			*action_string = "Attempting to start unit";
+			periph->flags |= CAM_PERIPH_RECOVERY_INPROG;
+
+			/*
+			 * Check for removable media and set
+			 * load/eject flag appropriately.
+			 */
+			if (SID_IS_REMOVABLE(&cgd.inq_data))
+				le = TRUE;
+			else
+				le = FALSE;
+
+			scsi_start_stop(&ccb->csio,
+					/*retries*/1,
+					camperiphdone,
+					MSG_SIMPLE_Q_TAG,
+					/*start*/TRUE,
+					/*load/eject*/le,
+					/*immediate*/FALSE,
+					SSD_FULL_SIZE,
+					/*timeout*/50000);
+			break;
+		}
+		case SS_TUR:
+		{
+			/*
+			 * Send a Test Unit Ready to the device.
+			 * If the 'many' flag is set, we send 120
+			 * test unit ready commands, one every half 
+			 * second.  Otherwise, we just send one TUR.
+			 * We only want to do this if the retry 
+			 * count has not been exhausted.
+			 */
+			int retries;
+
+			if ((err_action & SSQ_MANY) != 0) {
+				*action_string = "Polling device for readiness";
+				retries = 120;
+			} else {
+				*action_string = "Testing device for readiness";
+				retries = 1;
+			}
+			periph->flags |= CAM_PERIPH_RECOVERY_INPROG;
+			scsi_test_unit_ready(&ccb->csio,
+					     retries,
+					     camperiphdone,
+					     MSG_SIMPLE_Q_TAG,
+					     SSD_FULL_SIZE,
+					     /*timeout*/5000);
+
+			/*
+			 * Accomplish our 500ms delay by deferring
+			 * the release of our device queue appropriately.
+			 */
+			*relsim_flags = RELSIM_RELEASE_AFTER_TIMEOUT;
+			*timeout = 500;
+			break;
+		}
+		default:
+			panic("Unhandled error action %x", err_action);
+		}
+		
+		if ((err_action & SS_MASK) >= SS_START) {
+			/*
+			 * Drop the priority, so that the recovery
+			 * CCB is the first to execute.  Freeze the queue
+			 * after this command is sent so that we can
+			 * restore the old csio and have it queued in
+			 * the proper order before we release normal 
+			 * transactions to the device.
+			 */
+			ccb->ccb_h.pinfo.priority--;
+			ccb->ccb_h.flags |= CAM_DEV_QFREEZE;
+			ccb->ccb_h.saved_ccb_ptr = orig_ccb;
+			error = ERESTART;
+			*orig = orig_ccb;
+		}
+
+sense_error_done:
+		*action = err_action;
+	}
+	return (error);
+}
+#endif
+/*
+ * Generic error handler.  Peripheral drivers usually filter
+ * out the errors that they handle in a unique manner, then
+ * call this function.
+ */
+int
+cam_periph_error(union ccb *ccb, cam_flags camflags,
+		 u_int32_t sense_flags)
+{
+	struct cam_path *newpath;
+	union ccb  *orig_ccb, *scan_ccb;
+	struct cam_periph *periph;
+	const char *action_string;
+	cam_status  status;
+	int	    frozen, error, openings, devctl_err;
+	u_int32_t   action, relsim_flags, timeout;
+
+	action = SSQ_PRINT_SENSE;
+	periph = xpt_path_periph(ccb->ccb_h.path);
+	action_string = NULL;
+	status = ccb->ccb_h.status;
+	frozen = (status & CAM_DEV_QFRZN) != 0;
+	status &= CAM_STATUS_MASK;
+	devctl_err = openings = relsim_flags = timeout = 0;
+	orig_ccb = ccb;
+
+	/* Filter the errors that should be reported via devctl */
+	switch (ccb->ccb_h.status & CAM_STATUS_MASK) {
+	case CAM_CMD_TIMEOUT:
+	case CAM_REQ_ABORTED:
+	case CAM_REQ_CMP_ERR:
+	case CAM_REQ_TERMIO:
+	case CAM_UNREC_HBA_ERROR:
+	case CAM_DATA_RUN_ERR:
+	case CAM_SCSI_STATUS_ERROR:
+	case CAM_ATA_STATUS_ERROR:
+	case CAM_SMP_STATUS_ERROR:
+		devctl_err++;
+		break;
+	default:
+		break;
+	}
+
+	switch (status) {
+	case CAM_REQ_CMP:
+		error = 0;
+		action &= ~SSQ_PRINT_SENSE;
+		break;
+	case CAM_SCSI_STATUS_ERROR:
+		error = camperiphscsistatuserror(ccb, &orig_ccb,
+		    camflags, sense_flags, &openings, &relsim_flags,
+		    &timeout, &action, &action_string);
+		break;
+	case CAM_AUTOSENSE_FAIL:
+		error = EIO;	/* we have to kill the command */
+		break;
+	case CAM_UA_ABORT:
+	case CAM_UA_TERMIO:
+	case CAM_MSG_REJECT_REC:
+		/* XXX Don't know that these are correct */
+		error = EIO;
+		break;
+	case CAM_SEL_TIMEOUT:
+		if ((camflags & CAM_RETRY_SELTO) != 0) {
+			if (ccb->ccb_h.retry_count > 0 &&
+			    (periph->flags & CAM_PERIPH_INVALID) == 0) {
+				ccb->ccb_h.retry_count--;
+				error = ERESTART;
+
+				/*
+				 * Wait a bit to give the device
+				 * time to recover before we try again.
+				 */
+				relsim_flags = RELSIM_RELEASE_AFTER_TIMEOUT;
+				timeout = periph_selto_delay;
+				break;
+			}
+			action_string = "Retries exhausted";
+		}
+		/* FALLTHROUGH */
+	case CAM_DEV_NOT_THERE:
+		error = ENXIO;
+		action = SSQ_LOST;
+		break;
+	case CAM_REQ_INVALID:
+	case CAM_PATH_INVALID:
+	case CAM_NO_HBA:
+	case CAM_PROVIDE_FAIL:
+	case CAM_REQ_TOO_BIG:
+	case CAM_LUN_INVALID:
+	case CAM_TID_INVALID:
+	case CAM_FUNC_NOTAVAIL:
+		error = EINVAL;
+		break;
+	case CAM_SCSI_BUS_RESET:
+	case CAM_BDR_SENT:
+		/*
+		 * Commands that repeatedly timeout and cause these
+		 * kinds of error recovery actions, should return
+		 * CAM_CMD_TIMEOUT, which allows us to safely assume
+		 * that this command was an innocent bystander to
+		 * these events and should be unconditionally
+		 * retried.
+		 */
+	case CAM_REQUEUE_REQ:
+		/* Unconditional requeue if device is still there */
+		if (periph->flags & CAM_PERIPH_INVALID) {
+			action_string = "Periph was invalidated";
+			error = EIO;
+		} else if (sense_flags & SF_NO_RETRY) {
+			error = EIO;
+			action_string = "Retry was blocked";
+		} else {
+			error = ERESTART;
+			action &= ~SSQ_PRINT_SENSE;
+		}
+		break;
+	case CAM_RESRC_UNAVAIL:
+		/* Wait a bit for the resource shortage to abate. */
+		timeout = periph_noresrc_delay;
+		/* FALLTHROUGH */
+	case CAM_BUSY:
+		if (timeout == 0) {
+			/* Wait a bit for the busy condition to abate. */
+			timeout = periph_busy_delay;
+		}
+		relsim_flags = RELSIM_RELEASE_AFTER_TIMEOUT;
+		/* FALLTHROUGH */
+	case CAM_ATA_STATUS_ERROR:
+	case CAM_REQ_CMP_ERR:
+	case CAM_CMD_TIMEOUT:
+	case CAM_UNEXP_BUSFREE:
+	case CAM_UNCOR_PARITY:
+	case CAM_DATA_RUN_ERR:
+	default:
+		if (periph->flags & CAM_PERIPH_INVALID) {
+			error = EIO;
+			action_string = "Periph was invalidated";
+		} else if (ccb->ccb_h.retry_count == 0) {
+			error = EIO;
+			action_string = "Retries exhausted";
+		} else if (sense_flags & SF_NO_RETRY) {
+			error = EIO;
+			action_string = "Retry was blocked";
+		} else {
+			ccb->ccb_h.retry_count--;
+			error = ERESTART;
+		}
+		break;
+	}
+
+	if ((sense_flags & SF_PRINT_ALWAYS) ||
+	    CAM_DEBUGGED(ccb->ccb_h.path, CAM_DEBUG_INFO))
+		action |= SSQ_PRINT_SENSE;
+	else if (sense_flags & SF_NO_PRINT)
+		action &= ~SSQ_PRINT_SENSE;
+	if ((action & SSQ_PRINT_SENSE) != 0)
+#ifndef __rtems__
+		cam_error_print(orig_ccb, CAM_ESF_ALL, CAM_EPF_ALL);
+ #endif /* __rtems__ */
+	if (error != 0 && (action & SSQ_PRINT_SENSE) != 0) {
+		if (error != ERESTART) {
+			if (action_string == NULL)
+				action_string = "Unretryable error";
+			xpt_print(ccb->ccb_h.path, "Error %d, %s\n",
+			    error, action_string);
+		} else if (action_string != NULL)
+			xpt_print(ccb->ccb_h.path, "%s\n", action_string);
+		else
+			xpt_print(ccb->ccb_h.path, "Retrying command\n");
+	}
+
+	if (devctl_err && (error != 0 || (action & SSQ_PRINT_SENSE) != 0))
+		cam_periph_devctl_notify(orig_ccb);
+
+	if ((action & SSQ_LOST) != 0) {
+		lun_id_t lun_id;
+
+		/*
+		 * For a selection timeout, we consider all of the LUNs on
+		 * the target to be gone.  If the status is CAM_DEV_NOT_THERE,
+		 * then we only get rid of the device(s) specified by the
+		 * path in the original CCB.
+		 */
+		if (status == CAM_SEL_TIMEOUT)
+			lun_id = CAM_LUN_WILDCARD;
+		else
+			lun_id = xpt_path_lun_id(ccb->ccb_h.path);
+
+		/* Should we do more if we can't create the path?? */
+		if (xpt_create_path(&newpath, periph,
+				    xpt_path_path_id(ccb->ccb_h.path),
+				    xpt_path_target_id(ccb->ccb_h.path),
+				    lun_id) == CAM_REQ_CMP) {
+
+			/*
+			 * Let peripheral drivers know that this
+			 * device has gone away.
+			 */
+			xpt_async(AC_LOST_DEVICE, newpath, NULL);
+			xpt_free_path(newpath);
+		}
+	}
+
+	/* Broadcast UNIT ATTENTIONs to all periphs. */
+	if ((action & SSQ_UA) != 0)
+		xpt_async(AC_UNIT_ATTENTION, orig_ccb->ccb_h.path, orig_ccb);
+
+	/* Rescan target on "Reported LUNs data has changed" */
+	if ((action & SSQ_RESCAN) != 0) {
+		if (xpt_create_path(&newpath, NULL,
+				    xpt_path_path_id(ccb->ccb_h.path),
+				    xpt_path_target_id(ccb->ccb_h.path),
+				    CAM_LUN_WILDCARD) == CAM_REQ_CMP) {
+
+			scan_ccb = xpt_alloc_ccb_nowait();
+			if (scan_ccb != NULL) {
+				scan_ccb->ccb_h.path = newpath;
+				scan_ccb->ccb_h.func_code = XPT_SCAN_TGT;
+				scan_ccb->crcn.flags = 0;
+				xpt_rescan(scan_ccb);
+			} else {
+				xpt_print(newpath,
+				    "Can't allocate CCB to rescan target\n");
+				xpt_free_path(newpath);
+			}
+		}
+	}
+
+	/* Attempt a retry */
+	if (error == ERESTART || error == 0) {
+		if (frozen != 0)
+			ccb->ccb_h.status &= ~CAM_DEV_QFRZN;
+		if (error == ERESTART)
+			xpt_action(ccb);
+		if (frozen != 0)
+			cam_release_devq(ccb->ccb_h.path,
+					 relsim_flags,
+					 openings,
+					 timeout,
+					 /*getcount_only*/0);
+	}
+
+	return (error);
+}
+
+#define CAM_PERIPH_DEVD_MSG_SIZE	256
+
+static void
+cam_periph_devctl_notify(union ccb *ccb)
+{
+	struct cam_periph *periph;
+	struct ccb_getdev *cgd;
+	struct sbuf sb;
+	int serr, sk, asc, ascq;
+	char *sbmsg, *type;
+
+	sbmsg = malloc(CAM_PERIPH_DEVD_MSG_SIZE, M_CAMPERIPH, M_NOWAIT);
+	if (sbmsg == NULL)
+		return;
+
+	sbuf_new(&sb, sbmsg, CAM_PERIPH_DEVD_MSG_SIZE, SBUF_FIXEDLEN);
+
+	periph = xpt_path_periph(ccb->ccb_h.path);
+	sbuf_printf(&sb, "device=%s%d ", periph->periph_name,
+	    periph->unit_number);
+
+	sbuf_printf(&sb, "serial=\"");
+	if ((cgd = (struct ccb_getdev *)xpt_alloc_ccb_nowait()) != NULL) {
+		xpt_setup_ccb(&cgd->ccb_h, ccb->ccb_h.path,
+		    CAM_PRIORITY_NORMAL);
+		cgd->ccb_h.func_code = XPT_GDEV_TYPE;
+		xpt_action((union ccb *)cgd);
+
+		if (cgd->ccb_h.status == CAM_REQ_CMP)
+			sbuf_bcat(&sb, cgd->serial_num, cgd->serial_num_len);
+		xpt_free_ccb((union ccb *)cgd);
+	}
+	sbuf_printf(&sb, "\" ");
+	sbuf_printf(&sb, "cam_status=\"0x%x\" ", ccb->ccb_h.status);
+
+	switch (ccb->ccb_h.status & CAM_STATUS_MASK) {
+	case CAM_CMD_TIMEOUT:
+		sbuf_printf(&sb, "timeout=%d ", ccb->ccb_h.timeout);
+		type = "timeout";
+		break;
+	case CAM_SCSI_STATUS_ERROR:
+#ifndef __rtems__
+		sbuf_printf(&sb, "scsi_status=%d ", ccb->csio.scsi_status);
+		if (scsi_extract_sense_ccb(ccb, &serr, &sk, &asc, &ascq))
+			sbuf_printf(&sb, "scsi_sense=\"%02x %02x %02x %02x\" ",
+			    serr, sk, asc, ascq);
+#endif /* __rtems__ */
+		type = "error";
+		break;
+	case CAM_ATA_STATUS_ERROR:
+#ifndef __rtems__
+		sbuf_printf(&sb, "RES=\"");
+		ata_res_sbuf(&ccb->ataio.res, &sb);
+		sbuf_printf(&sb, "\" ");
+#endif /* __rtems__ */
+		type = "error";
+		break;
+	default:
+		type = "error";
+		break;
+	}
+
+	if (ccb->ccb_h.func_code == XPT_SCSI_IO) {
+		sbuf_printf(&sb, "CDB=\"");
+#ifndef __rtems__
+		scsi_cdb_sbuf(scsiio_cdb_ptr(&ccb->csio), &sb);
+#endif /* __rtems__ */
+		sbuf_printf(&sb, "\" ");
+	} else if (ccb->ccb_h.func_code == XPT_ATA_IO) {
+		sbuf_printf(&sb, "ACB=\"");
+#ifndef __rtems__
+		ata_cmd_sbuf(&ccb->ataio.cmd, &sb);
+#endif /* __rtems__ */
+		sbuf_printf(&sb, "\" ");
+	}
+
+	if (sbuf_finish(&sb) == 0)
+		devctl_notify("CAM", "periph", type, sbuf_data(&sb));
+	sbuf_delete(&sb);
+	free(sbmsg, M_CAMPERIPH);
+}
+
diff --git a/freebsd/sys/cam/cam_periph.h b/freebsd/sys/cam/cam_periph.h
index 87f153c..e20a777 100644
--- a/freebsd/sys/cam/cam_periph.h
+++ b/freebsd/sys/cam/cam_periph.h
@@ -195,7 +195,7 @@ void		cam_periph_freeze_after_event(struct cam_periph *periph,
 					      struct timeval* event_time,
 					      u_int duration_ms);
 int		cam_periph_error(union ccb *ccb, cam_flags camflags,
-				 u_int32_t sense_flags, union ccb *save_ccb);
+				 u_int32_t sense_flags);
 
 static __inline struct mtx *
 cam_periph_mtx(struct cam_periph *periph)
diff --git a/freebsd/sys/cam/cam_queue.c b/freebsd/sys/cam/cam_queue.c
new file mode 100644
index 0000000..247d03e
--- /dev/null
+++ b/freebsd/sys/cam/cam_queue.c
@@ -0,0 +1,399 @@
+#include <machine/rtems-bsd-kernel-space.h>
+
+/*-
+ * CAM request queue management functions.
+ *
+ * Copyright (c) 1997 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/types.h>
+#include <sys/malloc.h>
+#include <sys/kernel.h>
+
+#include <cam/cam.h>
+#include <cam/cam_ccb.h>
+#include <cam/cam_queue.h>
+#include <cam/cam_debug.h>
+
+static MALLOC_DEFINE(M_CAMQ, "CAM queue", "CAM queue buffers");
+static MALLOC_DEFINE(M_CAMDEVQ, "CAM dev queue", "CAM dev queue buffers");
+static MALLOC_DEFINE(M_CAMCCBQ, "CAM ccb queue", "CAM ccb queue buffers");
+
+static __inline int
+		queue_cmp(cam_pinfo **queue_array, int i, int j);
+static __inline void
+		swap(cam_pinfo **queue_array, int i, int j);
+static void	heap_up(cam_pinfo **queue_array, int new_index);
+static void	heap_down(cam_pinfo **queue_array, int index,
+			  int last_index);
+
+struct camq *
+camq_alloc(int size)
+{
+	struct camq *camq;
+	camq = (struct camq *)malloc(sizeof(*camq), M_CAMQ, M_NOWAIT);
+	if (camq != NULL) {
+		if (camq_init(camq, size) != 0) {
+			free(camq, M_CAMQ);
+			camq = NULL;
+		}
+	}
+	return (camq);
+}
+	
+int
+camq_init(struct camq *camq, int size)
+{
+	bzero(camq, sizeof(*camq));
+	camq->array_size = size;
+	if (camq->array_size != 0) {
+		camq->queue_array = (cam_pinfo**)malloc(size*sizeof(cam_pinfo*),
+							M_CAMQ, M_NOWAIT);
+		if (camq->queue_array == NULL) {
+			printf("camq_init: - cannot malloc array!\n");
+			return (1);
+		}
+		/*
+		 * Heap algorithms like everything numbered from 1, so
+		 * offset our pointer into the heap array by one element.
+		 */
+		camq->queue_array--;
+	}
+	return (0);
+}
+
+/*
+ * Free a camq structure.  This should only be called if a controller
+ * driver failes somehow during its attach routine or is unloaded and has
+ * obtained a camq structure.  The XPT should ensure that the queue
+ * is empty before calling this routine.
+ */
+void
+camq_free(struct camq *queue)
+{
+	if (queue != NULL) {
+		camq_fini(queue);
+		free(queue, M_CAMQ);
+	}
+}
+
+void
+camq_fini(struct camq *queue)
+{
+	if (queue->queue_array != NULL) {
+		/*
+		 * Heap algorithms like everything numbered from 1, so
+		 * our pointer into the heap array is offset by one element.
+		 */
+		queue->queue_array++;
+		free(queue->queue_array, M_CAMQ);
+	}
+}
+
+u_int32_t
+camq_resize(struct camq *queue, int new_size)
+{
+	cam_pinfo **new_array;
+	KASSERT(new_size >= queue->entries, ("camq_resize: "
+	    "New queue size can't accommodate queued entries (%d < %d).",
+	    new_size, queue->entries));
+	new_array = (cam_pinfo **)malloc(new_size * sizeof(cam_pinfo *),
+					 M_CAMQ, M_NOWAIT);
+	if (new_array == NULL) {
+		/* Couldn't satisfy request */
+		return (CAM_RESRC_UNAVAIL);
+	}
+	/*
+	 * Heap algorithms like everything numbered from 1, so
+	 * remember that our pointer into the heap array is offset
+	 * by one element.
+	 */
+	if (queue->queue_array != NULL) {
+		queue->queue_array++;
+		bcopy(queue->queue_array, new_array,
+		      queue->entries * sizeof(cam_pinfo *));
+		free(queue->queue_array, M_CAMQ);
+	}
+	queue->queue_array = new_array-1;
+	queue->array_size = new_size;
+	return (CAM_REQ_CMP);
+}
+
+/*
+ * camq_insert: Given an array of cam_pinfo* elememnts with
+ * the Heap(1, num_elements) property and array_size - num_elements >= 1,
+ * output Heap(1, num_elements+1) including new_entry in the array.
+ */
+void
+camq_insert(struct camq *queue, cam_pinfo *new_entry)
+{
+	KASSERT(queue->entries < queue->array_size,
+	    ("camq_insert: Attempt to insert into a full queue (%d >= %d)",
+	    queue->entries, queue->array_size));
+	queue->entries++;
+	queue->queue_array[queue->entries] = new_entry;
+	new_entry->index = queue->entries;
+	if (queue->entries != 0)
+		heap_up(queue->queue_array, queue->entries);
+}
+
+/*
+ * camq_remove:  Given an array of cam_pinfo* elevements with the
+ * Heap(1, num_elements) property and an index such that 1 <= index <=
+ * num_elements, remove that entry and restore the Heap(1, num_elements-1)
+ * property.
+ */
+cam_pinfo *
+camq_remove(struct camq *queue, int index)
+{
+	cam_pinfo *removed_entry;
+	if (index <= 0 || index > queue->entries)
+		panic("%s: Attempt to remove out-of-bounds index %d "
+		    "from queue %p of size %d", __func__, index, queue,
+		    queue->entries);
+
+	removed_entry = queue->queue_array[index];
+	if (queue->entries != index) {
+		queue->queue_array[index] = queue->queue_array[queue->entries];
+		queue->queue_array[index]->index = index;
+		heap_down(queue->queue_array, index, queue->entries - 1);
+	}
+	removed_entry->index = CAM_UNQUEUED_INDEX;
+	queue->entries--;
+	return (removed_entry);
+}
+
+/*
+ * camq_change_priority:  Given an array of cam_pinfo* elements with the
+ * Heap(1, num_entries) property, an index such that 1 <= index <= num_elements,
+ * and a new priority for the element at index, change the priority of
+ * element index and restore the Heap(0, num_elements) property.
+ */
+void
+camq_change_priority(struct camq *queue, int index, u_int32_t new_priority)
+{
+	if (new_priority > queue->queue_array[index]->priority) {
+		queue->queue_array[index]->priority = new_priority;
+		heap_down(queue->queue_array, index, queue->entries);
+	} else {
+		/* new_priority <= old_priority */
+		queue->queue_array[index]->priority = new_priority;
+		heap_up(queue->queue_array, index);
+	}
+}
+
+struct cam_devq *
+cam_devq_alloc(int devices, int openings)
+{
+	struct cam_devq *devq;
+	devq = (struct cam_devq *)malloc(sizeof(*devq), M_CAMDEVQ, M_NOWAIT);
+	if (devq == NULL) {
+		printf("cam_devq_alloc: - cannot malloc!\n");
+		return (NULL);
+	}
+	if (cam_devq_init(devq, devices, openings) != 0) {
+		free(devq, M_CAMDEVQ);
+		return (NULL);
+	}
+	return (devq);
+}
+
+int
+cam_devq_init(struct cam_devq *devq, int devices, int openings)
+{
+	bzero(devq, sizeof(*devq));
+	mtx_init(&devq->send_mtx, "CAM queue lock", NULL, MTX_DEF);
+	if (camq_init(&devq->send_queue, devices) != 0)
+		return (1);
+	devq->send_openings = openings;
+	devq->send_active = 0;
+	return (0);
+}
+
+void
+cam_devq_free(struct cam_devq *devq)
+{
+	camq_fini(&devq->send_queue);
+	mtx_destroy(&devq->send_mtx);
+	free(devq, M_CAMDEVQ);
+}
+
+u_int32_t
+cam_devq_resize(struct cam_devq *camq, int devices)
+{
+	u_int32_t retval;
+	retval = camq_resize(&camq->send_queue, devices);
+	return (retval);
+}
+
+struct cam_ccbq *
+cam_ccbq_alloc(int openings)
+{
+	struct cam_ccbq *ccbq;
+	ccbq = (struct cam_ccbq *)malloc(sizeof(*ccbq), M_CAMCCBQ, M_NOWAIT);
+	if (ccbq == NULL) {
+		printf("cam_ccbq_alloc: - cannot malloc!\n");
+		return (NULL);
+	}
+	if (cam_ccbq_init(ccbq, openings) != 0) {
+		free(ccbq, M_CAMCCBQ);
+		return (NULL);		
+	}
+	
+	return (ccbq);
+}
+
+void
+cam_ccbq_free(struct cam_ccbq *ccbq)
+{
+	if (ccbq) {
+		cam_ccbq_fini(ccbq);
+		free(ccbq, M_CAMCCBQ);
+	}
+}
+
+u_int32_t
+cam_ccbq_resize(struct cam_ccbq *ccbq, int new_size)
+{
+	int delta;
+	delta = new_size - (ccbq->dev_active + ccbq->dev_openings);
+	ccbq->total_openings += delta;
+	ccbq->dev_openings += delta;
+
+	new_size = imax(64, 1 << fls(new_size + new_size / 2));
+	if (new_size > ccbq->queue.array_size)
+		return (camq_resize(&ccbq->queue, new_size));
+	else
+		return (CAM_REQ_CMP);
+}
+
+int
+cam_ccbq_init(struct cam_ccbq *ccbq, int openings)
+{
+	bzero(ccbq, sizeof(*ccbq));
+	if (camq_init(&ccbq->queue,
+	    imax(64, 1 << fls(openings + openings / 2))) != 0)
+		return (1);
+	ccbq->total_openings = openings;
+	ccbq->dev_openings = openings;
+	return (0);
+}
+
+void
+cam_ccbq_fini(struct cam_ccbq *ccbq)
+{
+	camq_fini(&ccbq->queue);
+}
+
+/*
+ * Heap routines for manipulating CAM queues.
+ */
+/*
+ * queue_cmp: Given an array of cam_pinfo* elements and indexes i
+ * and j, return less than 0, 0, or greater than 0 if i is less than,
+ * equal too, or greater than j respectively.
+ */
+static __inline int
+queue_cmp(cam_pinfo **queue_array, int i, int j)
+{
+	if (queue_array[i]->priority == queue_array[j]->priority)
+		return (  queue_array[i]->generation
+			- queue_array[j]->generation );
+	else
+		return (  queue_array[i]->priority
+			- queue_array[j]->priority );
+}
+
+/*
+ * swap: Given an array of cam_pinfo* elements and indexes i and j,
+ * exchange elements i and j.
+ */
+static __inline void
+swap(cam_pinfo **queue_array, int i, int j)
+{
+	cam_pinfo *temp_qentry;
+
+	temp_qentry = queue_array[j];
+	queue_array[j] = queue_array[i];
+	queue_array[i] = temp_qentry;
+	queue_array[j]->index = j;
+	queue_array[i]->index = i;
+}
+
+/*
+ * heap_up:  Given an array of cam_pinfo* elements with the
+ * Heap(1, new_index-1) property and a new element in location
+ * new_index, output Heap(1, new_index).
+ */
+static void
+heap_up(cam_pinfo **queue_array, int new_index)
+{
+	int child;
+	int parent;
+
+	child = new_index;
+
+	while (child != 1) {
+
+		parent = child >> 1;
+		if (queue_cmp(queue_array, parent, child) <= 0)
+			break;
+		swap(queue_array, parent, child);
+		child = parent;
+	}
+}
+
+/*
+ * heap_down:  Given an array of cam_pinfo* elements with the
+ * Heap(index + 1, num_entries) property with index containing
+ * an unsorted entry, output Heap(index, num_entries).
+ */
+static void
+heap_down(cam_pinfo **queue_array, int index, int num_entries)
+{
+	int child;
+	int parent;
+	
+	parent = index;
+	child = parent << 1;
+	for (; child <= num_entries; child = parent << 1) {
+
+		if (child < num_entries) {
+			/* child+1 is the right child of parent */
+			if (queue_cmp(queue_array, child + 1, child) < 0)
+				child++;
+		}
+		/* child is now the least child of parent */
+		if (queue_cmp(queue_array, parent, child) <= 0)
+			break;
+		swap(queue_array, child, parent);
+		parent = child;
+	}
+}
diff --git a/freebsd/sys/cam/cam_queue.h b/freebsd/sys/cam/cam_queue.h
new file mode 100644
index 0000000..455590a
--- /dev/null
+++ b/freebsd/sys/cam/cam_queue.h
@@ -0,0 +1,291 @@
+/*-
+ * CAM request queue management definitions.
+ *
+ * Copyright (c) 1997 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _CAM_CAM_QUEUE_H
+#define _CAM_CAM_QUEUE_H 1
+
+#ifdef _KERNEL
+
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/queue.h>
+#include <cam/cam.h>
+
+/*
+ * This structure implements a heap based priority queue.  The queue
+ * assumes that the objects stored in it begin with a cam_qentry
+ * structure holding the priority information used to sort the objects.
+ * This structure is opaque to clients (outside of the XPT layer) to allow
+ * the implementation to change without affecting them.
+ */
+struct camq {
+	cam_pinfo **queue_array;
+	int	   array_size;
+	int	   entries;
+	u_int32_t  generation;
+	u_int32_t  qfrozen_cnt;
+};
+
+TAILQ_HEAD(ccb_hdr_tailq, ccb_hdr);
+LIST_HEAD(ccb_hdr_list, ccb_hdr);
+SLIST_HEAD(ccb_hdr_slist, ccb_hdr);
+
+struct cam_ccbq {
+	struct	camq queue;
+	struct ccb_hdr_tailq	queue_extra_head;
+	int	queue_extra_entries;
+	int	total_openings;
+	int	allocated;
+	int	dev_openings;
+	int	dev_active;
+};
+
+struct cam_ed;
+
+struct cam_devq {
+	struct mtx	 send_mtx;
+	struct camq	 send_queue;
+	int		 send_openings;
+	int		 send_active;
+};
+
+
+struct cam_devq *cam_devq_alloc(int devices, int openings);
+
+int		 cam_devq_init(struct cam_devq *devq, int devices,
+			       int openings);
+
+void		 cam_devq_free(struct cam_devq *devq);
+
+u_int32_t	 cam_devq_resize(struct cam_devq *camq, int openings);
+	
+/*
+ * Allocate a cam_ccb_queue structure and initialize it.
+ */
+struct cam_ccbq	*cam_ccbq_alloc(int openings);
+
+u_int32_t	cam_ccbq_resize(struct cam_ccbq *ccbq, int devices);
+
+int		cam_ccbq_init(struct cam_ccbq *ccbq, int openings);
+
+void		cam_ccbq_free(struct cam_ccbq *ccbq);
+
+void		cam_ccbq_fini(struct cam_ccbq *ccbq);
+
+/*
+ * Allocate and initialize a cam_queue structure.
+ */
+struct camq	*camq_alloc(int size);
+
+/*
+ * Resize a cam queue
+ */
+u_int32_t	camq_resize(struct camq *queue, int new_size);
+
+/* 
+ * Initialize a camq structure.  Return 0 on success, 1 on failure.
+ */
+int		camq_init(struct camq *camq, int size);
+
+/*
+ * Free a cam_queue structure.  This should only be called if a controller
+ * driver failes somehow during its attach routine or is unloaded and has
+ * obtained a cam_queue structure.
+ */
+void		camq_free(struct camq *queue);
+
+/*
+ * Finialize any internal storage or state of a cam_queue.
+ */
+void		camq_fini(struct camq *queue);
+
+/*
+ * cam_queue_insert: Given a CAM queue with at least one open spot,
+ * insert the new entry maintaining order.
+ */
+void		camq_insert(struct camq *queue, cam_pinfo *new_entry);
+
+/*
+ * camq_remove: Remove and arbitrary entry from the queue maintaining
+ * queue order.
+ */
+cam_pinfo	*camq_remove(struct camq *queue, int index);
+#define CAMQ_HEAD 1	/* Head of queue index */
+
+/* Index the first element in the heap */
+#define CAMQ_GET_HEAD(camq) ((camq)->queue_array[CAMQ_HEAD])
+
+/* Get the first element priority. */
+#define CAMQ_GET_PRIO(camq) (((camq)->entries > 0) ?			\
+			    ((camq)->queue_array[CAMQ_HEAD]->priority) : 0)
+
+/*
+ * camq_change_priority: Raise or lower the priority of an entry
+ * maintaining queue order.
+ */
+void		camq_change_priority(struct camq *queue, int index,
+				     u_int32_t new_priority);
+
+static __inline int
+cam_ccbq_pending_ccb_count(struct cam_ccbq *ccbq);
+
+static __inline void
+cam_ccbq_take_opening(struct cam_ccbq *ccbq);
+
+static __inline void
+cam_ccbq_insert_ccb(struct cam_ccbq *ccbq, union ccb *new_ccb);
+
+static __inline void
+cam_ccbq_remove_ccb(struct cam_ccbq *ccbq, union ccb *ccb);
+
+static __inline union ccb *
+cam_ccbq_peek_ccb(struct cam_ccbq *ccbq, int index);
+
+static __inline void
+cam_ccbq_send_ccb(struct cam_ccbq *queue, union ccb *send_ccb);
+
+static __inline void
+cam_ccbq_ccb_done(struct cam_ccbq *ccbq, union ccb *done_ccb);
+
+static __inline void
+cam_ccbq_release_opening(struct cam_ccbq *ccbq);
+
+
+static __inline int
+cam_ccbq_pending_ccb_count(struct cam_ccbq *ccbq)
+{
+	return (ccbq->queue.entries + ccbq->queue_extra_entries);
+}
+
+static __inline void
+cam_ccbq_take_opening(struct cam_ccbq *ccbq)
+{
+	ccbq->allocated++;
+}
+
+static __inline void
+cam_ccbq_insert_ccb(struct cam_ccbq *ccbq, union ccb *new_ccb)
+{
+	struct ccb_hdr *old_ccb;
+	struct camq *queue = &ccbq->queue;
+
+	KASSERT((new_ccb->ccb_h.func_code & XPT_FC_QUEUED) != 0 &&
+	    (new_ccb->ccb_h.func_code & XPT_FC_USER_CCB) == 0,
+	    ("%s: Cannot queue ccb %p func_code %#x", __func__, new_ccb,
+	     new_ccb->ccb_h.func_code));
+
+	/*
+	 * If queue is already full, try to resize.
+	 * If resize fail, push CCB with lowest priority out to the TAILQ.
+	 */
+	if (queue->entries == queue->array_size &&
+	    camq_resize(&ccbq->queue, queue->array_size * 2) != CAM_REQ_CMP) {
+		old_ccb = (struct ccb_hdr *)camq_remove(queue, queue->entries);
+		TAILQ_INSERT_HEAD(&ccbq->queue_extra_head, old_ccb,
+		    xpt_links.tqe);
+		old_ccb->pinfo.index = CAM_EXTRAQ_INDEX;
+		ccbq->queue_extra_entries++;
+	}
+
+	camq_insert(queue, &new_ccb->ccb_h.pinfo);
+}
+
+static __inline void
+cam_ccbq_remove_ccb(struct cam_ccbq *ccbq, union ccb *ccb)
+{
+	struct ccb_hdr *cccb, *bccb;
+	struct camq *queue = &ccbq->queue;
+	cam_pinfo *removed_entry __unused;
+
+	/* If the CCB is on the TAILQ, remove it from there. */
+	if (ccb->ccb_h.pinfo.index == CAM_EXTRAQ_INDEX) {
+		TAILQ_REMOVE(&ccbq->queue_extra_head, &ccb->ccb_h,
+		    xpt_links.tqe);
+		ccb->ccb_h.pinfo.index = CAM_UNQUEUED_INDEX;
+		ccbq->queue_extra_entries--;
+		return;
+	}
+
+	removed_entry = camq_remove(queue, ccb->ccb_h.pinfo.index);
+	KASSERT(removed_entry == &ccb->ccb_h.pinfo,
+	    ("%s: Removed wrong entry from queue (%p != %p)", __func__,
+	     removed_entry, &ccb->ccb_h.pinfo));
+
+	/*
+	 * If there are some CCBs on TAILQ, find the best one and move it
+	 * to the emptied space in the queue.
+	 */
+	bccb = TAILQ_FIRST(&ccbq->queue_extra_head);
+	if (bccb == NULL)
+		return;
+	TAILQ_FOREACH(cccb, &ccbq->queue_extra_head, xpt_links.tqe) {
+		if (bccb->pinfo.priority > cccb->pinfo.priority ||
+		    (bccb->pinfo.priority == cccb->pinfo.priority &&
+		     GENERATIONCMP(bccb->pinfo.generation, >,
+		      cccb->pinfo.generation)))
+		        bccb = cccb;
+	}
+	TAILQ_REMOVE(&ccbq->queue_extra_head, bccb, xpt_links.tqe);
+	ccbq->queue_extra_entries--;
+	camq_insert(queue, &bccb->pinfo);
+}
+
+static __inline union ccb *
+cam_ccbq_peek_ccb(struct cam_ccbq *ccbq, int index)
+{
+	return((union ccb *)ccbq->queue.queue_array[index]);
+}
+
+static __inline void
+cam_ccbq_send_ccb(struct cam_ccbq *ccbq, union ccb *send_ccb)
+{
+
+	send_ccb->ccb_h.pinfo.index = CAM_ACTIVE_INDEX;
+	ccbq->dev_active++;
+	ccbq->dev_openings--;
+}
+
+static __inline void
+cam_ccbq_ccb_done(struct cam_ccbq *ccbq, union ccb *done_ccb)
+{
+
+	ccbq->dev_active--;
+	ccbq->dev_openings++;
+}
+
+static __inline void
+cam_ccbq_release_opening(struct cam_ccbq *ccbq)
+{
+
+	ccbq->allocated--;
+}
+
+#endif /* _KERNEL */
+#endif  /* _CAM_CAM_QUEUE_H */
diff --git a/freebsd/sys/cam/cam_sim.h b/freebsd/sys/cam/cam_sim.h
index 4c4c8c5..b1d38d6 100644
--- a/freebsd/sys/cam/cam_sim.h
+++ b/freebsd/sys/cam/cam_sim.h
@@ -130,15 +130,14 @@ struct cam_sim {
 #ifndef __rtems__
 	TAILQ_HEAD(, ccb_hdr)	sim_doneq;
 	TAILQ_ENTRY(cam_sim)	links;
-	u_int32_t		path_id;/* The Boot device may set this to 0? */
 #else /* __rtems__ */
+	u_int32_t		path_id;/* The Boot device may set this to 0? */
 	char			*disk;
 	enum bsd_sim_state	state;
 	struct cv		state_changed;
 	union ccb		ccb;
 #endif /* __rtems__ */
 	u_int32_t		unit_number;
-#ifndef __rtems__
 	u_int32_t		bus_id;
 	int			max_tagged_dev_openings;
 	int			max_dev_openings;
@@ -148,7 +147,6 @@ struct cam_sim {
 	struct callout		callout;
 	struct cam_devq 	*devq;	/* Device Queue to use for this SIM */
 	int			refcount; /* References to the SIM. */
-#endif /* __rtems__ */
 };
 
 #define CAM_SIM_LOCK(sim)	mtx_lock((sim)->mtx)
@@ -157,11 +155,7 @@ struct cam_sim {
 static __inline u_int32_t
 cam_sim_path(struct cam_sim *sim)
 {
-#ifndef __rtems__
 	return (sim->path_id);
-#else /* __rtems__ */
-	return (0);
-#endif /* __rtems__ */
 }
 
 static __inline const char *
@@ -182,13 +176,11 @@ cam_sim_unit(struct cam_sim *sim)
 	return (sim->unit_number);
 }
 
-#ifndef __rtems__
 static __inline u_int32_t
 cam_sim_bus(struct cam_sim *sim)
 {
 	return (sim->bus_id);
 }
-#endif /* __rtems__ */
 
 #endif /* _KERNEL */
 #endif /* _CAM_CAM_SIM_H */
diff --git a/freebsd/sys/cam/cam_xpt.c b/freebsd/sys/cam/cam_xpt.c
new file mode 100644
index 0000000..032350c
--- /dev/null
+++ b/freebsd/sys/cam/cam_xpt.c
@@ -0,0 +1,5491 @@
+#include <machine/rtems-bsd-kernel-space.h>
+
+/*-
+ * Implementation of the Common Access Method Transport (XPT) layer.
+ *
+ * Copyright (c) 1997, 1998, 1999 Justin T. Gibbs.
+ * Copyright (c) 1997, 1998, 1999 Kenneth D. Merry.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ */
+
+#include <rtems/bsd/local/opt_printf.h>
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/bio.h>
+#include <sys/bus.h>
+#include <sys/systm.h>
+#include <sys/types.h>
+#include <sys/malloc.h>
+#include <sys/kernel.h>
+#include <sys/time.h>
+#include <sys/conf.h>
+#include <sys/fcntl.h>
+#include <sys/interrupt.h>
+#include <sys/proc.h>
+#include <sys/sbuf.h>
+#include <sys/smp.h>
+#include <sys/taskqueue.h>
+
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/sysctl.h>
+#include <sys/kthread.h>
+
+#include <cam/cam.h>
+#include <cam/cam_ccb.h>
+#include <cam/cam_periph.h>
+#include <cam/cam_queue.h>
+#include <cam/cam_sim.h>
+#include <cam/cam_xpt.h>
+#include <cam/cam_xpt_sim.h>
+#include <cam/cam_xpt_periph.h>
+#include <cam/cam_xpt_internal.h>
+#include <cam/cam_debug.h>
+#include <cam/cam_compat.h>
+
+#include <cam/scsi/scsi_all.h>
+#ifndef __rtems__
+#include <cam/scsi/scsi_message.h>
+#include <cam/scsi/scsi_pass.h>
+#else
+#define CAMIOCOMMAND	_IOWR(CAM_VERSION, 2, union ccb)
+#define CAMGETPASSTHRU	_IOWR(CAM_VERSION, 3, union ccb)
+#endif
+
+RTEMS_BSD_DEFINE_SET(cam_xpt_proto_set, struct xpt_proto);
+RTEMS_BSD_DEFINE_SET(cam_xpt_xport_set, struct xpt_xport);
+#ifndef __rtems__
+#include <machine/md_var.h>	/* geometry translation */
+#else
+#include "md_var.h"
+#endif
+#include <machine/stdarg.h>	/* for xpt_print below */
+
+#include <rtems/bsd/local/opt_cam.h>
+
+/* Wild guess based on not wanting to grow the stack too much */
+#define XPT_PRINT_MAXLEN	512
+#ifdef PRINTF_BUFR_SIZE
+#define XPT_PRINT_LEN	PRINTF_BUFR_SIZE
+#else
+#define XPT_PRINT_LEN	128
+#endif
+_Static_assert(XPT_PRINT_LEN <= XPT_PRINT_MAXLEN, "XPT_PRINT_LEN is too large");
+
+/*
+ * This is the maximum number of high powered commands (e.g. start unit)
+ * that can be outstanding at a particular time.
+ */
+#ifndef CAM_MAX_HIGHPOWER
+#define CAM_MAX_HIGHPOWER  4
+#endif
+
+/* Datastructures internal to the xpt layer */
+MALLOC_DEFINE(M_CAMXPT, "CAM XPT", "CAM XPT buffers");
+MALLOC_DEFINE(M_CAMDEV, "CAM DEV", "CAM devices");
+MALLOC_DEFINE(M_CAMCCB, "CAM CCB", "CAM CCBs");
+MALLOC_DEFINE(M_CAMPATH, "CAM path", "CAM paths");
+
+/* Object for defering XPT actions to a taskqueue */
+struct xpt_task {
+	struct task	task;
+	void		*data1;
+	uintptr_t	data2;
+};
+
+struct xpt_softc {
+	uint32_t		xpt_generation;
+
+	/* number of high powered commands that can go through right now */
+	struct mtx		xpt_highpower_lock;
+	STAILQ_HEAD(highpowerlist, cam_ed)	highpowerq;
+	int			num_highpower;
+
+	/* queue for handling async rescan requests. */
+	TAILQ_HEAD(, ccb_hdr) ccb_scanq;
+	int buses_to_config;
+	int buses_config_done;
+
+	/*
+	 * Registered buses
+	 *
+	 * N.B., "busses" is an archaic spelling of "buses".  In new code
+	 * "buses" is preferred.
+	 */
+	TAILQ_HEAD(,cam_eb)	xpt_busses;
+	u_int			bus_generation;
+
+	struct intr_config_hook	*xpt_config_hook;
+
+	int			boot_delay;
+	struct callout 		boot_callout;
+
+	struct mtx		xpt_topo_lock;
+	struct mtx		xpt_lock;
+	struct taskqueue	*xpt_taskq;
+};
+
+typedef enum {
+	DM_RET_COPY		= 0x01,
+	DM_RET_FLAG_MASK	= 0x0f,
+	DM_RET_NONE		= 0x00,
+	DM_RET_STOP		= 0x10,
+	DM_RET_DESCEND		= 0x20,
+	DM_RET_ERROR		= 0x30,
+	DM_RET_ACTION_MASK	= 0xf0
+} dev_match_ret;
+
+typedef enum {
+	XPT_DEPTH_BUS,
+	XPT_DEPTH_TARGET,
+	XPT_DEPTH_DEVICE,
+	XPT_DEPTH_PERIPH
+} xpt_traverse_depth;
+
+struct xpt_traverse_config {
+	xpt_traverse_depth	depth;
+	void			*tr_func;
+	void			*tr_arg;
+};
+
+typedef	int	xpt_busfunc_t (struct cam_eb *bus, void *arg);
+typedef	int	xpt_targetfunc_t (struct cam_et *target, void *arg);
+typedef	int	xpt_devicefunc_t (struct cam_ed *device, void *arg);
+typedef	int	xpt_periphfunc_t (struct cam_periph *periph, void *arg);
+typedef int	xpt_pdrvfunc_t (struct periph_driver **pdrv, void *arg);
+
+/* Transport layer configuration information */
+static struct xpt_softc xsoftc;
+
+MTX_SYSINIT(xpt_topo_init, &xsoftc.xpt_topo_lock, "XPT topology lock", MTX_DEF);
+
+SYSCTL_INT(_kern_cam, OID_AUTO, boot_delay, CTLFLAG_RDTUN,
+           &xsoftc.boot_delay, 0, "Bus registration wait time");
+SYSCTL_UINT(_kern_cam, OID_AUTO, xpt_generation, CTLFLAG_RD,
+	    &xsoftc.xpt_generation, 0, "CAM peripheral generation count");
+
+struct cam_doneq {
+	struct mtx_padalign	cam_doneq_mtx;
+	STAILQ_HEAD(, ccb_hdr)	cam_doneq;
+	int			cam_doneq_sleep;
+};
+
+static struct cam_doneq cam_doneqs[MAXCPU];
+static int cam_num_doneqs;
+static struct proc *cam_proc;
+
+SYSCTL_INT(_kern_cam, OID_AUTO, num_doneqs, CTLFLAG_RDTUN,
+           &cam_num_doneqs, 0, "Number of completion queues/threads");
+
+struct cam_periph *xpt_periph;
+
+static periph_init_t xpt_periph_init;
+
+static struct periph_driver xpt_driver =
+{
+	xpt_periph_init, "xpt",
+	TAILQ_HEAD_INITIALIZER(xpt_driver.units), /* generation */ 0,
+	CAM_PERIPH_DRV_EARLY
+};
+
+PERIPHDRIVER_DECLARE(xpt, xpt_driver);
+
+static d_open_t xptopen;
+static d_close_t xptclose;
+static d_ioctl_t xptioctl;
+static d_ioctl_t xptdoioctl;
+
+static struct cdevsw xpt_cdevsw = {
+	.d_version =	D_VERSION,
+	.d_flags =	0,
+	.d_open =	xptopen,
+	.d_close =	xptclose,
+	.d_ioctl =	xptioctl,
+	.d_name =	"xpt",
+};
+
+/* Storage for debugging datastructures */
+struct cam_path *cam_dpath;
+u_int32_t cam_dflags = CAM_DEBUG_FLAGS;
+SYSCTL_UINT(_kern_cam, OID_AUTO, dflags, CTLFLAG_RWTUN,
+	&cam_dflags, 0, "Enabled debug flags");
+u_int32_t cam_debug_delay = CAM_DEBUG_DELAY;
+SYSCTL_UINT(_kern_cam, OID_AUTO, debug_delay, CTLFLAG_RWTUN,
+	&cam_debug_delay, 0, "Delay in us after each debug message");
+
+/* Our boot-time initialization hook */
+static int cam_module_event_handler(module_t, int /*modeventtype_t*/, void *);
+
+static moduledata_t cam_moduledata = {
+	"cam",
+	cam_module_event_handler,
+	NULL
+};
+
+static int	xpt_init(void *);
+
+DECLARE_MODULE(cam, cam_moduledata, SI_SUB_CONFIGURE, SI_ORDER_SECOND);
+MODULE_VERSION(cam, 1);
+
+
+static void		xpt_async_bcast(struct async_list *async_head,
+					u_int32_t async_code,
+					struct cam_path *path,
+					void *async_arg);
+static path_id_t xptnextfreepathid(void);
+static path_id_t xptpathid(const char *sim_name, int sim_unit, int sim_bus);
+static union ccb *xpt_get_ccb(struct cam_periph *periph);
+/* Made below two methods non-static so that they can be directly called by mmc_da.c while making out own CCB */
+union ccb *xpt_get_ccb_nowait(struct cam_periph *periph);
+void	 xpt_run_allocq(struct cam_periph *periph, int sleep);
+static void	 xpt_run_allocq_task(void *context, int pending);
+static void	 xpt_run_devq(struct cam_devq *devq);
+static timeout_t xpt_release_devq_timeout;
+static void	 xpt_release_simq_timeout(void *arg) __unused;
+static void	 xpt_acquire_bus(struct cam_eb *bus);
+static void	 xpt_release_bus(struct cam_eb *bus);
+static uint32_t	 xpt_freeze_devq_device(struct cam_ed *dev, u_int count);
+static int	 xpt_release_devq_device(struct cam_ed *dev, u_int count,
+		    int run_queue);
+static struct cam_et*
+		 xpt_alloc_target(struct cam_eb *bus, target_id_t target_id);
+static void	 xpt_acquire_target(struct cam_et *target);
+static void	 xpt_release_target(struct cam_et *target);
+static struct cam_eb*
+		 xpt_find_bus(path_id_t path_id);
+static struct cam_et*
+		 xpt_find_target(struct cam_eb *bus, target_id_t target_id);
+static struct cam_ed*
+		 xpt_find_device(struct cam_et *target, lun_id_t lun_id);
+static void	 xpt_config(void *arg);
+static int	 xpt_schedule_dev(struct camq *queue, cam_pinfo *dev_pinfo,
+				 u_int32_t new_priority);
+static xpt_devicefunc_t xptpassannouncefunc;
+static void	 xptaction(struct cam_sim *sim, union ccb *work_ccb);
+static void	 xptpoll(struct cam_sim *sim);
+static void	 camisr_runqueue(void);
+static void	 xpt_done_process(struct ccb_hdr *ccb_h);
+static void	 xpt_done_td(void *);
+static dev_match_ret	xptbusmatch(struct dev_match_pattern *patterns,
+				    u_int num_patterns, struct cam_eb *bus);
+static dev_match_ret	xptdevicematch(struct dev_match_pattern *patterns,
+				       u_int num_patterns,
+				       struct cam_ed *device);
+static dev_match_ret	xptperiphmatch(struct dev_match_pattern *patterns,
+				       u_int num_patterns,
+				       struct cam_periph *periph);
+static xpt_busfunc_t	xptedtbusfunc;
+static xpt_targetfunc_t	xptedttargetfunc;
+static xpt_devicefunc_t	xptedtdevicefunc;
+static xpt_periphfunc_t	xptedtperiphfunc;
+static xpt_pdrvfunc_t	xptplistpdrvfunc;
+static xpt_periphfunc_t	xptplistperiphfunc;
+static int		xptedtmatch(struct ccb_dev_match *cdm);
+static int		xptperiphlistmatch(struct ccb_dev_match *cdm);
+static int		xptbustraverse(struct cam_eb *start_bus,
+				       xpt_busfunc_t *tr_func, void *arg);
+static int		xpttargettraverse(struct cam_eb *bus,
+					  struct cam_et *start_target,
+					  xpt_targetfunc_t *tr_func, void *arg);
+static int		xptdevicetraverse(struct cam_et *target,
+					  struct cam_ed *start_device,
+					  xpt_devicefunc_t *tr_func, void *arg);
+static int		xptperiphtraverse(struct cam_ed *device,
+					  struct cam_periph *start_periph,
+					  xpt_periphfunc_t *tr_func, void *arg);
+static int		xptpdrvtraverse(struct periph_driver **start_pdrv,
+					xpt_pdrvfunc_t *tr_func, void *arg);
+static int		xptpdperiphtraverse(struct periph_driver **pdrv,
+					    struct cam_periph *start_periph,
+					    xpt_periphfunc_t *tr_func,
+					    void *arg);
+static xpt_busfunc_t	xptdefbusfunc;
+static xpt_targetfunc_t	xptdeftargetfunc;
+static xpt_devicefunc_t	xptdefdevicefunc;
+static xpt_periphfunc_t	xptdefperiphfunc;
+static void		xpt_finishconfig_task(void *context, int pending);
+static void		xpt_dev_async_default(u_int32_t async_code,
+					      struct cam_eb *bus,
+					      struct cam_et *target,
+					      struct cam_ed *device,
+					      void *async_arg);
+static struct cam_ed *	xpt_alloc_device_default(struct cam_eb *bus,
+						 struct cam_et *target,
+						 lun_id_t lun_id);
+static xpt_devicefunc_t	xptsetasyncfunc;
+static xpt_busfunc_t	xptsetasyncbusfunc;
+static cam_status	xptregister(struct cam_periph *periph,
+				    void *arg);
+static __inline int device_is_queued(struct cam_ed *device);
+
+static __inline int
+xpt_schedule_devq(struct cam_devq *devq, struct cam_ed *dev)
+{
+	int	retval;
+
+	mtx_assert(&devq->send_mtx, MA_OWNED);
+	if ((dev->ccbq.queue.entries > 0) &&
+	    (dev->ccbq.dev_openings > 0) &&
+	    (dev->ccbq.queue.qfrozen_cnt == 0)) {
+		/*
+		 * The priority of a device waiting for controller
+		 * resources is that of the highest priority CCB
+		 * enqueued.
+		 */
+		retval =
+		    xpt_schedule_dev(&devq->send_queue,
+				     &dev->devq_entry,
+				     CAMQ_GET_PRIO(&dev->ccbq.queue));
+	} else {
+		retval = 0;
+	}
+	return (retval);
+}
+
+static __inline int
+device_is_queued(struct cam_ed *device)
+{
+	return (device->devq_entry.index != CAM_UNQUEUED_INDEX);
+}
+
+static void
+xpt_periph_init()
+{
+	make_dev(&xpt_cdevsw, 0, UID_ROOT, GID_OPERATOR, 0600, "xpt0");
+}
+
+static int
+xptopen(struct cdev *dev, int flags, int fmt, struct thread *td)
+{
+
+	/*
+	 * Only allow read-write access.
+	 */
+	if (((flags & FWRITE) == 0) || ((flags & FREAD) == 0))
+		return(EPERM);
+
+	/*
+	 * We don't allow nonblocking access.
+	 */
+	if ((flags & O_NONBLOCK) != 0) {
+		printf("%s: can't do nonblocking access\n", devtoname(dev));
+		return(ENODEV);
+	}
+
+	return(0);
+}
+
+static int
+xptclose(struct cdev *dev, int flag, int fmt, struct thread *td)
+{
+
+	return(0);
+}
+
+/*
+ * Don't automatically grab the xpt softc lock here even though this is going
+ * through the xpt device.  The xpt device is really just a back door for
+ * accessing other devices and SIMs, so the right thing to do is to grab
+ * the appropriate SIM lock once the bus/SIM is located.
+ */
+static int
+xptioctl(struct cdev *dev, u_long cmd, caddr_t addr, int flag, struct thread *td)
+{
+	int error;
+
+	if ((error = xptdoioctl(dev, cmd, addr, flag, td)) == ENOTTY) {
+		error = cam_compat_ioctl(dev, cmd, addr, flag, td, xptdoioctl);
+	}
+	return (error);
+}
+
+static int
+xptdoioctl(struct cdev *dev, u_long cmd, caddr_t addr, int flag, struct thread *td)
+{
+	int error;
+
+	error = 0;
+
+	switch(cmd) {
+	/*
+	 * For the transport layer CAMIOCOMMAND ioctl, we really only want
+	 * to accept CCB types that don't quite make sense to send through a
+	 * passthrough driver. XPT_PATH_INQ is an exception to this, as stated
+	 * in the CAM spec.
+	 */
+	case CAMIOCOMMAND: {
+		union ccb *ccb;
+		union ccb *inccb;
+		struct cam_eb *bus;
+
+		inccb = (union ccb *)addr;
+#if defined(BUF_TRACKING) || defined(FULL_BUF_TRACKING)
+		if (inccb->ccb_h.func_code == XPT_SCSI_IO)
+			inccb->csio.bio = NULL;
+#endif
+
+		if (inccb->ccb_h.flags & CAM_UNLOCKED)
+			return (EINVAL);
+
+		bus = xpt_find_bus(inccb->ccb_h.path_id);
+		if (bus == NULL)
+			return (EINVAL);
+
+		switch (inccb->ccb_h.func_code) {
+		case XPT_SCAN_BUS:
+		case XPT_RESET_BUS:
+			if (inccb->ccb_h.target_id != CAM_TARGET_WILDCARD ||
+			    inccb->ccb_h.target_lun != CAM_LUN_WILDCARD) {
+				xpt_release_bus(bus);
+				return (EINVAL);
+			}
+			break;
+		case XPT_SCAN_TGT:
+			if (inccb->ccb_h.target_id == CAM_TARGET_WILDCARD ||
+			    inccb->ccb_h.target_lun != CAM_LUN_WILDCARD) {
+				xpt_release_bus(bus);
+				return (EINVAL);
+			}
+			break;
+		default:
+			break;
+		}
+
+		switch(inccb->ccb_h.func_code) {
+		case XPT_SCAN_BUS:
+		case XPT_RESET_BUS:
+		case XPT_PATH_INQ:
+		case XPT_ENG_INQ:
+		case XPT_SCAN_LUN:
+		case XPT_SCAN_TGT:
+
+			ccb = xpt_alloc_ccb();
+
+			/*
+			 * Create a path using the bus, target, and lun the
+			 * user passed in.
+			 */
+			if (xpt_create_path(&ccb->ccb_h.path, NULL,
+					    inccb->ccb_h.path_id,
+					    inccb->ccb_h.target_id,
+					    inccb->ccb_h.target_lun) !=
+					    CAM_REQ_CMP){
+				error = EINVAL;
+				xpt_free_ccb(ccb);
+				break;
+			}
+			/* Ensure all of our fields are correct */
+			xpt_setup_ccb(&ccb->ccb_h, ccb->ccb_h.path,
+				      inccb->ccb_h.pinfo.priority);
+			xpt_merge_ccb(ccb, inccb);
+			xpt_path_lock(ccb->ccb_h.path);
+			cam_periph_runccb(ccb, NULL, 0, 0, NULL);
+			xpt_path_unlock(ccb->ccb_h.path);
+			bcopy(ccb, inccb, sizeof(union ccb));
+			xpt_free_path(ccb->ccb_h.path);
+			xpt_free_ccb(ccb);
+			break;
+
+		case XPT_DEBUG: {
+			union ccb ccb;
+
+			/*
+			 * This is an immediate CCB, so it's okay to
+			 * allocate it on the stack.
+			 */
+
+			/*
+			 * Create a path using the bus, target, and lun the
+			 * user passed in.
+			 */
+			if (xpt_create_path(&ccb.ccb_h.path, NULL,
+					    inccb->ccb_h.path_id,
+					    inccb->ccb_h.target_id,
+					    inccb->ccb_h.target_lun) !=
+					    CAM_REQ_CMP){
+				error = EINVAL;
+				break;
+			}
+			/* Ensure all of our fields are correct */
+			xpt_setup_ccb(&ccb.ccb_h, ccb.ccb_h.path,
+				      inccb->ccb_h.pinfo.priority);
+			xpt_merge_ccb(&ccb, inccb);
+			xpt_action(&ccb);
+			bcopy(&ccb, inccb, sizeof(union ccb));
+			xpt_free_path(ccb.ccb_h.path);
+			break;
+
+		}
+		case XPT_DEV_MATCH: {
+			struct cam_periph_map_info mapinfo;
+			struct cam_path *old_path;
+
+			/*
+			 * We can't deal with physical addresses for this
+			 * type of transaction.
+			 */
+			if ((inccb->ccb_h.flags & CAM_DATA_MASK) !=
+			    CAM_DATA_VADDR) {
+				error = EINVAL;
+				break;
+			}
+
+			/*
+			 * Save this in case the caller had it set to
+			 * something in particular.
+			 */
+			old_path = inccb->ccb_h.path;
+
+			/*
+			 * We really don't need a path for the matching
+			 * code.  The path is needed because of the
+			 * debugging statements in xpt_action().  They
+			 * assume that the CCB has a valid path.
+			 */
+			inccb->ccb_h.path = xpt_periph->path;
+
+			bzero(&mapinfo, sizeof(mapinfo));
+
+#ifndef __rtems__
+			/*
+			 * Map the pattern and match buffers into kernel
+			 * virtual address space.
+			 */
+			error = cam_periph_mapmem(inccb, &mapinfo, MAXPHYS);
+
+			if (error) {
+				inccb->ccb_h.path = old_path;
+				break;
+			}
+
+#endif
+			/*
+			 * This is an immediate CCB, we can send it on directly.
+			 */
+			xpt_action(inccb);
+
+#ifndef __rtems__
+			/*
+			 * Map the buffers back into user space.
+			 */
+			cam_periph_unmapmem(inccb, &mapinfo);
+
+#endif
+			inccb->ccb_h.path = old_path;
+
+			error = 0;
+			break;
+		}
+		default:
+			error = ENOTSUP;
+			break;
+		}
+		xpt_release_bus(bus);
+		break;
+	}
+	/*
+	 * This is the getpassthru ioctl. It takes a XPT_GDEVLIST ccb as input,
+	 * with the periphal driver name and unit name filled in.  The other
+	 * fields don't really matter as input.  The passthrough driver name
+	 * ("pass"), and unit number are passed back in the ccb.  The current
+	 * device generation number, and the index into the device peripheral
+	 * driver list, and the status are also passed back.  Note that
+	 * since we do everything in one pass, unlike the XPT_GDEVLIST ccb,
+	 * we never return a status of CAM_GDEVLIST_LIST_CHANGED.  It is
+	 * (or rather should be) impossible for the device peripheral driver
+	 * list to change since we look at the whole thing in one pass, and
+	 * we do it with lock protection.
+	 *
+	 */
+	case CAMGETPASSTHRU: {
+		union ccb *ccb;
+		struct cam_periph *periph;
+		struct periph_driver **p_drv;
+		char   *name;
+		u_int unit;
+		int base_periph_found;
+
+		ccb = (union ccb *)addr;
+		unit = ccb->cgdl.unit_number;
+		name = ccb->cgdl.periph_name;
+		base_periph_found = 0;
+#if defined(BUF_TRACKING) || defined(FULL_BUF_TRACKING)
+		if (ccb->ccb_h.func_code == XPT_SCSI_IO)
+			ccb->csio.bio = NULL;
+#endif
+
+		/*
+		 * Sanity check -- make sure we don't get a null peripheral
+		 * driver name.
+		 */
+		if (*ccb->cgdl.periph_name == '\0') {
+			error = EINVAL;
+			break;
+		}
+
+		/* Keep the list from changing while we traverse it */
+		xpt_lock_buses();
+
+		/* first find our driver in the list of drivers */
+		for (p_drv = periph_drivers; *p_drv != NULL; p_drv++)
+			if (strcmp((*p_drv)->driver_name, name) == 0)
+				break;
+
+		if (*p_drv == NULL) {
+			xpt_unlock_buses();
+			ccb->ccb_h.status = CAM_REQ_CMP_ERR;
+			ccb->cgdl.status = CAM_GDEVLIST_ERROR;
+			*ccb->cgdl.periph_name = '\0';
+			ccb->cgdl.unit_number = 0;
+			error = ENOENT;
+			break;
+		}
+
+		/*
+		 * Run through every peripheral instance of this driver
+		 * and check to see whether it matches the unit passed
+		 * in by the user.  If it does, get out of the loops and
+		 * find the passthrough driver associated with that
+		 * peripheral driver.
+		 */
+		for (periph = TAILQ_FIRST(&(*p_drv)->units); periph != NULL;
+		     periph = TAILQ_NEXT(periph, unit_links)) {
+
+			if (periph->unit_number == unit)
+				break;
+		}
+		/*
+		 * If we found the peripheral driver that the user passed
+		 * in, go through all of the peripheral drivers for that
+		 * particular device and look for a passthrough driver.
+		 */
+		if (periph != NULL) {
+			struct cam_ed *device;
+			int i;
+
+			base_periph_found = 1;
+			device = periph->path->device;
+			for (i = 0, periph = SLIST_FIRST(&device->periphs);
+			     periph != NULL;
+			     periph = SLIST_NEXT(periph, periph_links), i++) {
+				/*
+				 * Check to see whether we have a
+				 * passthrough device or not.
+				 */
+				if (strcmp(periph->periph_name, "pass") == 0) {
+					/*
+					 * Fill in the getdevlist fields.
+					 */
+					strcpy(ccb->cgdl.periph_name,
+					       periph->periph_name);
+					ccb->cgdl.unit_number =
+						periph->unit_number;
+					if (SLIST_NEXT(periph, periph_links))
+						ccb->cgdl.status =
+							CAM_GDEVLIST_MORE_DEVS;
+					else
+						ccb->cgdl.status =
+						       CAM_GDEVLIST_LAST_DEVICE;
+					ccb->cgdl.generation =
+						device->generation;
+					ccb->cgdl.index = i;
+					/*
+					 * Fill in some CCB header fields
+					 * that the user may want.
+					 */
+					ccb->ccb_h.path_id =
+						periph->path->bus->path_id;
+					ccb->ccb_h.target_id =
+						periph->path->target->target_id;
+					ccb->ccb_h.target_lun =
+						periph->path->device->lun_id;
+					ccb->ccb_h.status = CAM_REQ_CMP;
+					break;
+				}
+			}
+		}
+
+		/*
+		 * If the periph is null here, one of two things has
+		 * happened.  The first possibility is that we couldn't
+		 * find the unit number of the particular peripheral driver
+		 * that the user is asking about.  e.g. the user asks for
+		 * the passthrough driver for "da11".  We find the list of
+		 * "da" peripherals all right, but there is no unit 11.
+		 * The other possibility is that we went through the list
+		 * of peripheral drivers attached to the device structure,
+		 * but didn't find one with the name "pass".  Either way,
+		 * we return ENOENT, since we couldn't find something.
+		 */
+		if (periph == NULL) {
+			ccb->ccb_h.status = CAM_REQ_CMP_ERR;
+			ccb->cgdl.status = CAM_GDEVLIST_ERROR;
+			*ccb->cgdl.periph_name = '\0';
+			ccb->cgdl.unit_number = 0;
+			error = ENOENT;
+			/*
+			 * It is unfortunate that this is even necessary,
+			 * but there are many, many clueless users out there.
+			 * If this is true, the user is looking for the
+			 * passthrough driver, but doesn't have one in his
+			 * kernel.
+			 */
+			if (base_periph_found == 1) {
+				printf("xptioctl: pass driver is not in the "
+				       "kernel\n");
+				printf("xptioctl: put \"device pass\" in "
+				       "your kernel config file\n");
+			}
+		}
+		xpt_unlock_buses();
+		break;
+		}
+	default:
+		error = ENOTTY;
+		break;
+	}
+
+	return(error);
+}
+
+static int
+cam_module_event_handler(module_t mod, int what, void *arg)
+{
+	int error;
+
+	switch (what) {
+	case MOD_LOAD:
+		if ((error = xpt_init(NULL)) != 0)
+			return (error);
+		break;
+	case MOD_UNLOAD:
+		return EBUSY;
+	default:
+		return EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
+static struct xpt_proto *
+xpt_proto_find(cam_proto proto)
+{
+	struct xpt_proto **pp;
+
+	SET_FOREACH(pp, cam_xpt_proto_set) {
+		if ((*pp)->proto == proto)
+			return *pp;
+	}
+
+	return NULL;
+}
+
+static void
+xpt_rescan_done(struct cam_periph *periph, union ccb *done_ccb)
+{
+
+	if (done_ccb->ccb_h.ppriv_ptr1 == NULL) {
+		xpt_free_path(done_ccb->ccb_h.path);
+		xpt_free_ccb(done_ccb);
+	} else {
+		done_ccb->ccb_h.cbfcnp = done_ccb->ccb_h.ppriv_ptr1;
+		(*done_ccb->ccb_h.cbfcnp)(periph, done_ccb);
+	}
+#ifndef __rtems__
+	xpt_release_boot();
+#endif
+}
+
+/* thread to handle bus rescans */
+static void
+xpt_scanner_thread(void *dummy)
+{
+	union ccb	*ccb;
+	struct cam_path	 path;
+
+	xpt_lock_buses();
+	for (;;) {
+		if (TAILQ_EMPTY(&xsoftc.ccb_scanq))
+			msleep(&xsoftc.ccb_scanq, &xsoftc.xpt_topo_lock, PRIBIO,
+			       "-", 0);
+		if ((ccb = (union ccb *)TAILQ_FIRST(&xsoftc.ccb_scanq)) != NULL) {
+			TAILQ_REMOVE(&xsoftc.ccb_scanq, &ccb->ccb_h, sim_links.tqe);
+			xpt_unlock_buses();
+
+                        //printf("xpt_scanner_thread is firing on path ");
+                        //xpt_print_path(ccb->ccb_h.path);printf("\n");
+			/*
+			 * Since lock can be dropped inside and path freed
+			 * by completion callback even before return here,
+			 * take our own path copy for reference.
+			 */
+			xpt_copy_path(&path, ccb->ccb_h.path);
+			xpt_path_lock(&path);
+			xpt_action(ccb);
+			xpt_path_unlock(&path);
+			xpt_release_path(&path);
+
+			xpt_lock_buses();
+		}
+	}
+}
+
+void
+xpt_rescan(union ccb *ccb)
+{
+	struct ccb_hdr *hdr;
+
+	/* Prepare request */
+	if (ccb->ccb_h.path->target->target_id == CAM_TARGET_WILDCARD &&
+	    ccb->ccb_h.path->device->lun_id == CAM_LUN_WILDCARD)
+		ccb->ccb_h.func_code = XPT_SCAN_BUS;
+	else if (ccb->ccb_h.path->target->target_id != CAM_TARGET_WILDCARD &&
+	    ccb->ccb_h.path->device->lun_id == CAM_LUN_WILDCARD)
+		ccb->ccb_h.func_code = XPT_SCAN_TGT;
+	else if (ccb->ccb_h.path->target->target_id != CAM_TARGET_WILDCARD &&
+	    ccb->ccb_h.path->device->lun_id != CAM_LUN_WILDCARD)
+		ccb->ccb_h.func_code = XPT_SCAN_LUN;
+	else {
+		xpt_print(ccb->ccb_h.path, "illegal scan path\n");
+		xpt_free_path(ccb->ccb_h.path);
+		xpt_free_ccb(ccb);
+		return;
+	}
+	CAM_DEBUG(ccb->ccb_h.path, CAM_DEBUG_TRACE,
+	    ("xpt_rescan: func %#x %s\n", ccb->ccb_h.func_code,
+ 		xpt_action_name(ccb->ccb_h.func_code)));
+
+	ccb->ccb_h.ppriv_ptr1 = ccb->ccb_h.cbfcnp;
+	ccb->ccb_h.cbfcnp = xpt_rescan_done;
+	xpt_setup_ccb(&ccb->ccb_h, ccb->ccb_h.path, CAM_PRIORITY_XPT);
+	/* Don't make duplicate entries for the same paths. */
+	xpt_lock_buses();
+	if (ccb->ccb_h.ppriv_ptr1 == NULL) {
+		TAILQ_FOREACH(hdr, &xsoftc.ccb_scanq, sim_links.tqe) {
+			if (xpt_path_comp(hdr->path, ccb->ccb_h.path) == 0) {
+				wakeup(&xsoftc.ccb_scanq);
+				xpt_unlock_buses();
+				xpt_print(ccb->ccb_h.path, "rescan already queued\n");
+				xpt_free_path(ccb->ccb_h.path);
+				xpt_free_ccb(ccb);
+				return;
+			}
+		}
+	}
+	TAILQ_INSERT_TAIL(&xsoftc.ccb_scanq, &ccb->ccb_h, sim_links.tqe);
+	xsoftc.buses_to_config++;
+	wakeup(&xsoftc.ccb_scanq);
+	xpt_unlock_buses();
+}
+
+/* Functions accessed by the peripheral drivers */
+static int
+xpt_init(void *dummy)
+{
+	struct cam_sim *xpt_sim;
+	struct cam_path *path;
+	struct cam_devq *devq;
+	cam_status status;
+	int error, i;
+
+	TAILQ_INIT(&xsoftc.xpt_busses);
+	TAILQ_INIT(&xsoftc.ccb_scanq);
+	STAILQ_INIT(&xsoftc.highpowerq);
+	xsoftc.num_highpower = CAM_MAX_HIGHPOWER;
+
+	mtx_init(&xsoftc.xpt_lock, "XPT lock", NULL, MTX_DEF);
+	mtx_init(&xsoftc.xpt_highpower_lock, "XPT highpower lock", NULL, MTX_DEF);
+	xsoftc.xpt_taskq = taskqueue_create("CAM XPT task", M_WAITOK,
+	    taskqueue_thread_enqueue, /*context*/&xsoftc.xpt_taskq);
+
+#ifdef CAM_BOOT_DELAY
+	/*
+	 * Override this value at compile time to assist our users
+	 * who don't use loader to boot a kernel.
+	 */
+	xsoftc.boot_delay = CAM_BOOT_DELAY;
+#endif
+	/*
+	 * The xpt layer is, itself, the equivalent of a SIM.
+	 * Allow 16 ccbs in the ccb pool for it.  This should
+	 * give decent parallelism when we probe buses and
+	 * perform other XPT functions.
+	 */
+	devq = cam_simq_alloc(16);
+	xpt_sim = cam_sim_alloc(xptaction,
+				xptpoll,
+				"xpt",
+				/*softc*/NULL,
+				/*unit*/0,
+				/*mtx*/&xsoftc.xpt_lock,
+				/*max_dev_transactions*/0,
+				/*max_tagged_dev_transactions*/0,
+				devq);
+	if (xpt_sim == NULL)
+		return (ENOMEM);
+
+	mtx_lock(&xsoftc.xpt_lock);
+	if ((status = xpt_bus_register(xpt_sim, NULL, 0)) != CAM_SUCCESS) {
+		mtx_unlock(&xsoftc.xpt_lock);
+		printf("xpt_init: xpt_bus_register failed with status %#x,"
+		       " failing attach\n", status);
+		return (EINVAL);
+	}
+	mtx_unlock(&xsoftc.xpt_lock);
+
+	/*
+	 * Looking at the XPT from the SIM layer, the XPT is
+	 * the equivalent of a peripheral driver.  Allocate
+	 * a peripheral driver entry for us.
+	 */
+	if ((status = xpt_create_path(&path, NULL, CAM_XPT_PATH_ID,
+				      CAM_TARGET_WILDCARD,
+				      CAM_LUN_WILDCARD)) != CAM_REQ_CMP) {
+		printf("xpt_init: xpt_create_path failed with status %#x,"
+		       " failing attach\n", status);
+		return (EINVAL);
+	}
+	xpt_path_lock(path);
+	cam_periph_alloc(xptregister, NULL, NULL, NULL, "xpt", CAM_PERIPH_BIO,
+			 path, NULL, 0, xpt_sim);
+	xpt_path_unlock(path);
+	xpt_free_path(path);
+
+	if (cam_num_doneqs < 1)
+		cam_num_doneqs = 1 + mp_ncpus / 6;
+	else if (cam_num_doneqs > MAXCPU)
+		cam_num_doneqs = MAXCPU;
+	for (i = 0; i < cam_num_doneqs; i++) {
+		mtx_init(&cam_doneqs[i].cam_doneq_mtx, "CAM doneq", NULL,
+		    MTX_DEF);
+		STAILQ_INIT(&cam_doneqs[i].cam_doneq);
+		error = kproc_kthread_add(xpt_done_td, &cam_doneqs[i],
+		    &cam_proc, NULL, 0, 0, "cam", "doneq%d", i);
+		if (error != 0) {
+			cam_num_doneqs = i;
+			break;
+		}
+	}
+	if (cam_num_doneqs < 1) {
+		printf("xpt_init: Cannot init completion queues "
+		       "- failing attach\n");
+		return (ENOMEM);
+	}
+	/*
+	 * Register a callback for when interrupts are enabled.
+	 */
+	xsoftc.xpt_config_hook =
+	    (struct intr_config_hook *)malloc(sizeof(struct intr_config_hook),
+					      M_CAMXPT, M_NOWAIT | M_ZERO);
+	if (xsoftc.xpt_config_hook == NULL) {
+		printf("xpt_init: Cannot malloc config hook "
+		       "- failing attach\n");
+		return (ENOMEM);
+	}
+	xsoftc.xpt_config_hook->ich_func = xpt_config;
+	if (config_intrhook_establish(xsoftc.xpt_config_hook) != 0) {
+		free (xsoftc.xpt_config_hook, M_CAMXPT);
+		printf("xpt_init: config_intrhook_establish failed "
+		       "- failing attach\n");
+	}
+
+	return (0);
+}
+
+static cam_status
+xptregister(struct cam_periph *periph, void *arg)
+{
+	struct cam_sim *xpt_sim;
+
+	if (periph == NULL) {
+		printf("xptregister: periph was NULL!!\n");
+		return(CAM_REQ_CMP_ERR);
+	}
+
+	xpt_sim = (struct cam_sim *)arg;
+	xpt_sim->softc = periph;
+	xpt_periph = periph;
+	periph->softc = NULL;
+
+	return(CAM_REQ_CMP);
+}
+
+int32_t
+xpt_add_periph(struct cam_periph *periph)
+{
+	struct cam_ed *device;
+	int32_t	 status;
+
+	TASK_INIT(&periph->periph_run_task, 0, xpt_run_allocq_task, periph);
+	device = periph->path->device;
+	status = CAM_REQ_CMP;
+	if (device != NULL) {
+		mtx_lock(&device->target->bus->eb_mtx);
+		device->generation++;
+		SLIST_INSERT_HEAD(&device->periphs, periph, periph_links);
+		mtx_unlock(&device->target->bus->eb_mtx);
+		atomic_add_32(&xsoftc.xpt_generation, 1);
+	}
+
+	return (status);
+}
+
+void
+xpt_remove_periph(struct cam_periph *periph)
+{
+	struct cam_ed *device;
+
+	device = periph->path->device;
+	if (device != NULL) {
+		mtx_lock(&device->target->bus->eb_mtx);
+		device->generation++;
+		SLIST_REMOVE(&device->periphs, periph, cam_periph, periph_links);
+		mtx_unlock(&device->target->bus->eb_mtx);
+		atomic_add_32(&xsoftc.xpt_generation, 1);
+	}
+}
+
+
+void
+xpt_announce_periph(struct cam_periph *periph, char *announce_string)
+{
+	struct	cam_path *path = periph->path;
+	struct  xpt_proto *proto;
+
+	cam_periph_assert(periph, MA_OWNED);
+	periph->flags |= CAM_PERIPH_ANNOUNCED;
+
+	printf("%s%d at %s%d bus %d scbus%d target %d lun %jx\n",
+	       periph->periph_name, periph->unit_number,
+	       path->bus->sim->sim_name,
+	       path->bus->sim->unit_number,
+	       path->bus->sim->bus_id,
+	       path->bus->path_id,
+	       path->target->target_id,
+	       (uintmax_t)path->device->lun_id);
+	printf("%s%d: ", periph->periph_name, periph->unit_number);
+	proto = xpt_proto_find(path->device->protocol);
+	if (proto)
+		proto->ops->announce(path->device);
+	else
+		printf("%s%d: Unknown protocol device %d\n",
+		    periph->periph_name, periph->unit_number,
+		    path->device->protocol);
+	if (path->device->serial_num_len > 0) {
+		/* Don't wrap the screen  - print only the first 60 chars */
+		printf("%s%d: Serial Number %.60s\n", periph->periph_name,
+		       periph->unit_number, path->device->serial_num);
+	}
+	/* Announce transport details. */
+	path->bus->xport->ops->announce(periph);
+	/* Announce command queueing. */
+	if (path->device->inq_flags & SID_CmdQue
+	 || path->device->flags & CAM_DEV_TAG_AFTER_COUNT) {
+		printf("%s%d: Command Queueing enabled\n",
+		       periph->periph_name, periph->unit_number);
+	}
+	/* Announce caller's details if they've passed in. */
+	if (announce_string != NULL)
+		printf("%s%d: %s\n", periph->periph_name,
+		       periph->unit_number, announce_string);
+}
+
+void
+xpt_announce_quirks(struct cam_periph *periph, int quirks, char *bit_string)
+{
+	if (quirks != 0) {
+		printf("%s%d: quirks=0x%b\n", periph->periph_name,
+		    periph->unit_number, quirks, bit_string);
+	}
+}
+
+void
+xpt_denounce_periph(struct cam_periph *periph)
+{
+	struct	cam_path *path = periph->path;
+	struct  xpt_proto *proto;
+
+	cam_periph_assert(periph, MA_OWNED);
+	printf("%s%d at %s%d bus %d scbus%d target %d lun %jx\n",
+	       periph->periph_name, periph->unit_number,
+	       path->bus->sim->sim_name,
+	       path->bus->sim->unit_number,
+	       path->bus->sim->bus_id,
+	       path->bus->path_id,
+	       path->target->target_id,
+	       (uintmax_t)path->device->lun_id);
+	printf("%s%d: ", periph->periph_name, periph->unit_number);
+	proto = xpt_proto_find(path->device->protocol);
+	if (proto)
+		proto->ops->denounce(path->device);
+	else
+		printf("%s%d: Unknown protocol device %d\n",
+		    periph->periph_name, periph->unit_number,
+		    path->device->protocol);
+	if (path->device->serial_num_len > 0)
+		printf(" s/n %.60s", path->device->serial_num);
+	printf(" detached\n");
+}
+
+
+int
+xpt_getattr(char *buf, size_t len, const char *attr, struct cam_path *path)
+{
+	int ret = -1, l, o;
+	struct ccb_dev_advinfo cdai;
+	struct scsi_vpd_id_descriptor *idd;
+
+	xpt_path_assert(path, MA_OWNED);
+
+	memset(&cdai, 0, sizeof(cdai));
+	xpt_setup_ccb(&cdai.ccb_h, path, CAM_PRIORITY_NORMAL);
+	cdai.ccb_h.func_code = XPT_DEV_ADVINFO;
+	cdai.flags = CDAI_FLAG_NONE;
+	cdai.bufsiz = len;
+
+	if (!strcmp(attr, "GEOM::ident"))
+		cdai.buftype = CDAI_TYPE_SERIAL_NUM;
+	else if (!strcmp(attr, "GEOM::physpath"))
+		cdai.buftype = CDAI_TYPE_PHYS_PATH;
+	else if (strcmp(attr, "GEOM::lunid") == 0 ||
+		 strcmp(attr, "GEOM::lunname") == 0) {
+		cdai.buftype = CDAI_TYPE_SCSI_DEVID;
+		cdai.bufsiz = CAM_SCSI_DEVID_MAXLEN;
+	} else
+		goto out;
+
+	cdai.buf = malloc(cdai.bufsiz, M_CAMXPT, M_NOWAIT|M_ZERO);
+	if (cdai.buf == NULL) {
+		ret = ENOMEM;
+		goto out;
+	}
+	xpt_action((union ccb *)&cdai); /* can only be synchronous */
+	if ((cdai.ccb_h.status & CAM_DEV_QFRZN) != 0)
+		cam_release_devq(cdai.ccb_h.path, 0, 0, 0, FALSE);
+	if (cdai.provsiz == 0)
+		goto out;
+	if (cdai.buftype == CDAI_TYPE_SCSI_DEVID) {
+		if (strcmp(attr, "GEOM::lunid") == 0) {
+			idd = scsi_get_devid((struct scsi_vpd_device_id *)cdai.buf,
+			    cdai.provsiz, scsi_devid_is_lun_naa);
+			if (idd == NULL)
+				idd = scsi_get_devid((struct scsi_vpd_device_id *)cdai.buf,
+				    cdai.provsiz, scsi_devid_is_lun_eui64);
+			if (idd == NULL)
+				idd = scsi_get_devid((struct scsi_vpd_device_id *)cdai.buf,
+				    cdai.provsiz, scsi_devid_is_lun_uuid);
+			if (idd == NULL)
+				idd = scsi_get_devid((struct scsi_vpd_device_id *)cdai.buf,
+				    cdai.provsiz, scsi_devid_is_lun_md5);
+		} else
+			idd = NULL;
+		if (idd == NULL)
+			idd = scsi_get_devid((struct scsi_vpd_device_id *)cdai.buf,
+			    cdai.provsiz, scsi_devid_is_lun_t10);
+		if (idd == NULL)
+			idd = scsi_get_devid((struct scsi_vpd_device_id *)cdai.buf,
+			    cdai.provsiz, scsi_devid_is_lun_name);
+		if (idd == NULL)
+			goto out;
+		ret = 0;
+		if ((idd->proto_codeset & SVPD_ID_CODESET_MASK) == SVPD_ID_CODESET_ASCII) {
+			if (idd->length < len) {
+				for (l = 0; l < idd->length; l++)
+					buf[l] = idd->identifier[l] ?
+					    idd->identifier[l] : ' ';
+				buf[l] = 0;
+			} else
+				ret = EFAULT;
+		} else if ((idd->proto_codeset & SVPD_ID_CODESET_MASK) == SVPD_ID_CODESET_UTF8) {
+			l = strnlen(idd->identifier, idd->length);
+			if (l < len) {
+				bcopy(idd->identifier, buf, l);
+				buf[l] = 0;
+			} else
+				ret = EFAULT;
+		} else if ((idd->id_type & SVPD_ID_TYPE_MASK) == SVPD_ID_TYPE_UUID
+		    && idd->identifier[0] == 0x10) {
+			if ((idd->length - 2) * 2 + 4 < len) {
+				for (l = 2, o = 0; l < idd->length; l++) {
+					if (l == 6 || l == 8 || l == 10 || l == 12)
+					    o += sprintf(buf + o, "-");
+					o += sprintf(buf + o, "%02x",
+					    idd->identifier[l]);
+				}
+			} else
+				ret = EFAULT;
+		} else {
+			if (idd->length * 2 < len) {
+				for (l = 0; l < idd->length; l++)
+					sprintf(buf + l * 2, "%02x",
+					    idd->identifier[l]);
+			} else
+				ret = EFAULT;
+		}
+	} else {
+		ret = 0;
+		if (strlcpy(buf, cdai.buf, len) >= len)
+			ret = EFAULT;
+	}
+
+out:
+	if (cdai.buf != NULL)
+		free(cdai.buf, M_CAMXPT);
+	return ret;
+}
+
+static dev_match_ret
+xptbusmatch(struct dev_match_pattern *patterns, u_int num_patterns,
+	    struct cam_eb *bus)
+{
+	dev_match_ret retval;
+	u_int i;
+
+	retval = DM_RET_NONE;
+
+	/*
+	 * If we aren't given something to match against, that's an error.
+	 */
+	if (bus == NULL)
+		return(DM_RET_ERROR);
+
+	/*
+	 * If there are no match entries, then this bus matches no
+	 * matter what.
+	 */
+	if ((patterns == NULL) || (num_patterns == 0))
+		return(DM_RET_DESCEND | DM_RET_COPY);
+
+	for (i = 0; i < num_patterns; i++) {
+		struct bus_match_pattern *cur_pattern;
+
+		/*
+		 * If the pattern in question isn't for a bus node, we
+		 * aren't interested.  However, we do indicate to the
+		 * calling routine that we should continue descending the
+		 * tree, since the user wants to match against lower-level
+		 * EDT elements.
+		 */
+		if (patterns[i].type != DEV_MATCH_BUS) {
+			if ((retval & DM_RET_ACTION_MASK) == DM_RET_NONE)
+				retval |= DM_RET_DESCEND;
+			continue;
+		}
+
+		cur_pattern = &patterns[i].pattern.bus_pattern;
+
+		/*
+		 * If they want to match any bus node, we give them any
+		 * device node.
+		 */
+		if (cur_pattern->flags == BUS_MATCH_ANY) {
+			/* set the copy flag */
+			retval |= DM_RET_COPY;
+
+			/*
+			 * If we've already decided on an action, go ahead
+			 * and return.
+			 */
+			if ((retval & DM_RET_ACTION_MASK) != DM_RET_NONE)
+				return(retval);
+		}
+
+		/*
+		 * Not sure why someone would do this...
+		 */
+		if (cur_pattern->flags == BUS_MATCH_NONE)
+			continue;
+
+		if (((cur_pattern->flags & BUS_MATCH_PATH) != 0)
+		 && (cur_pattern->path_id != bus->path_id))
+			continue;
+
+		if (((cur_pattern->flags & BUS_MATCH_BUS_ID) != 0)
+		 && (cur_pattern->bus_id != bus->sim->bus_id))
+			continue;
+
+		if (((cur_pattern->flags & BUS_MATCH_UNIT) != 0)
+		 && (cur_pattern->unit_number != bus->sim->unit_number))
+			continue;
+
+		if (((cur_pattern->flags & BUS_MATCH_NAME) != 0)
+		 && (strncmp(cur_pattern->dev_name, bus->sim->sim_name,
+			     DEV_IDLEN) != 0))
+			continue;
+
+		/*
+		 * If we get to this point, the user definitely wants
+		 * information on this bus.  So tell the caller to copy the
+		 * data out.
+		 */
+		retval |= DM_RET_COPY;
+
+		/*
+		 * If the return action has been set to descend, then we
+		 * know that we've already seen a non-bus matching
+		 * expression, therefore we need to further descend the tree.
+		 * This won't change by continuing around the loop, so we
+		 * go ahead and return.  If we haven't seen a non-bus
+		 * matching expression, we keep going around the loop until
+		 * we exhaust the matching expressions.  We'll set the stop
+		 * flag once we fall out of the loop.
+		 */
+		if ((retval & DM_RET_ACTION_MASK) == DM_RET_DESCEND)
+			return(retval);
+	}
+
+	/*
+	 * If the return action hasn't been set to descend yet, that means
+	 * we haven't seen anything other than bus matching patterns.  So
+	 * tell the caller to stop descending the tree -- the user doesn't
+	 * want to match against lower level tree elements.
+	 */
+	if ((retval & DM_RET_ACTION_MASK) == DM_RET_NONE)
+		retval |= DM_RET_STOP;
+
+	return(retval);
+}
+
+static dev_match_ret
+xptdevicematch(struct dev_match_pattern *patterns, u_int num_patterns,
+	       struct cam_ed *device)
+{
+	dev_match_ret retval;
+	u_int i;
+
+	retval = DM_RET_NONE;
+
+	/*
+	 * If we aren't given something to match against, that's an error.
+	 */
+	if (device == NULL)
+		return(DM_RET_ERROR);
+
+	/*
+	 * If there are no match entries, then this device matches no
+	 * matter what.
+	 */
+	if ((patterns == NULL) || (num_patterns == 0))
+		return(DM_RET_DESCEND | DM_RET_COPY);
+
+	for (i = 0; i < num_patterns; i++) {
+		struct device_match_pattern *cur_pattern;
+		struct scsi_vpd_device_id *device_id_page;
+
+		/*
+		 * If the pattern in question isn't for a device node, we
+		 * aren't interested.
+		 */
+		if (patterns[i].type != DEV_MATCH_DEVICE) {
+			if ((patterns[i].type == DEV_MATCH_PERIPH)
+			 && ((retval & DM_RET_ACTION_MASK) == DM_RET_NONE))
+				retval |= DM_RET_DESCEND;
+			continue;
+		}
+
+		cur_pattern = &patterns[i].pattern.device_pattern;
+
+		/* Error out if mutually exclusive options are specified. */
+		if ((cur_pattern->flags & (DEV_MATCH_INQUIRY|DEV_MATCH_DEVID))
+		 == (DEV_MATCH_INQUIRY|DEV_MATCH_DEVID))
+			return(DM_RET_ERROR);
+
+		/*
+		 * If they want to match any device node, we give them any
+		 * device node.
+		 */
+		if (cur_pattern->flags == DEV_MATCH_ANY)
+			goto copy_dev_node;
+
+		/*
+		 * Not sure why someone would do this...
+		 */
+		if (cur_pattern->flags == DEV_MATCH_NONE)
+			continue;
+
+		if (((cur_pattern->flags & DEV_MATCH_PATH) != 0)
+		 && (cur_pattern->path_id != device->target->bus->path_id))
+			continue;
+
+		if (((cur_pattern->flags & DEV_MATCH_TARGET) != 0)
+		 && (cur_pattern->target_id != device->target->target_id))
+			continue;
+
+		if (((cur_pattern->flags & DEV_MATCH_LUN) != 0)
+		 && (cur_pattern->target_lun != device->lun_id))
+			continue;
+
+#ifndef __rtems__
+		if (((cur_pattern->flags & DEV_MATCH_INQUIRY) != 0)
+		 && (cam_quirkmatch((caddr_t)&device->inq_data,
+				    (caddr_t)&cur_pattern->data.inq_pat,
+				    1, sizeof(cur_pattern->data.inq_pat),
+				    scsi_static_inquiry_match) == NULL))
+			continue;
+
+		device_id_page = (struct scsi_vpd_device_id *)device->device_id;
+		if (((cur_pattern->flags & DEV_MATCH_DEVID) != 0)
+		 && (device->device_id_len < SVPD_DEVICE_ID_HDR_LEN
+		  || scsi_devid_match((uint8_t *)device_id_page->desc_list,
+				      device->device_id_len
+				    - SVPD_DEVICE_ID_HDR_LEN,
+				      cur_pattern->data.devid_pat.id,
+				      cur_pattern->data.devid_pat.id_len) != 0))
+			continue;
+
+#endif /* __rtems__ */
+copy_dev_node:
+		/*
+		 * If we get to this point, the user definitely wants
+		 * information on this device.  So tell the caller to copy
+		 * the data out.
+		 */
+		retval |= DM_RET_COPY;
+
+		/*
+		 * If the return action has been set to descend, then we
+		 * know that we've already seen a peripheral matching
+		 * expression, therefore we need to further descend the tree.
+		 * This won't change by continuing around the loop, so we
+		 * go ahead and return.  If we haven't seen a peripheral
+		 * matching expression, we keep going around the loop until
+		 * we exhaust the matching expressions.  We'll set the stop
+		 * flag once we fall out of the loop.
+		 */
+		if ((retval & DM_RET_ACTION_MASK) == DM_RET_DESCEND)
+			return(retval);
+	}
+
+	/*
+	 * If the return action hasn't been set to descend yet, that means
+	 * we haven't seen any peripheral matching patterns.  So tell the
+	 * caller to stop descending the tree -- the user doesn't want to
+	 * match against lower level tree elements.
+	 */
+	if ((retval & DM_RET_ACTION_MASK) == DM_RET_NONE)
+		retval |= DM_RET_STOP;
+
+	return(retval);
+}
+
+/*
+ * Match a single peripheral against any number of match patterns.
+ */
+static dev_match_ret
+xptperiphmatch(struct dev_match_pattern *patterns, u_int num_patterns,
+	       struct cam_periph *periph)
+{
+	dev_match_ret retval;
+	u_int i;
+
+	/*
+	 * If we aren't given something to match against, that's an error.
+	 */
+	if (periph == NULL)
+		return(DM_RET_ERROR);
+
+	/*
+	 * If there are no match entries, then this peripheral matches no
+	 * matter what.
+	 */
+	if ((patterns == NULL) || (num_patterns == 0))
+		return(DM_RET_STOP | DM_RET_COPY);
+
+	/*
+	 * There aren't any nodes below a peripheral node, so there's no
+	 * reason to descend the tree any further.
+	 */
+	retval = DM_RET_STOP;
+
+	for (i = 0; i < num_patterns; i++) {
+		struct periph_match_pattern *cur_pattern;
+
+		/*
+		 * If the pattern in question isn't for a peripheral, we
+		 * aren't interested.
+		 */
+		if (patterns[i].type != DEV_MATCH_PERIPH)
+			continue;
+
+		cur_pattern = &patterns[i].pattern.periph_pattern;
+
+		/*
+		 * If they want to match on anything, then we will do so.
+		 */
+		if (cur_pattern->flags == PERIPH_MATCH_ANY) {
+			/* set the copy flag */
+			retval |= DM_RET_COPY;
+
+			/*
+			 * We've already set the return action to stop,
+			 * since there are no nodes below peripherals in
+			 * the tree.
+			 */
+			return(retval);
+		}
+
+		/*
+		 * Not sure why someone would do this...
+		 */
+		if (cur_pattern->flags == PERIPH_MATCH_NONE)
+			continue;
+
+		if (((cur_pattern->flags & PERIPH_MATCH_PATH) != 0)
+		 && (cur_pattern->path_id != periph->path->bus->path_id))
+			continue;
+
+		/*
+		 * For the target and lun id's, we have to make sure the
+		 * target and lun pointers aren't NULL.  The xpt peripheral
+		 * has a wildcard target and device.
+		 */
+		if (((cur_pattern->flags & PERIPH_MATCH_TARGET) != 0)
+		 && ((periph->path->target == NULL)
+		 ||(cur_pattern->target_id != periph->path->target->target_id)))
+			continue;
+
+		if (((cur_pattern->flags & PERIPH_MATCH_LUN) != 0)
+		 && ((periph->path->device == NULL)
+		 || (cur_pattern->target_lun != periph->path->device->lun_id)))
+			continue;
+
+		if (((cur_pattern->flags & PERIPH_MATCH_UNIT) != 0)
+		 && (cur_pattern->unit_number != periph->unit_number))
+			continue;
+
+		if (((cur_pattern->flags & PERIPH_MATCH_NAME) != 0)
+		 && (strncmp(cur_pattern->periph_name, periph->periph_name,
+			     DEV_IDLEN) != 0))
+			continue;
+
+		/*
+		 * If we get to this point, the user definitely wants
+		 * information on this peripheral.  So tell the caller to
+		 * copy the data out.
+		 */
+		retval |= DM_RET_COPY;
+
+		/*
+		 * The return action has already been set to stop, since
+		 * peripherals don't have any nodes below them in the EDT.
+		 */
+		return(retval);
+	}
+
+	/*
+	 * If we get to this point, the peripheral that was passed in
+	 * doesn't match any of the patterns.
+	 */
+	return(retval);
+}
+
+static int
+xptedtbusfunc(struct cam_eb *bus, void *arg)
+{
+	struct ccb_dev_match *cdm;
+	struct cam_et *target;
+	dev_match_ret retval;
+
+	cdm = (struct ccb_dev_match *)arg;
+
+	/*
+	 * If our position is for something deeper in the tree, that means
+	 * that we've already seen this node.  So, we keep going down.
+	 */
+	if ((cdm->pos.position_type & CAM_DEV_POS_BUS)
+	 && (cdm->pos.cookie.bus == bus)
+	 && (cdm->pos.position_type & CAM_DEV_POS_TARGET)
+	 && (cdm->pos.cookie.target != NULL))
+		retval = DM_RET_DESCEND;
+	else
+		retval = xptbusmatch(cdm->patterns, cdm->num_patterns, bus);
+
+	/*
+	 * If we got an error, bail out of the search.
+	 */
+	if ((retval & DM_RET_ACTION_MASK) == DM_RET_ERROR) {
+		cdm->status = CAM_DEV_MATCH_ERROR;
+		return(0);
+	}
+
+	/*
+	 * If the copy flag is set, copy this bus out.
+	 */
+	if (retval & DM_RET_COPY) {
+		int spaceleft, j;
+
+		spaceleft = cdm->match_buf_len - (cdm->num_matches *
+			sizeof(struct dev_match_result));
+
+		/*
+		 * If we don't have enough space to put in another
+		 * match result, save our position and tell the
+		 * user there are more devices to check.
+		 */
+		if (spaceleft < sizeof(struct dev_match_result)) {
+			bzero(&cdm->pos, sizeof(cdm->pos));
+			cdm->pos.position_type =
+				CAM_DEV_POS_EDT | CAM_DEV_POS_BUS;
+
+			cdm->pos.cookie.bus = bus;
+			cdm->pos.generations[CAM_BUS_GENERATION]=
+				xsoftc.bus_generation;
+			cdm->status = CAM_DEV_MATCH_MORE;
+			return(0);
+		}
+		j = cdm->num_matches;
+		cdm->num_matches++;
+		cdm->matches[j].type = DEV_MATCH_BUS;
+		cdm->matches[j].result.bus_result.path_id = bus->path_id;
+		cdm->matches[j].result.bus_result.bus_id = bus->sim->bus_id;
+		cdm->matches[j].result.bus_result.unit_number =
+			bus->sim->unit_number;
+		strncpy(cdm->matches[j].result.bus_result.dev_name,
+			bus->sim->sim_name, DEV_IDLEN);
+	}
+
+	/*
+	 * If the user is only interested in buses, there's no
+	 * reason to descend to the next level in the tree.
+	 */
+	if ((retval & DM_RET_ACTION_MASK) == DM_RET_STOP)
+		return(1);
+
+	/*
+	 * If there is a target generation recorded, check it to
+	 * make sure the target list hasn't changed.
+	 */
+	mtx_lock(&bus->eb_mtx);
+	if ((cdm->pos.position_type & CAM_DEV_POS_BUS)
+	 && (cdm->pos.cookie.bus == bus)
+	 && (cdm->pos.position_type & CAM_DEV_POS_TARGET)
+	 && (cdm->pos.cookie.target != NULL)) {
+		if ((cdm->pos.generations[CAM_TARGET_GENERATION] !=
+		    bus->generation)) {
+			mtx_unlock(&bus->eb_mtx);
+			cdm->status = CAM_DEV_MATCH_LIST_CHANGED;
+			return (0);
+		}
+		target = (struct cam_et *)cdm->pos.cookie.target;
+		target->refcount++;
+	} else
+		target = NULL;
+	mtx_unlock(&bus->eb_mtx);
+
+	return (xpttargettraverse(bus, target, xptedttargetfunc, arg));
+}
+
+static int
+xptedttargetfunc(struct cam_et *target, void *arg)
+{
+	struct ccb_dev_match *cdm;
+	struct cam_eb *bus;
+	struct cam_ed *device;
+
+	cdm = (struct ccb_dev_match *)arg;
+	bus = target->bus;
+
+	/*
+	 * If there is a device list generation recorded, check it to
+	 * make sure the device list hasn't changed.
+	 */
+	mtx_lock(&bus->eb_mtx);
+	if ((cdm->pos.position_type & CAM_DEV_POS_BUS)
+	 && (cdm->pos.cookie.bus == bus)
+	 && (cdm->pos.position_type & CAM_DEV_POS_TARGET)
+	 && (cdm->pos.cookie.target == target)
+	 && (cdm->pos.position_type & CAM_DEV_POS_DEVICE)
+	 && (cdm->pos.cookie.device != NULL)) {
+		if (cdm->pos.generations[CAM_DEV_GENERATION] !=
+		    target->generation) {
+			mtx_unlock(&bus->eb_mtx);
+			cdm->status = CAM_DEV_MATCH_LIST_CHANGED;
+			return(0);
+		}
+		device = (struct cam_ed *)cdm->pos.cookie.device;
+		device->refcount++;
+	} else
+		device = NULL;
+	mtx_unlock(&bus->eb_mtx);
+
+	return (xptdevicetraverse(target, device, xptedtdevicefunc, arg));
+}
+
+static int
+xptedtdevicefunc(struct cam_ed *device, void *arg)
+{
+	struct cam_eb *bus;
+	struct cam_periph *periph;
+	struct ccb_dev_match *cdm;
+	dev_match_ret retval;
+
+	cdm = (struct ccb_dev_match *)arg;
+	bus = device->target->bus;
+
+	/*
+	 * If our position is for something deeper in the tree, that means
+	 * that we've already seen this node.  So, we keep going down.
+	 */
+	if ((cdm->pos.position_type & CAM_DEV_POS_DEVICE)
+	 && (cdm->pos.cookie.device == device)
+	 && (cdm->pos.position_type & CAM_DEV_POS_PERIPH)
+	 && (cdm->pos.cookie.periph != NULL))
+		retval = DM_RET_DESCEND;
+	else
+		retval = xptdevicematch(cdm->patterns, cdm->num_patterns,
+					device);
+
+	if ((retval & DM_RET_ACTION_MASK) == DM_RET_ERROR) {
+		cdm->status = CAM_DEV_MATCH_ERROR;
+		return(0);
+	}
+
+	/*
+	 * If the copy flag is set, copy this device out.
+	 */
+	if (retval & DM_RET_COPY) {
+		int spaceleft, j;
+
+		spaceleft = cdm->match_buf_len - (cdm->num_matches *
+			sizeof(struct dev_match_result));
+
+		/*
+		 * If we don't have enough space to put in another
+		 * match result, save our position and tell the
+		 * user there are more devices to check.
+		 */
+		if (spaceleft < sizeof(struct dev_match_result)) {
+			bzero(&cdm->pos, sizeof(cdm->pos));
+			cdm->pos.position_type =
+				CAM_DEV_POS_EDT | CAM_DEV_POS_BUS |
+				CAM_DEV_POS_TARGET | CAM_DEV_POS_DEVICE;
+
+			cdm->pos.cookie.bus = device->target->bus;
+			cdm->pos.generations[CAM_BUS_GENERATION]=
+				xsoftc.bus_generation;
+			cdm->pos.cookie.target = device->target;
+			cdm->pos.generations[CAM_TARGET_GENERATION] =
+				device->target->bus->generation;
+			cdm->pos.cookie.device = device;
+			cdm->pos.generations[CAM_DEV_GENERATION] =
+				device->target->generation;
+			cdm->status = CAM_DEV_MATCH_MORE;
+			return(0);
+		}
+		j = cdm->num_matches;
+		cdm->num_matches++;
+		cdm->matches[j].type = DEV_MATCH_DEVICE;
+		cdm->matches[j].result.device_result.path_id =
+			device->target->bus->path_id;
+		cdm->matches[j].result.device_result.target_id =
+			device->target->target_id;
+		cdm->matches[j].result.device_result.target_lun =
+			device->lun_id;
+		cdm->matches[j].result.device_result.protocol =
+			device->protocol;
+		bcopy(&device->inq_data,
+		      &cdm->matches[j].result.device_result.inq_data,
+		      sizeof(struct scsi_inquiry_data));
+		bcopy(&device->ident_data,
+		      &cdm->matches[j].result.device_result.ident_data,
+		      sizeof(struct ata_params));
+
+		/* Let the user know whether this device is unconfigured */
+		if (device->flags & CAM_DEV_UNCONFIGURED)
+			cdm->matches[j].result.device_result.flags =
+				DEV_RESULT_UNCONFIGURED;
+		else
+			cdm->matches[j].result.device_result.flags =
+				DEV_RESULT_NOFLAG;
+	}
+
+	/*
+	 * If the user isn't interested in peripherals, don't descend
+	 * the tree any further.
+	 */
+	if ((retval & DM_RET_ACTION_MASK) == DM_RET_STOP)
+		return(1);
+
+	/*
+	 * If there is a peripheral list generation recorded, make sure
+	 * it hasn't changed.
+	 */
+	xpt_lock_buses();
+	mtx_lock(&bus->eb_mtx);
+	if ((cdm->pos.position_type & CAM_DEV_POS_BUS)
+	 && (cdm->pos.cookie.bus == bus)
+	 && (cdm->pos.position_type & CAM_DEV_POS_TARGET)
+	 && (cdm->pos.cookie.target == device->target)
+	 && (cdm->pos.position_type & CAM_DEV_POS_DEVICE)
+	 && (cdm->pos.cookie.device == device)
+	 && (cdm->pos.position_type & CAM_DEV_POS_PERIPH)
+	 && (cdm->pos.cookie.periph != NULL)) {
+		if (cdm->pos.generations[CAM_PERIPH_GENERATION] !=
+		    device->generation) {
+			mtx_unlock(&bus->eb_mtx);
+			xpt_unlock_buses();
+			cdm->status = CAM_DEV_MATCH_LIST_CHANGED;
+			return(0);
+		}
+		periph = (struct cam_periph *)cdm->pos.cookie.periph;
+		periph->refcount++;
+	} else
+		periph = NULL;
+	mtx_unlock(&bus->eb_mtx);
+	xpt_unlock_buses();
+
+	return (xptperiphtraverse(device, periph, xptedtperiphfunc, arg));
+}
+
+static int
+xptedtperiphfunc(struct cam_periph *periph, void *arg)
+{
+	struct ccb_dev_match *cdm;
+	dev_match_ret retval;
+
+	cdm = (struct ccb_dev_match *)arg;
+
+	retval = xptperiphmatch(cdm->patterns, cdm->num_patterns, periph);
+
+	if ((retval & DM_RET_ACTION_MASK) == DM_RET_ERROR) {
+		cdm->status = CAM_DEV_MATCH_ERROR;
+		return(0);
+	}
+
+	/*
+	 * If the copy flag is set, copy this peripheral out.
+	 */
+	if (retval & DM_RET_COPY) {
+		int spaceleft, j;
+
+		spaceleft = cdm->match_buf_len - (cdm->num_matches *
+			sizeof(struct dev_match_result));
+
+		/*
+		 * If we don't have enough space to put in another
+		 * match result, save our position and tell the
+		 * user there are more devices to check.
+		 */
+		if (spaceleft < sizeof(struct dev_match_result)) {
+			bzero(&cdm->pos, sizeof(cdm->pos));
+			cdm->pos.position_type =
+				CAM_DEV_POS_EDT | CAM_DEV_POS_BUS |
+				CAM_DEV_POS_TARGET | CAM_DEV_POS_DEVICE |
+				CAM_DEV_POS_PERIPH;
+
+			cdm->pos.cookie.bus = periph->path->bus;
+			cdm->pos.generations[CAM_BUS_GENERATION]=
+				xsoftc.bus_generation;
+			cdm->pos.cookie.target = periph->path->target;
+			cdm->pos.generations[CAM_TARGET_GENERATION] =
+				periph->path->bus->generation;
+			cdm->pos.cookie.device = periph->path->device;
+			cdm->pos.generations[CAM_DEV_GENERATION] =
+				periph->path->target->generation;
+			cdm->pos.cookie.periph = periph;
+			cdm->pos.generations[CAM_PERIPH_GENERATION] =
+				periph->path->device->generation;
+			cdm->status = CAM_DEV_MATCH_MORE;
+			return(0);
+		}
+
+		j = cdm->num_matches;
+		cdm->num_matches++;
+		cdm->matches[j].type = DEV_MATCH_PERIPH;
+		cdm->matches[j].result.periph_result.path_id =
+			periph->path->bus->path_id;
+		cdm->matches[j].result.periph_result.target_id =
+			periph->path->target->target_id;
+		cdm->matches[j].result.periph_result.target_lun =
+			periph->path->device->lun_id;
+		cdm->matches[j].result.periph_result.unit_number =
+			periph->unit_number;
+		strncpy(cdm->matches[j].result.periph_result.periph_name,
+			periph->periph_name, DEV_IDLEN);
+	}
+
+	return(1);
+}
+
+static int
+xptedtmatch(struct ccb_dev_match *cdm)
+{
+	struct cam_eb *bus;
+	int ret;
+
+	cdm->num_matches = 0;
+
+	/*
+	 * Check the bus list generation.  If it has changed, the user
+	 * needs to reset everything and start over.
+	 */
+	xpt_lock_buses();
+	if ((cdm->pos.position_type & CAM_DEV_POS_BUS)
+	 && (cdm->pos.cookie.bus != NULL)) {
+		if (cdm->pos.generations[CAM_BUS_GENERATION] !=
+		    xsoftc.bus_generation) {
+			xpt_unlock_buses();
+			cdm->status = CAM_DEV_MATCH_LIST_CHANGED;
+			return(0);
+		}
+		bus = (struct cam_eb *)cdm->pos.cookie.bus;
+		bus->refcount++;
+	} else
+		bus = NULL;
+	xpt_unlock_buses();
+
+	ret = xptbustraverse(bus, xptedtbusfunc, cdm);
+
+	/*
+	 * If we get back 0, that means that we had to stop before fully
+	 * traversing the EDT.  It also means that one of the subroutines
+	 * has set the status field to the proper value.  If we get back 1,
+	 * we've fully traversed the EDT and copied out any matching entries.
+	 */
+	if (ret == 1)
+		cdm->status = CAM_DEV_MATCH_LAST;
+
+	return(ret);
+}
+
+static int
+xptplistpdrvfunc(struct periph_driver **pdrv, void *arg)
+{
+	struct cam_periph *periph;
+	struct ccb_dev_match *cdm;
+
+	cdm = (struct ccb_dev_match *)arg;
+
+	xpt_lock_buses();
+	if ((cdm->pos.position_type & CAM_DEV_POS_PDPTR)
+	 && (cdm->pos.cookie.pdrv == pdrv)
+	 && (cdm->pos.position_type & CAM_DEV_POS_PERIPH)
+	 && (cdm->pos.cookie.periph != NULL)) {
+		if (cdm->pos.generations[CAM_PERIPH_GENERATION] !=
+		    (*pdrv)->generation) {
+			xpt_unlock_buses();
+			cdm->status = CAM_DEV_MATCH_LIST_CHANGED;
+			return(0);
+		}
+		periph = (struct cam_periph *)cdm->pos.cookie.periph;
+		periph->refcount++;
+	} else
+		periph = NULL;
+	xpt_unlock_buses();
+
+	return (xptpdperiphtraverse(pdrv, periph, xptplistperiphfunc, arg));
+}
+
+static int
+xptplistperiphfunc(struct cam_periph *periph, void *arg)
+{
+	struct ccb_dev_match *cdm;
+	dev_match_ret retval;
+
+	cdm = (struct ccb_dev_match *)arg;
+
+	retval = xptperiphmatch(cdm->patterns, cdm->num_patterns, periph);
+
+	if ((retval & DM_RET_ACTION_MASK) == DM_RET_ERROR) {
+		cdm->status = CAM_DEV_MATCH_ERROR;
+		return(0);
+	}
+
+	/*
+	 * If the copy flag is set, copy this peripheral out.
+	 */
+	if (retval & DM_RET_COPY) {
+		int spaceleft, j;
+
+		spaceleft = cdm->match_buf_len - (cdm->num_matches *
+			sizeof(struct dev_match_result));
+
+		/*
+		 * If we don't have enough space to put in another
+		 * match result, save our position and tell the
+		 * user there are more devices to check.
+		 */
+		if (spaceleft < sizeof(struct dev_match_result)) {
+			struct periph_driver **pdrv;
+
+			pdrv = NULL;
+			bzero(&cdm->pos, sizeof(cdm->pos));
+			cdm->pos.position_type =
+				CAM_DEV_POS_PDRV | CAM_DEV_POS_PDPTR |
+				CAM_DEV_POS_PERIPH;
+
+			/*
+			 * This may look a bit non-sensical, but it is
+			 * actually quite logical.  There are very few
+			 * peripheral drivers, and bloating every peripheral
+			 * structure with a pointer back to its parent
+			 * peripheral driver linker set entry would cost
+			 * more in the long run than doing this quick lookup.
+			 */
+			for (pdrv = periph_drivers; *pdrv != NULL; pdrv++) {
+				if (strcmp((*pdrv)->driver_name,
+				    periph->periph_name) == 0)
+					break;
+			}
+
+			if (*pdrv == NULL) {
+				cdm->status = CAM_DEV_MATCH_ERROR;
+				return(0);
+			}
+
+			cdm->pos.cookie.pdrv = pdrv;
+			/*
+			 * The periph generation slot does double duty, as
+			 * does the periph pointer slot.  They are used for
+			 * both edt and pdrv lookups and positioning.
+			 */
+			cdm->pos.cookie.periph = periph;
+			cdm->pos.generations[CAM_PERIPH_GENERATION] =
+				(*pdrv)->generation;
+			cdm->status = CAM_DEV_MATCH_MORE;
+			return(0);
+		}
+
+		j = cdm->num_matches;
+		cdm->num_matches++;
+		cdm->matches[j].type = DEV_MATCH_PERIPH;
+		cdm->matches[j].result.periph_result.path_id =
+			periph->path->bus->path_id;
+
+		/*
+		 * The transport layer peripheral doesn't have a target or
+		 * lun.
+		 */
+		if (periph->path->target)
+			cdm->matches[j].result.periph_result.target_id =
+				periph->path->target->target_id;
+		else
+			cdm->matches[j].result.periph_result.target_id =
+				CAM_TARGET_WILDCARD;
+
+		if (periph->path->device)
+			cdm->matches[j].result.periph_result.target_lun =
+				periph->path->device->lun_id;
+		else
+			cdm->matches[j].result.periph_result.target_lun =
+				CAM_LUN_WILDCARD;
+
+		cdm->matches[j].result.periph_result.unit_number =
+			periph->unit_number;
+		strncpy(cdm->matches[j].result.periph_result.periph_name,
+			periph->periph_name, DEV_IDLEN);
+	}
+
+	return(1);
+}
+
+static int
+xptperiphlistmatch(struct ccb_dev_match *cdm)
+{
+	int ret;
+
+	cdm->num_matches = 0;
+
+	/*
+	 * At this point in the edt traversal function, we check the bus
+	 * list generation to make sure that no buses have been added or
+	 * removed since the user last sent a XPT_DEV_MATCH ccb through.
+	 * For the peripheral driver list traversal function, however, we
+	 * don't have to worry about new peripheral driver types coming or
+	 * going; they're in a linker set, and therefore can't change
+	 * without a recompile.
+	 */
+
+	if ((cdm->pos.position_type & CAM_DEV_POS_PDPTR)
+	 && (cdm->pos.cookie.pdrv != NULL))
+		ret = xptpdrvtraverse(
+				(struct periph_driver **)cdm->pos.cookie.pdrv,
+				xptplistpdrvfunc, cdm);
+	else
+		ret = xptpdrvtraverse(NULL, xptplistpdrvfunc, cdm);
+
+	/*
+	 * If we get back 0, that means that we had to stop before fully
+	 * traversing the peripheral driver tree.  It also means that one of
+	 * the subroutines has set the status field to the proper value.  If
+	 * we get back 1, we've fully traversed the EDT and copied out any
+	 * matching entries.
+	 */
+	if (ret == 1)
+		cdm->status = CAM_DEV_MATCH_LAST;
+
+	return(ret);
+}
+
+static int
+xptbustraverse(struct cam_eb *start_bus, xpt_busfunc_t *tr_func, void *arg)
+{
+	struct cam_eb *bus, *next_bus;
+	int retval;
+
+	retval = 1;
+	if (start_bus)
+		bus = start_bus;
+	else {
+		xpt_lock_buses();
+		bus = TAILQ_FIRST(&xsoftc.xpt_busses);
+		if (bus == NULL) {
+			xpt_unlock_buses();
+			return (retval);
+		}
+		bus->refcount++;
+		xpt_unlock_buses();
+	}
+	for (; bus != NULL; bus = next_bus) {
+		retval = tr_func(bus, arg);
+		if (retval == 0) {
+			xpt_release_bus(bus);
+			break;
+		}
+		xpt_lock_buses();
+		next_bus = TAILQ_NEXT(bus, links);
+		if (next_bus)
+			next_bus->refcount++;
+		xpt_unlock_buses();
+		xpt_release_bus(bus);
+	}
+	return(retval);
+}
+
+static int
+xpttargettraverse(struct cam_eb *bus, struct cam_et *start_target,
+		  xpt_targetfunc_t *tr_func, void *arg)
+{
+	struct cam_et *target, *next_target;
+	int retval;
+
+	retval = 1;
+	if (start_target)
+		target = start_target;
+	else {
+		mtx_lock(&bus->eb_mtx);
+		target = TAILQ_FIRST(&bus->et_entries);
+		if (target == NULL) {
+			mtx_unlock(&bus->eb_mtx);
+			return (retval);
+		}
+		target->refcount++;
+		mtx_unlock(&bus->eb_mtx);
+	}
+	for (; target != NULL; target = next_target) {
+		retval = tr_func(target, arg);
+		if (retval == 0) {
+			xpt_release_target(target);
+			break;
+		}
+		mtx_lock(&bus->eb_mtx);
+		next_target = TAILQ_NEXT(target, links);
+		if (next_target)
+			next_target->refcount++;
+		mtx_unlock(&bus->eb_mtx);
+		xpt_release_target(target);
+	}
+	return(retval);
+}
+
+static int
+xptdevicetraverse(struct cam_et *target, struct cam_ed *start_device,
+		  xpt_devicefunc_t *tr_func, void *arg)
+{
+	struct cam_eb *bus;
+	struct cam_ed *device, *next_device;
+	int retval;
+
+	retval = 1;
+	bus = target->bus;
+	if (start_device)
+		device = start_device;
+	else {
+		mtx_lock(&bus->eb_mtx);
+		device = TAILQ_FIRST(&target->ed_entries);
+		if (device == NULL) {
+			mtx_unlock(&bus->eb_mtx);
+			return (retval);
+		}
+		device->refcount++;
+		mtx_unlock(&bus->eb_mtx);
+	}
+	for (; device != NULL; device = next_device) {
+		mtx_lock(&device->device_mtx);
+		retval = tr_func(device, arg);
+		mtx_unlock(&device->device_mtx);
+		if (retval == 0) {
+			xpt_release_device(device);
+			break;
+		}
+		mtx_lock(&bus->eb_mtx);
+		next_device = TAILQ_NEXT(device, links);
+		if (next_device)
+			next_device->refcount++;
+		mtx_unlock(&bus->eb_mtx);
+		xpt_release_device(device);
+	}
+	return(retval);
+}
+
+static int
+xptperiphtraverse(struct cam_ed *device, struct cam_periph *start_periph,
+		  xpt_periphfunc_t *tr_func, void *arg)
+{
+	struct cam_eb *bus;
+	struct cam_periph *periph, *next_periph;
+	int retval;
+
+	retval = 1;
+
+	bus = device->target->bus;
+	if (start_periph)
+		periph = start_periph;
+	else {
+		xpt_lock_buses();
+		mtx_lock(&bus->eb_mtx);
+		periph = SLIST_FIRST(&device->periphs);
+		while (periph != NULL && (periph->flags & CAM_PERIPH_FREE) != 0)
+			periph = SLIST_NEXT(periph, periph_links);
+		if (periph == NULL) {
+			mtx_unlock(&bus->eb_mtx);
+			xpt_unlock_buses();
+			return (retval);
+		}
+		periph->refcount++;
+		mtx_unlock(&bus->eb_mtx);
+		xpt_unlock_buses();
+	}
+	for (; periph != NULL; periph = next_periph) {
+		retval = tr_func(periph, arg);
+		if (retval == 0) {
+			cam_periph_release_locked(periph);
+			break;
+		}
+		xpt_lock_buses();
+		mtx_lock(&bus->eb_mtx);
+		next_periph = SLIST_NEXT(periph, periph_links);
+		while (next_periph != NULL &&
+		    (next_periph->flags & CAM_PERIPH_FREE) != 0)
+			next_periph = SLIST_NEXT(next_periph, periph_links);
+		if (next_periph)
+			next_periph->refcount++;
+		mtx_unlock(&bus->eb_mtx);
+		xpt_unlock_buses();
+		cam_periph_release_locked(periph);
+	}
+	return(retval);
+}
+
+static int
+xptpdrvtraverse(struct periph_driver **start_pdrv,
+		xpt_pdrvfunc_t *tr_func, void *arg)
+{
+	struct periph_driver **pdrv;
+	int retval;
+
+	retval = 1;
+
+	/*
+	 * We don't traverse the peripheral driver list like we do the
+	 * other lists, because it is a linker set, and therefore cannot be
+	 * changed during runtime.  If the peripheral driver list is ever
+	 * re-done to be something other than a linker set (i.e. it can
+	 * change while the system is running), the list traversal should
+	 * be modified to work like the other traversal functions.
+	 */
+	for (pdrv = (start_pdrv ? start_pdrv : periph_drivers);
+	     *pdrv != NULL; pdrv++) {
+		retval = tr_func(pdrv, arg);
+
+		if (retval == 0)
+			return(retval);
+	}
+
+	return(retval);
+}
+
+static int
+xptpdperiphtraverse(struct periph_driver **pdrv,
+		    struct cam_periph *start_periph,
+		    xpt_periphfunc_t *tr_func, void *arg)
+{
+	struct cam_periph *periph, *next_periph;
+	int retval;
+
+	retval = 1;
+
+	if (start_periph)
+		periph = start_periph;
+	else {
+		xpt_lock_buses();
+		periph = TAILQ_FIRST(&(*pdrv)->units);
+		while (periph != NULL && (periph->flags & CAM_PERIPH_FREE) != 0)
+			periph = TAILQ_NEXT(periph, unit_links);
+		if (periph == NULL) {
+			xpt_unlock_buses();
+			return (retval);
+		}
+		periph->refcount++;
+		xpt_unlock_buses();
+	}
+	for (; periph != NULL; periph = next_periph) {
+		cam_periph_lock(periph);
+		retval = tr_func(periph, arg);
+		cam_periph_unlock(periph);
+		if (retval == 0) {
+			cam_periph_release(periph);
+			break;
+		}
+		xpt_lock_buses();
+		next_periph = TAILQ_NEXT(periph, unit_links);
+		while (next_periph != NULL &&
+		    (next_periph->flags & CAM_PERIPH_FREE) != 0)
+			next_periph = TAILQ_NEXT(next_periph, unit_links);
+		if (next_periph)
+			next_periph->refcount++;
+		xpt_unlock_buses();
+		cam_periph_release(periph);
+	}
+	return(retval);
+}
+
+static int
+xptdefbusfunc(struct cam_eb *bus, void *arg)
+{
+	struct xpt_traverse_config *tr_config;
+
+	tr_config = (struct xpt_traverse_config *)arg;
+
+	if (tr_config->depth == XPT_DEPTH_BUS) {
+		xpt_busfunc_t *tr_func;
+
+		tr_func = (xpt_busfunc_t *)tr_config->tr_func;
+
+		return(tr_func(bus, tr_config->tr_arg));
+	} else
+		return(xpttargettraverse(bus, NULL, xptdeftargetfunc, arg));
+}
+
+static int
+xptdeftargetfunc(struct cam_et *target, void *arg)
+{
+	struct xpt_traverse_config *tr_config;
+
+	tr_config = (struct xpt_traverse_config *)arg;
+
+	if (tr_config->depth == XPT_DEPTH_TARGET) {
+		xpt_targetfunc_t *tr_func;
+
+		tr_func = (xpt_targetfunc_t *)tr_config->tr_func;
+
+		return(tr_func(target, tr_config->tr_arg));
+	} else
+		return(xptdevicetraverse(target, NULL, xptdefdevicefunc, arg));
+}
+
+static int
+xptdefdevicefunc(struct cam_ed *device, void *arg)
+{
+	struct xpt_traverse_config *tr_config;
+
+	tr_config = (struct xpt_traverse_config *)arg;
+
+	if (tr_config->depth == XPT_DEPTH_DEVICE) {
+		xpt_devicefunc_t *tr_func;
+
+		tr_func = (xpt_devicefunc_t *)tr_config->tr_func;
+
+		return(tr_func(device, tr_config->tr_arg));
+	} else
+		return(xptperiphtraverse(device, NULL, xptdefperiphfunc, arg));
+}
+
+static int
+xptdefperiphfunc(struct cam_periph *periph, void *arg)
+{
+	struct xpt_traverse_config *tr_config;
+	xpt_periphfunc_t *tr_func;
+
+	tr_config = (struct xpt_traverse_config *)arg;
+
+	tr_func = (xpt_periphfunc_t *)tr_config->tr_func;
+
+	/*
+	 * Unlike the other default functions, we don't check for depth
+	 * here.  The peripheral driver level is the last level in the EDT,
+	 * so if we're here, we should execute the function in question.
+	 */
+	return(tr_func(periph, tr_config->tr_arg));
+}
+
+/*
+ * Execute the given function for every bus in the EDT.
+ */
+static int
+xpt_for_all_busses(xpt_busfunc_t *tr_func, void *arg)
+{
+	struct xpt_traverse_config tr_config;
+
+	tr_config.depth = XPT_DEPTH_BUS;
+	tr_config.tr_func = tr_func;
+	tr_config.tr_arg = arg;
+
+	return(xptbustraverse(NULL, xptdefbusfunc, &tr_config));
+}
+
+/*
+ * Execute the given function for every device in the EDT.
+ */
+static int
+xpt_for_all_devices(xpt_devicefunc_t *tr_func, void *arg)
+{
+	struct xpt_traverse_config tr_config;
+
+	tr_config.depth = XPT_DEPTH_DEVICE;
+	tr_config.tr_func = tr_func;
+	tr_config.tr_arg = arg;
+
+	return(xptbustraverse(NULL, xptdefbusfunc, &tr_config));
+}
+
+static int
+xptsetasyncfunc(struct cam_ed *device, void *arg)
+{
+	struct cam_path path;
+	struct ccb_getdev cgd;
+	struct ccb_setasync *csa = (struct ccb_setasync *)arg;
+
+	/*
+	 * Don't report unconfigured devices (Wildcard devs,
+	 * devices only for target mode, device instances
+	 * that have been invalidated but are waiting for
+	 * their last reference count to be released).
+	 */
+	if ((device->flags & CAM_DEV_UNCONFIGURED) != 0)
+		return (1);
+
+	xpt_compile_path(&path,
+			 NULL,
+			 device->target->bus->path_id,
+			 device->target->target_id,
+			 device->lun_id);
+	xpt_setup_ccb(&cgd.ccb_h, &path, CAM_PRIORITY_NORMAL);
+	cgd.ccb_h.func_code = XPT_GDEV_TYPE;
+	xpt_action((union ccb *)&cgd);
+	csa->callback(csa->callback_arg,
+			    AC_FOUND_DEVICE,
+			    &path, &cgd);
+	xpt_release_path(&path);
+
+	return(1);
+}
+
+static int
+xptsetasyncbusfunc(struct cam_eb *bus, void *arg)
+{
+	struct cam_path path;
+	struct ccb_pathinq cpi;
+	struct ccb_setasync *csa = (struct ccb_setasync *)arg;
+
+	xpt_compile_path(&path, /*periph*/NULL,
+			 bus->path_id,
+			 CAM_TARGET_WILDCARD,
+			 CAM_LUN_WILDCARD);
+	xpt_path_lock(&path);
+	xpt_setup_ccb(&cpi.ccb_h, &path, CAM_PRIORITY_NORMAL);
+	cpi.ccb_h.func_code = XPT_PATH_INQ;
+	xpt_action((union ccb *)&cpi);
+	csa->callback(csa->callback_arg,
+			    AC_PATH_REGISTERED,
+			    &path, &cpi);
+	xpt_path_unlock(&path);
+	xpt_release_path(&path);
+
+	return(1);
+}
+
+void
+xpt_action(union ccb *start_ccb)
+{
+
+	CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_TRACE,
+	    ("xpt_action: func %#x %s\n", start_ccb->ccb_h.func_code,
+		xpt_action_name(start_ccb->ccb_h.func_code)));
+
+	start_ccb->ccb_h.status = CAM_REQ_INPROG;
+	(*(start_ccb->ccb_h.path->bus->xport->ops->action))(start_ccb);
+}
+
+void
+xpt_action_default(union ccb *start_ccb)
+{
+	struct cam_path *path;
+	struct cam_sim *sim;
+	struct mtx *mtx;
+
+	path = start_ccb->ccb_h.path;
+	CAM_DEBUG(path, CAM_DEBUG_TRACE,
+	    ("xpt_action_default: func %#x %s\n", start_ccb->ccb_h.func_code,
+		xpt_action_name(start_ccb->ccb_h.func_code)));
+
+	switch (start_ccb->ccb_h.func_code) {
+	case XPT_SCSI_IO:
+	{
+		struct cam_ed *device;
+
+		/*
+		 * For the sake of compatibility with SCSI-1
+		 * devices that may not understand the identify
+		 * message, we include lun information in the
+		 * second byte of all commands.  SCSI-1 specifies
+		 * that luns are a 3 bit value and reserves only 3
+		 * bits for lun information in the CDB.  Later
+		 * revisions of the SCSI spec allow for more than 8
+		 * luns, but have deprecated lun information in the
+		 * CDB.  So, if the lun won't fit, we must omit.
+		 *
+		 * Also be aware that during initial probing for devices,
+		 * the inquiry information is unknown but initialized to 0.
+		 * This means that this code will be exercised while probing
+		 * devices with an ANSI revision greater than 2.
+		 */
+		device = path->device;
+		if (device->protocol_version <= SCSI_REV_2
+		 && start_ccb->ccb_h.target_lun < 8
+		 && (start_ccb->ccb_h.flags & CAM_CDB_POINTER) == 0) {
+
+			start_ccb->csio.cdb_io.cdb_bytes[1] |=
+			    start_ccb->ccb_h.target_lun << 5;
+		}
+		start_ccb->csio.scsi_status = SCSI_STATUS_OK;
+	}
+	/* FALLTHROUGH */
+	case XPT_TARGET_IO:
+	case XPT_CONT_TARGET_IO:
+		start_ccb->csio.sense_resid = 0;
+		start_ccb->csio.resid = 0;
+		/* FALLTHROUGH */
+	case XPT_ATA_IO:
+		if (start_ccb->ccb_h.func_code == XPT_ATA_IO)
+			start_ccb->ataio.resid = 0;
+		/* FALLTHROUGH */
+	case XPT_NVME_IO:
+		if (start_ccb->ccb_h.func_code == XPT_NVME_IO)
+			start_ccb->nvmeio.resid = 0;
+		/* FALLTHROUGH */
+	case XPT_MMC_IO:
+		/* XXX just like nmve_io? */
+	case XPT_RESET_DEV:
+	case XPT_ENG_EXEC:
+	case XPT_SMP_IO:
+	{
+		struct cam_devq *devq;
+
+		devq = path->bus->sim->devq;
+		mtx_lock(&devq->send_mtx);
+		cam_ccbq_insert_ccb(&path->device->ccbq, start_ccb);
+		if (xpt_schedule_devq(devq, path->device) != 0)
+			xpt_run_devq(devq);
+		mtx_unlock(&devq->send_mtx);
+		break;
+	}
+	case XPT_CALC_GEOMETRY:
+		/* Filter out garbage */
+		if (start_ccb->ccg.block_size == 0
+		 || start_ccb->ccg.volume_size == 0) {
+			start_ccb->ccg.cylinders = 0;
+			start_ccb->ccg.heads = 0;
+			start_ccb->ccg.secs_per_track = 0;
+			start_ccb->ccb_h.status = CAM_REQ_CMP;
+			break;
+		}
+#if defined(__sparc64__)
+		/*
+		 * For sparc64, we may need adjust the geometry of large
+		 * disks in order to fit the limitations of the 16-bit
+		 * fields of the VTOC8 disk label.
+		 */
+		if (scsi_da_bios_params(&start_ccb->ccg) != 0) {
+			start_ccb->ccb_h.status = CAM_REQ_CMP;
+			break;
+		}
+#endif
+		goto call_sim;
+	case XPT_ABORT:
+	{
+		union ccb* abort_ccb;
+
+		abort_ccb = start_ccb->cab.abort_ccb;
+		if (XPT_FC_IS_DEV_QUEUED(abort_ccb)) {
+			struct cam_ed *device;
+			struct cam_devq *devq;
+
+			device = abort_ccb->ccb_h.path->device;
+			devq = device->sim->devq;
+
+			mtx_lock(&devq->send_mtx);
+			if (abort_ccb->ccb_h.pinfo.index > 0) {
+				cam_ccbq_remove_ccb(&device->ccbq, abort_ccb);
+				abort_ccb->ccb_h.status =
+				    CAM_REQ_ABORTED|CAM_DEV_QFRZN;
+				xpt_freeze_devq_device(device, 1);
+				mtx_unlock(&devq->send_mtx);
+				xpt_done(abort_ccb);
+				start_ccb->ccb_h.status = CAM_REQ_CMP;
+				break;
+			}
+			mtx_unlock(&devq->send_mtx);
+
+			if (abort_ccb->ccb_h.pinfo.index == CAM_UNQUEUED_INDEX
+			 && (abort_ccb->ccb_h.status & CAM_SIM_QUEUED) == 0) {
+				/*
+				 * We've caught this ccb en route to
+				 * the SIM.  Flag it for abort and the
+				 * SIM will do so just before starting
+				 * real work on the CCB.
+				 */
+				abort_ccb->ccb_h.status =
+				    CAM_REQ_ABORTED|CAM_DEV_QFRZN;
+				xpt_freeze_devq(abort_ccb->ccb_h.path, 1);
+				start_ccb->ccb_h.status = CAM_REQ_CMP;
+				break;
+			}
+		}
+		if (XPT_FC_IS_QUEUED(abort_ccb)
+		 && (abort_ccb->ccb_h.pinfo.index == CAM_DONEQ_INDEX)) {
+			/*
+			 * It's already completed but waiting
+			 * for our SWI to get to it.
+			 */
+			start_ccb->ccb_h.status = CAM_UA_ABORT;
+			break;
+		}
+		/*
+		 * If we weren't able to take care of the abort request
+		 * in the XPT, pass the request down to the SIM for processing.
+		 */
+	}
+	/* FALLTHROUGH */
+	case XPT_ACCEPT_TARGET_IO:
+	case XPT_EN_LUN:
+	case XPT_IMMED_NOTIFY:
+	case XPT_NOTIFY_ACK:
+	case XPT_RESET_BUS:
+	case XPT_IMMEDIATE_NOTIFY:
+	case XPT_NOTIFY_ACKNOWLEDGE:
+	case XPT_GET_SIM_KNOB_OLD:
+	case XPT_GET_SIM_KNOB:
+	case XPT_SET_SIM_KNOB:
+	case XPT_GET_TRAN_SETTINGS:
+	case XPT_SET_TRAN_SETTINGS:
+	case XPT_PATH_INQ:
+call_sim:
+		sim = path->bus->sim;
+		mtx = sim->mtx;
+		if (mtx && !mtx_owned(mtx))
+			mtx_lock(mtx);
+		else
+			mtx = NULL;
+
+		CAM_DEBUG(path, CAM_DEBUG_TRACE,
+		    ("Calling sim->sim_action(): func=%#x\n", start_ccb->ccb_h.func_code));
+		(*(sim->sim_action))(sim, start_ccb);
+		CAM_DEBUG(path, CAM_DEBUG_TRACE,
+		    ("sim->sim_action returned: status=%#x\n", start_ccb->ccb_h.status));
+		if (mtx)
+			mtx_unlock(mtx);
+		break;
+	case XPT_PATH_STATS:
+		start_ccb->cpis.last_reset = path->bus->last_reset;
+		start_ccb->ccb_h.status = CAM_REQ_CMP;
+		break;
+	case XPT_GDEV_TYPE:
+	{
+		struct cam_ed *dev;
+
+		dev = path->device;
+		if ((dev->flags & CAM_DEV_UNCONFIGURED) != 0) {
+			start_ccb->ccb_h.status = CAM_DEV_NOT_THERE;
+		} else {
+			struct ccb_getdev *cgd;
+
+			cgd = &start_ccb->cgd;
+			cgd->protocol = dev->protocol;
+			cgd->inq_data = dev->inq_data;
+			cgd->ident_data = dev->ident_data;
+			cgd->inq_flags = dev->inq_flags;
+			cgd->nvme_data = dev->nvme_data;
+			cgd->nvme_cdata = dev->nvme_cdata;
+			cgd->ccb_h.status = CAM_REQ_CMP;
+			cgd->serial_num_len = dev->serial_num_len;
+			if ((dev->serial_num_len > 0)
+			 && (dev->serial_num != NULL))
+				bcopy(dev->serial_num, cgd->serial_num,
+				      dev->serial_num_len);
+		}
+		break;
+	}
+	case XPT_GDEV_STATS:
+	{
+		struct ccb_getdevstats *cgds = &start_ccb->cgds;
+		struct cam_ed *dev = path->device;
+		struct cam_eb *bus = path->bus;
+		struct cam_et *tar = path->target;
+		struct cam_devq *devq = bus->sim->devq;
+
+		mtx_lock(&devq->send_mtx);
+		cgds->dev_openings = dev->ccbq.dev_openings;
+		cgds->dev_active = dev->ccbq.dev_active;
+		cgds->allocated = dev->ccbq.allocated;
+		cgds->queued = cam_ccbq_pending_ccb_count(&dev->ccbq);
+		cgds->held = cgds->allocated - cgds->dev_active - cgds->queued;
+		cgds->last_reset = tar->last_reset;
+		cgds->maxtags = dev->maxtags;
+		cgds->mintags = dev->mintags;
+		if (timevalcmp(&tar->last_reset, &bus->last_reset, <))
+			cgds->last_reset = bus->last_reset;
+		mtx_unlock(&devq->send_mtx);
+		cgds->ccb_h.status = CAM_REQ_CMP;
+		break;
+	}
+	case XPT_GDEVLIST:
+	{
+		struct cam_periph	*nperiph;
+		struct periph_list	*periph_head;
+		struct ccb_getdevlist	*cgdl;
+		u_int			i;
+		struct cam_ed		*device;
+		int			found;
+
+
+		found = 0;
+
+		/*
+		 * Don't want anyone mucking with our data.
+		 */
+		device = path->device;
+		periph_head = &device->periphs;
+		cgdl = &start_ccb->cgdl;
+
+		/*
+		 * Check and see if the list has changed since the user
+		 * last requested a list member.  If so, tell them that the
+		 * list has changed, and therefore they need to start over
+		 * from the beginning.
+		 */
+		if ((cgdl->index != 0) &&
+		    (cgdl->generation != device->generation)) {
+			cgdl->status = CAM_GDEVLIST_LIST_CHANGED;
+			break;
+		}
+
+		/*
+		 * Traverse the list of peripherals and attempt to find
+		 * the requested peripheral.
+		 */
+		for (nperiph = SLIST_FIRST(periph_head), i = 0;
+		     (nperiph != NULL) && (i <= cgdl->index);
+		     nperiph = SLIST_NEXT(nperiph, periph_links), i++) {
+			if (i == cgdl->index) {
+				strncpy(cgdl->periph_name,
+					nperiph->periph_name,
+					DEV_IDLEN);
+				cgdl->unit_number = nperiph->unit_number;
+				found = 1;
+			}
+		}
+		if (found == 0) {
+			cgdl->status = CAM_GDEVLIST_ERROR;
+			break;
+		}
+
+		if (nperiph == NULL)
+			cgdl->status = CAM_GDEVLIST_LAST_DEVICE;
+		else
+			cgdl->status = CAM_GDEVLIST_MORE_DEVS;
+
+		cgdl->index++;
+		cgdl->generation = device->generation;
+
+		cgdl->ccb_h.status = CAM_REQ_CMP;
+		break;
+	}
+	case XPT_DEV_MATCH:
+	{
+		dev_pos_type position_type;
+		struct ccb_dev_match *cdm;
+
+		cdm = &start_ccb->cdm;
+
+		/*
+		 * There are two ways of getting at information in the EDT.
+		 * The first way is via the primary EDT tree.  It starts
+		 * with a list of buses, then a list of targets on a bus,
+		 * then devices/luns on a target, and then peripherals on a
+		 * device/lun.  The "other" way is by the peripheral driver
+		 * lists.  The peripheral driver lists are organized by
+		 * peripheral driver.  (obviously)  So it makes sense to
+		 * use the peripheral driver list if the user is looking
+		 * for something like "da1", or all "da" devices.  If the
+		 * user is looking for something on a particular bus/target
+		 * or lun, it's generally better to go through the EDT tree.
+		 */
+
+		if (cdm->pos.position_type != CAM_DEV_POS_NONE)
+			position_type = cdm->pos.position_type;
+		else {
+			u_int i;
+
+			position_type = CAM_DEV_POS_NONE;
+
+			for (i = 0; i < cdm->num_patterns; i++) {
+				if ((cdm->patterns[i].type == DEV_MATCH_BUS)
+				 ||(cdm->patterns[i].type == DEV_MATCH_DEVICE)){
+					position_type = CAM_DEV_POS_EDT;
+					break;
+				}
+			}
+
+			if (cdm->num_patterns == 0)
+				position_type = CAM_DEV_POS_EDT;
+			else if (position_type == CAM_DEV_POS_NONE)
+				position_type = CAM_DEV_POS_PDRV;
+		}
+
+		switch(position_type & CAM_DEV_POS_TYPEMASK) {
+		case CAM_DEV_POS_EDT:
+			xptedtmatch(cdm);
+			break;
+		case CAM_DEV_POS_PDRV:
+			xptperiphlistmatch(cdm);
+			break;
+		default:
+			cdm->status = CAM_DEV_MATCH_ERROR;
+			break;
+		}
+
+		if (cdm->status == CAM_DEV_MATCH_ERROR)
+			start_ccb->ccb_h.status = CAM_REQ_CMP_ERR;
+		else
+			start_ccb->ccb_h.status = CAM_REQ_CMP;
+
+		break;
+	}
+	case XPT_SASYNC_CB:
+	{
+		struct ccb_setasync *csa;
+		struct async_node *cur_entry;
+		struct async_list *async_head;
+		u_int32_t added;
+
+		csa = &start_ccb->csa;
+		added = csa->event_enable;
+		async_head = &path->device->asyncs;
+
+		/*
+		 * If there is already an entry for us, simply
+		 * update it.
+		 */
+		cur_entry = SLIST_FIRST(async_head);
+		while (cur_entry != NULL) {
+			if ((cur_entry->callback_arg == csa->callback_arg)
+			 && (cur_entry->callback == csa->callback))
+				break;
+			cur_entry = SLIST_NEXT(cur_entry, links);
+		}
+
+		if (cur_entry != NULL) {
+		 	/*
+			 * If the request has no flags set,
+			 * remove the entry.
+			 */
+			added &= ~cur_entry->event_enable;
+			if (csa->event_enable == 0) {
+				SLIST_REMOVE(async_head, cur_entry,
+					     async_node, links);
+				xpt_release_device(path->device);
+				free(cur_entry, M_CAMXPT);
+			} else {
+				cur_entry->event_enable = csa->event_enable;
+			}
+			csa->event_enable = added;
+		} else {
+			cur_entry = malloc(sizeof(*cur_entry), M_CAMXPT,
+					   M_NOWAIT);
+			if (cur_entry == NULL) {
+				csa->ccb_h.status = CAM_RESRC_UNAVAIL;
+				break;
+			}
+			cur_entry->event_enable = csa->event_enable;
+			cur_entry->event_lock = (path->bus->sim->mtx &&
+			    mtx_owned(path->bus->sim->mtx)) ? 1 : 0;
+			cur_entry->callback_arg = csa->callback_arg;
+			cur_entry->callback = csa->callback;
+			SLIST_INSERT_HEAD(async_head, cur_entry, links);
+			xpt_acquire_device(path->device);
+		}
+		start_ccb->ccb_h.status = CAM_REQ_CMP;
+		break;
+	}
+	case XPT_REL_SIMQ:
+	{
+		struct ccb_relsim *crs;
+		struct cam_ed *dev;
+
+		crs = &start_ccb->crs;
+		dev = path->device;
+		if (dev == NULL) {
+
+			crs->ccb_h.status = CAM_DEV_NOT_THERE;
+			break;
+		}
+
+		if ((crs->release_flags & RELSIM_ADJUST_OPENINGS) != 0) {
+
+			/* Don't ever go below one opening */
+			if (crs->openings > 0) {
+				xpt_dev_ccbq_resize(path, crs->openings);
+				if (bootverbose) {
+					xpt_print(path,
+					    "number of openings is now %d\n",
+					    crs->openings);
+				}
+			}
+		}
+
+		mtx_lock(&dev->sim->devq->send_mtx);
+		if ((crs->release_flags & RELSIM_RELEASE_AFTER_TIMEOUT) != 0) {
+
+			if ((dev->flags & CAM_DEV_REL_TIMEOUT_PENDING) != 0) {
+
+				/*
+				 * Just extend the old timeout and decrement
+				 * the freeze count so that a single timeout
+				 * is sufficient for releasing the queue.
+				 */
+				start_ccb->ccb_h.flags &= ~CAM_DEV_QFREEZE;
+				callout_stop(&dev->callout);
+			} else {
+
+				start_ccb->ccb_h.flags |= CAM_DEV_QFREEZE;
+			}
+
+			callout_reset_sbt(&dev->callout,
+			    SBT_1MS * crs->release_timeout, 0,
+			    xpt_release_devq_timeout, dev, 0);
+
+			dev->flags |= CAM_DEV_REL_TIMEOUT_PENDING;
+
+		}
+
+		if ((crs->release_flags & RELSIM_RELEASE_AFTER_CMDCMPLT) != 0) {
+
+			if ((dev->flags & CAM_DEV_REL_ON_COMPLETE) != 0) {
+				/*
+				 * Decrement the freeze count so that a single
+				 * completion is still sufficient to unfreeze
+				 * the queue.
+				 */
+				start_ccb->ccb_h.flags &= ~CAM_DEV_QFREEZE;
+			} else {
+
+				dev->flags |= CAM_DEV_REL_ON_COMPLETE;
+				start_ccb->ccb_h.flags |= CAM_DEV_QFREEZE;
+			}
+		}
+
+		if ((crs->release_flags & RELSIM_RELEASE_AFTER_QEMPTY) != 0) {
+
+			if ((dev->flags & CAM_DEV_REL_ON_QUEUE_EMPTY) != 0
+			 || (dev->ccbq.dev_active == 0)) {
+
+				start_ccb->ccb_h.flags &= ~CAM_DEV_QFREEZE;
+			} else {
+
+				dev->flags |= CAM_DEV_REL_ON_QUEUE_EMPTY;
+				start_ccb->ccb_h.flags |= CAM_DEV_QFREEZE;
+			}
+		}
+		mtx_unlock(&dev->sim->devq->send_mtx);
+
+		if ((start_ccb->ccb_h.flags & CAM_DEV_QFREEZE) == 0)
+			xpt_release_devq(path, /*count*/1, /*run_queue*/TRUE);
+		start_ccb->crs.qfrozen_cnt = dev->ccbq.queue.qfrozen_cnt;
+		start_ccb->ccb_h.status = CAM_REQ_CMP;
+		break;
+	}
+	case XPT_DEBUG: {
+		struct cam_path *oldpath;
+
+		/* Check that all request bits are supported. */
+		if (start_ccb->cdbg.flags & ~(CAM_DEBUG_COMPILE)) {
+			start_ccb->ccb_h.status = CAM_FUNC_NOTAVAIL;
+			break;
+		}
+
+		cam_dflags = CAM_DEBUG_NONE;
+		if (cam_dpath != NULL) {
+			oldpath = cam_dpath;
+			cam_dpath = NULL;
+			xpt_free_path(oldpath);
+		}
+		if (start_ccb->cdbg.flags != CAM_DEBUG_NONE) {
+			if (xpt_create_path(&cam_dpath, NULL,
+					    start_ccb->ccb_h.path_id,
+					    start_ccb->ccb_h.target_id,
+					    start_ccb->ccb_h.target_lun) !=
+					    CAM_REQ_CMP) {
+				start_ccb->ccb_h.status = CAM_RESRC_UNAVAIL;
+			} else {
+				cam_dflags = start_ccb->cdbg.flags;
+				start_ccb->ccb_h.status = CAM_REQ_CMP;
+				xpt_print(cam_dpath, "debugging flags now %x\n",
+				    cam_dflags);
+			}
+		} else
+			start_ccb->ccb_h.status = CAM_REQ_CMP;
+		break;
+	}
+	case XPT_NOOP:
+		if ((start_ccb->ccb_h.flags & CAM_DEV_QFREEZE) != 0)
+			xpt_freeze_devq(path, 1);
+		start_ccb->ccb_h.status = CAM_REQ_CMP;
+		break;
+	case XPT_REPROBE_LUN:
+		xpt_async(AC_INQ_CHANGED, path, NULL);
+		start_ccb->ccb_h.status = CAM_REQ_CMP;
+		xpt_done(start_ccb);
+		break;
+	default:
+	case XPT_SDEV_TYPE:
+	case XPT_TERM_IO:
+	case XPT_ENG_INQ:
+		/* XXX Implement */
+		xpt_print(start_ccb->ccb_h.path,
+		    "%s: CCB type %#x %s not supported\n", __func__,
+		    start_ccb->ccb_h.func_code,
+		    xpt_action_name(start_ccb->ccb_h.func_code));
+		start_ccb->ccb_h.status = CAM_PROVIDE_FAIL;
+		if (start_ccb->ccb_h.func_code & XPT_FC_DEV_QUEUED) {
+			xpt_done(start_ccb);
+		}
+		break;
+	}
+	CAM_DEBUG(path, CAM_DEBUG_TRACE,
+	    ("xpt_action_default: func= %#x %s status %#x\n",
+		start_ccb->ccb_h.func_code,
+ 		xpt_action_name(start_ccb->ccb_h.func_code),
+		start_ccb->ccb_h.status));
+}
+
+void
+xpt_polled_action(union ccb *start_ccb)
+{
+	u_int32_t timeout;
+	struct	  cam_sim *sim;
+	struct	  cam_devq *devq;
+	struct	  cam_ed *dev;
+	struct mtx *mtx;
+
+	timeout = start_ccb->ccb_h.timeout * 10;
+	sim = start_ccb->ccb_h.path->bus->sim;
+	devq = sim->devq;
+	mtx = sim->mtx;
+	dev = start_ccb->ccb_h.path->device;
+
+	mtx_unlock(&dev->device_mtx);
+
+	/*
+	 * Steal an opening so that no other queued requests
+	 * can get it before us while we simulate interrupts.
+	 */
+	mtx_lock(&devq->send_mtx);
+	dev->ccbq.dev_openings--;
+	while((devq->send_openings <= 0 || dev->ccbq.dev_openings < 0) &&
+	    (--timeout > 0)) {
+		mtx_unlock(&devq->send_mtx);
+		DELAY(100);
+		if (mtx)
+			mtx_lock(mtx);
+		(*(sim->sim_poll))(sim);
+		if (mtx)
+			mtx_unlock(mtx);
+		camisr_runqueue();
+		mtx_lock(&devq->send_mtx);
+	}
+	dev->ccbq.dev_openings++;
+	mtx_unlock(&devq->send_mtx);
+
+	if (timeout != 0) {
+		xpt_action(start_ccb);
+		while(--timeout > 0) {
+			if (mtx)
+				mtx_lock(mtx);
+			(*(sim->sim_poll))(sim);
+			if (mtx)
+				mtx_unlock(mtx);
+			camisr_runqueue();
+			if ((start_ccb->ccb_h.status  & CAM_STATUS_MASK)
+			    != CAM_REQ_INPROG)
+				break;
+			DELAY(100);
+		}
+		if (timeout == 0) {
+			/*
+			 * XXX Is it worth adding a sim_timeout entry
+			 * point so we can attempt recovery?  If
+			 * this is only used for dumps, I don't think
+			 * it is.
+			 */
+			start_ccb->ccb_h.status = CAM_CMD_TIMEOUT;
+		}
+	} else {
+		start_ccb->ccb_h.status = CAM_RESRC_UNAVAIL;
+	}
+
+	mtx_lock(&dev->device_mtx);
+}
+
+/*
+ * Schedule a peripheral driver to receive a ccb when its
+ * target device has space for more transactions.
+ */
+void
+xpt_schedule(struct cam_periph *periph, u_int32_t new_priority)
+{
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("xpt_schedule\n"));
+	cam_periph_assert(periph, MA_OWNED);
+	if (new_priority < periph->scheduled_priority) {
+		periph->scheduled_priority = new_priority;
+		xpt_run_allocq(periph, 0);
+	}
+}
+
+
+/*
+ * Schedule a device to run on a given queue.
+ * If the device was inserted as a new entry on the queue,
+ * return 1 meaning the device queue should be run. If we
+ * were already queued, implying someone else has already
+ * started the queue, return 0 so the caller doesn't attempt
+ * to run the queue.
+ */
+static int
+xpt_schedule_dev(struct camq *queue, cam_pinfo *pinfo,
+		 u_int32_t new_priority)
+{
+	int retval;
+	u_int32_t old_priority;
+
+	CAM_DEBUG_PRINT(CAM_DEBUG_XPT, ("xpt_schedule_dev\n"));
+
+	old_priority = pinfo->priority;
+
+	/*
+	 * Are we already queued?
+	 */
+	if (pinfo->index != CAM_UNQUEUED_INDEX) {
+		/* Simply reorder based on new priority */
+		if (new_priority < old_priority) {
+			camq_change_priority(queue, pinfo->index,
+					     new_priority);
+			CAM_DEBUG_PRINT(CAM_DEBUG_XPT,
+					("changed priority to %d\n",
+					 new_priority));
+			retval = 1;
+		} else
+			retval = 0;
+	} else {
+		/* New entry on the queue */
+		if (new_priority < old_priority)
+			pinfo->priority = new_priority;
+
+		CAM_DEBUG_PRINT(CAM_DEBUG_XPT,
+				("Inserting onto queue\n"));
+		pinfo->generation = ++queue->generation;
+		camq_insert(queue, pinfo);
+		retval = 1;
+	}
+	return (retval);
+}
+
+static void
+xpt_run_allocq_task(void *context, int pending)
+{
+	struct cam_periph *periph = context;
+
+	cam_periph_lock(periph);
+	periph->flags &= ~CAM_PERIPH_RUN_TASK;
+	xpt_run_allocq(periph, 1);
+	cam_periph_unlock(periph);
+	cam_periph_release(periph);
+}
+/* Made non-static so that it can be called in mmc_da.c for CCB prep.*/
+void
+xpt_run_allocq(struct cam_periph *periph, int sleep)
+{
+	struct cam_ed	*device;
+	union ccb	*ccb;
+	uint32_t	 prio;
+
+	cam_periph_assert(periph, MA_OWNED);
+	if (periph->periph_allocating)
+		return;
+	periph->periph_allocating = 1;
+	CAM_DEBUG_PRINT(CAM_DEBUG_XPT, ("xpt_run_allocq(%p)\n", periph));
+	device = periph->path->device;
+	ccb = NULL;
+restart:
+	while ((prio = min(periph->scheduled_priority,
+	    periph->immediate_priority)) != CAM_PRIORITY_NONE &&
+	    (periph->periph_allocated - (ccb != NULL ? 1 : 0) <
+	     device->ccbq.total_openings || prio <= CAM_PRIORITY_OOB)) {
+
+		if (ccb == NULL &&
+		    (ccb = xpt_get_ccb_nowait(periph)) == NULL) {
+			if (sleep) {
+				ccb = xpt_get_ccb(periph);
+				goto restart;
+			}
+			if (periph->flags & CAM_PERIPH_RUN_TASK)
+				break;
+			cam_periph_doacquire(periph);
+			periph->flags |= CAM_PERIPH_RUN_TASK;
+			taskqueue_enqueue(xsoftc.xpt_taskq,
+			    &periph->periph_run_task);
+			break;
+		}
+		xpt_setup_ccb(&ccb->ccb_h, periph->path, prio);
+		if (prio == periph->immediate_priority) {
+			periph->immediate_priority = CAM_PRIORITY_NONE;
+			CAM_DEBUG_PRINT(CAM_DEBUG_XPT,
+					("waking cam_periph_getccb()\n"));
+			SLIST_INSERT_HEAD(&periph->ccb_list, &ccb->ccb_h,
+					  periph_links.sle);
+			wakeup(&periph->ccb_list);
+		} else {
+			periph->scheduled_priority = CAM_PRIORITY_NONE;
+			CAM_DEBUG_PRINT(CAM_DEBUG_XPT,
+					("calling periph_start()\n"));
+			periph->periph_start(periph, ccb);
+		}
+		ccb = NULL;
+	}
+	if (ccb != NULL)
+		xpt_release_ccb(ccb);
+	periph->periph_allocating = 0;
+}
+
+static void
+xpt_run_devq(struct cam_devq *devq)
+{
+	struct mtx *mtx;
+
+	CAM_DEBUG_PRINT(CAM_DEBUG_XPT, ("xpt_run_devq\n"));
+
+	devq->send_queue.qfrozen_cnt++;
+	while ((devq->send_queue.entries > 0)
+	    && (devq->send_openings > 0)
+	    && (devq->send_queue.qfrozen_cnt <= 1)) {
+		struct	cam_ed *device;
+		union ccb *work_ccb;
+		struct	cam_sim *sim;
+		struct xpt_proto *proto;
+
+		device = (struct cam_ed *)camq_remove(&devq->send_queue,
+							   CAMQ_HEAD);
+		CAM_DEBUG_PRINT(CAM_DEBUG_XPT,
+				("running device %p\n", device));
+
+		work_ccb = cam_ccbq_peek_ccb(&device->ccbq, CAMQ_HEAD);
+		if (work_ccb == NULL) {
+			printf("device on run queue with no ccbs???\n");
+			continue;
+		}
+
+		if ((work_ccb->ccb_h.flags & CAM_HIGH_POWER) != 0) {
+
+			mtx_lock(&xsoftc.xpt_highpower_lock);
+		 	if (xsoftc.num_highpower <= 0) {
+				/*
+				 * We got a high power command, but we
+				 * don't have any available slots.  Freeze
+				 * the device queue until we have a slot
+				 * available.
+				 */
+				xpt_freeze_devq_device(device, 1);
+				STAILQ_INSERT_TAIL(&xsoftc.highpowerq, device,
+						   highpowerq_entry);
+
+				mtx_unlock(&xsoftc.xpt_highpower_lock);
+				continue;
+			} else {
+				/*
+				 * Consume a high power slot while
+				 * this ccb runs.
+				 */
+				xsoftc.num_highpower--;
+			}
+			mtx_unlock(&xsoftc.xpt_highpower_lock);
+		}
+		cam_ccbq_remove_ccb(&device->ccbq, work_ccb);
+		cam_ccbq_send_ccb(&device->ccbq, work_ccb);
+		devq->send_openings--;
+		devq->send_active++;
+		xpt_schedule_devq(devq, device);
+		mtx_unlock(&devq->send_mtx);
+
+		if ((work_ccb->ccb_h.flags & CAM_DEV_QFREEZE) != 0) {
+			/*
+			 * The client wants to freeze the queue
+			 * after this CCB is sent.
+			 */
+			xpt_freeze_devq(work_ccb->ccb_h.path, 1);
+		}
+
+		/* In Target mode, the peripheral driver knows best... */
+		if (work_ccb->ccb_h.func_code == XPT_SCSI_IO) {
+			if ((device->inq_flags & SID_CmdQue) != 0
+			 && work_ccb->csio.tag_action != CAM_TAG_ACTION_NONE)
+				work_ccb->ccb_h.flags |= CAM_TAG_ACTION_VALID;
+			else
+				/*
+				 * Clear this in case of a retried CCB that
+				 * failed due to a rejected tag.
+				 */
+				work_ccb->ccb_h.flags &= ~CAM_TAG_ACTION_VALID;
+		}
+
+		KASSERT(device == work_ccb->ccb_h.path->device,
+		    ("device (%p) / path->device (%p) mismatch",
+			device, work_ccb->ccb_h.path->device));
+		proto = xpt_proto_find(device->protocol);
+		if (proto && proto->ops->debug_out)
+			proto->ops->debug_out(work_ccb);
+
+		/*
+		 * Device queues can be shared among multiple SIM instances
+		 * that reside on different buses.  Use the SIM from the
+		 * queued device, rather than the one from the calling bus.
+		 */
+		sim = device->sim;
+		mtx = sim->mtx;
+		if (mtx && !mtx_owned(mtx))
+			mtx_lock(mtx);
+		else
+			mtx = NULL;
+		work_ccb->ccb_h.qos.sim_data = sbinuptime(); // xxx uintprt_t too small 32bit platforms
+		(*(sim->sim_action))(sim, work_ccb);
+		if (mtx)
+			mtx_unlock(mtx);
+		mtx_lock(&devq->send_mtx);
+	}
+	devq->send_queue.qfrozen_cnt--;
+}
+
+/*
+ * This function merges stuff from the slave ccb into the master ccb, while
+ * keeping important fields in the master ccb constant.
+ */
+void
+xpt_merge_ccb(union ccb *master_ccb, union ccb *slave_ccb)
+{
+
+	/*
+	 * Pull fields that are valid for peripheral drivers to set
+	 * into the master CCB along with the CCB "payload".
+	 */
+	master_ccb->ccb_h.retry_count = slave_ccb->ccb_h.retry_count;
+	master_ccb->ccb_h.func_code = slave_ccb->ccb_h.func_code;
+	master_ccb->ccb_h.timeout = slave_ccb->ccb_h.timeout;
+	master_ccb->ccb_h.flags = slave_ccb->ccb_h.flags;
+	bcopy(&(&slave_ccb->ccb_h)[1], &(&master_ccb->ccb_h)[1],
+	      sizeof(union ccb) - sizeof(struct ccb_hdr));
+}
+
+void
+xpt_setup_ccb_flags(struct ccb_hdr *ccb_h, struct cam_path *path,
+		    u_int32_t priority, u_int32_t flags)
+{
+
+	CAM_DEBUG(path, CAM_DEBUG_TRACE, ("xpt_setup_ccb\n"));
+	ccb_h->pinfo.priority = priority;
+	ccb_h->path = path;
+	ccb_h->path_id = path->bus->path_id;
+	if (path->target)
+		ccb_h->target_id = path->target->target_id;
+	else
+		ccb_h->target_id = CAM_TARGET_WILDCARD;
+	if (path->device) {
+		ccb_h->target_lun = path->device->lun_id;
+		ccb_h->pinfo.generation = ++path->device->ccbq.queue.generation;
+	} else {
+		ccb_h->target_lun = CAM_TARGET_WILDCARD;
+	}
+	ccb_h->pinfo.index = CAM_UNQUEUED_INDEX;
+	ccb_h->flags = flags;
+	ccb_h->xflags = 0;
+}
+
+void
+xpt_setup_ccb(struct ccb_hdr *ccb_h, struct cam_path *path, u_int32_t priority)
+{
+	xpt_setup_ccb_flags(ccb_h, path, priority, /*flags*/ 0);
+}
+
+/* Path manipulation functions */
+cam_status
+xpt_create_path(struct cam_path **new_path_ptr, struct cam_periph *perph,
+		path_id_t path_id, target_id_t target_id, lun_id_t lun_id)
+{
+	struct	   cam_path *path;
+	cam_status status;
+
+	path = (struct cam_path *)malloc(sizeof(*path), M_CAMPATH, M_NOWAIT);
+
+	if (path == NULL) {
+		status = CAM_RESRC_UNAVAIL;
+		return(status);
+	}
+	status = xpt_compile_path(path, perph, path_id, target_id, lun_id);
+	if (status != CAM_REQ_CMP) {
+		free(path, M_CAMPATH);
+		path = NULL;
+	}
+	*new_path_ptr = path;
+	return (status);
+}
+
+cam_status
+xpt_create_path_unlocked(struct cam_path **new_path_ptr,
+			 struct cam_periph *periph, path_id_t path_id,
+			 target_id_t target_id, lun_id_t lun_id)
+{
+
+	return (xpt_create_path(new_path_ptr, periph, path_id, target_id,
+	    lun_id));
+}
+
+cam_status
+xpt_compile_path(struct cam_path *new_path, struct cam_periph *perph,
+		 path_id_t path_id, target_id_t target_id, lun_id_t lun_id)
+{
+	struct	     cam_eb *bus;
+	struct	     cam_et *target;
+	struct	     cam_ed *device;
+	cam_status   status;
+
+	status = CAM_REQ_CMP;	/* Completed without error */
+	target = NULL;		/* Wildcarded */
+	device = NULL;		/* Wildcarded */
+
+	/*
+	 * We will potentially modify the EDT, so block interrupts
+	 * that may attempt to create cam paths.
+	 */
+	bus = xpt_find_bus(path_id);
+	if (bus == NULL) {
+		status = CAM_PATH_INVALID;
+	} else {
+		xpt_lock_buses();
+		mtx_lock(&bus->eb_mtx);
+		target = xpt_find_target(bus, target_id);
+		if (target == NULL) {
+			/* Create one */
+			struct cam_et *new_target;
+
+			new_target = xpt_alloc_target(bus, target_id);
+			if (new_target == NULL) {
+				status = CAM_RESRC_UNAVAIL;
+			} else {
+				target = new_target;
+			}
+		}
+		xpt_unlock_buses();
+		if (target != NULL) {
+			device = xpt_find_device(target, lun_id);
+			if (device == NULL) {
+				/* Create one */
+				struct cam_ed *new_device;
+
+				new_device =
+				    (*(bus->xport->ops->alloc_device))(bus,
+								       target,
+								       lun_id);
+				if (new_device == NULL) {
+					status = CAM_RESRC_UNAVAIL;
+				} else {
+					device = new_device;
+				}
+			}
+		}
+		mtx_unlock(&bus->eb_mtx);
+	}
+
+	/*
+	 * Only touch the user's data if we are successful.
+	 */
+	if (status == CAM_REQ_CMP) {
+		new_path->periph = perph;
+		new_path->bus = bus;
+		new_path->target = target;
+		new_path->device = device;
+		CAM_DEBUG(new_path, CAM_DEBUG_TRACE, ("xpt_compile_path\n"));
+	} else {
+		if (device != NULL)
+			xpt_release_device(device);
+		if (target != NULL)
+			xpt_release_target(target);
+		if (bus != NULL)
+			xpt_release_bus(bus);
+	}
+	return (status);
+}
+
+cam_status
+xpt_clone_path(struct cam_path **new_path_ptr, struct cam_path *path)
+{
+	struct	   cam_path *new_path;
+
+	new_path = (struct cam_path *)malloc(sizeof(*path), M_CAMPATH, M_NOWAIT);
+	if (new_path == NULL)
+		return(CAM_RESRC_UNAVAIL);
+	xpt_copy_path(new_path, path);
+	*new_path_ptr = new_path;
+	return (CAM_REQ_CMP);
+}
+
+void
+xpt_copy_path(struct cam_path *new_path, struct cam_path *path)
+{
+
+	*new_path = *path;
+	if (path->bus != NULL)
+		xpt_acquire_bus(path->bus);
+	if (path->target != NULL)
+		xpt_acquire_target(path->target);
+	if (path->device != NULL)
+		xpt_acquire_device(path->device);
+}
+
+void
+xpt_release_path(struct cam_path *path)
+{
+	CAM_DEBUG(path, CAM_DEBUG_TRACE, ("xpt_release_path\n"));
+	if (path->device != NULL) {
+		xpt_release_device(path->device);
+		path->device = NULL;
+	}
+	if (path->target != NULL) {
+		xpt_release_target(path->target);
+		path->target = NULL;
+	}
+	if (path->bus != NULL) {
+		xpt_release_bus(path->bus);
+		path->bus = NULL;
+	}
+}
+
+void
+xpt_free_path(struct cam_path *path)
+{
+
+	CAM_DEBUG(path, CAM_DEBUG_TRACE, ("xpt_free_path\n"));
+	xpt_release_path(path);
+	free(path, M_CAMPATH);
+}
+
+void
+xpt_path_counts(struct cam_path *path, uint32_t *bus_ref,
+    uint32_t *periph_ref, uint32_t *target_ref, uint32_t *device_ref)
+{
+
+	xpt_lock_buses();
+	if (bus_ref) {
+		if (path->bus)
+			*bus_ref = path->bus->refcount;
+		else
+			*bus_ref = 0;
+	}
+	if (periph_ref) {
+		if (path->periph)
+			*periph_ref = path->periph->refcount;
+		else
+			*periph_ref = 0;
+	}
+	xpt_unlock_buses();
+	if (target_ref) {
+		if (path->target)
+			*target_ref = path->target->refcount;
+		else
+			*target_ref = 0;
+	}
+	if (device_ref) {
+		if (path->device)
+			*device_ref = path->device->refcount;
+		else
+			*device_ref = 0;
+	}
+}
+
+/*
+ * Return -1 for failure, 0 for exact match, 1 for match with wildcards
+ * in path1, 2 for match with wildcards in path2.
+ */
+int
+xpt_path_comp(struct cam_path *path1, struct cam_path *path2)
+{
+	int retval = 0;
+
+	if (path1->bus != path2->bus) {
+		if (path1->bus->path_id == CAM_BUS_WILDCARD)
+			retval = 1;
+		else if (path2->bus->path_id == CAM_BUS_WILDCARD)
+			retval = 2;
+		else
+			return (-1);
+	}
+	if (path1->target != path2->target) {
+		if (path1->target->target_id == CAM_TARGET_WILDCARD) {
+			if (retval == 0)
+				retval = 1;
+		} else if (path2->target->target_id == CAM_TARGET_WILDCARD)
+			retval = 2;
+		else
+			return (-1);
+	}
+	if (path1->device != path2->device) {
+		if (path1->device->lun_id == CAM_LUN_WILDCARD) {
+			if (retval == 0)
+				retval = 1;
+		} else if (path2->device->lun_id == CAM_LUN_WILDCARD)
+			retval = 2;
+		else
+			return (-1);
+	}
+	return (retval);
+}
+
+int
+xpt_path_comp_dev(struct cam_path *path, struct cam_ed *dev)
+{
+	int retval = 0;
+
+	if (path->bus != dev->target->bus) {
+		if (path->bus->path_id == CAM_BUS_WILDCARD)
+			retval = 1;
+		else if (dev->target->bus->path_id == CAM_BUS_WILDCARD)
+			retval = 2;
+		else
+			return (-1);
+	}
+	if (path->target != dev->target) {
+		if (path->target->target_id == CAM_TARGET_WILDCARD) {
+			if (retval == 0)
+				retval = 1;
+		} else if (dev->target->target_id == CAM_TARGET_WILDCARD)
+			retval = 2;
+		else
+			return (-1);
+	}
+	if (path->device != dev) {
+		if (path->device->lun_id == CAM_LUN_WILDCARD) {
+			if (retval == 0)
+				retval = 1;
+		} else if (dev->lun_id == CAM_LUN_WILDCARD)
+			retval = 2;
+		else
+			return (-1);
+	}
+	return (retval);
+}
+
+void
+xpt_print_path(struct cam_path *path)
+{
+	struct sbuf sb;
+	char buffer[XPT_PRINT_LEN];
+
+	sbuf_new(&sb, buffer, XPT_PRINT_LEN, SBUF_FIXEDLEN);
+	xpt_path_sbuf(path, &sb);
+	sbuf_finish(&sb);
+	printf("%s", sbuf_data(&sb));
+	sbuf_delete(&sb);
+}
+
+void
+xpt_print_device(struct cam_ed *device)
+{
+
+	if (device == NULL)
+		printf("(nopath): ");
+	else {
+		printf("(noperiph:%s%d:%d:%d:%jx): ", device->sim->sim_name,
+		       device->sim->unit_number,
+		       device->sim->bus_id,
+		       device->target->target_id,
+		       (uintmax_t)device->lun_id);
+	}
+}
+
+void
+xpt_print(struct cam_path *path, const char *fmt, ...)
+{
+	va_list ap;
+	struct sbuf sb;
+	char buffer[XPT_PRINT_LEN];
+
+	sbuf_new(&sb, buffer, XPT_PRINT_LEN, SBUF_FIXEDLEN);
+
+	xpt_path_sbuf(path, &sb);
+	va_start(ap, fmt);
+	sbuf_vprintf(&sb, fmt, ap);
+	va_end(ap);
+
+	sbuf_finish(&sb);
+	printf("%s", sbuf_data(&sb));
+	sbuf_delete(&sb);
+}
+
+int
+xpt_path_string(struct cam_path *path, char *str, size_t str_len)
+{
+	struct sbuf sb;
+	int len;
+
+	sbuf_new(&sb, str, str_len, 0);
+	len = xpt_path_sbuf(path, &sb);
+	sbuf_finish(&sb);
+	return (len);
+}
+
+int
+xpt_path_sbuf(struct cam_path *path, struct sbuf *sb)
+{
+
+	if (path == NULL)
+		sbuf_printf(sb, "(nopath): ");
+	else {
+		if (path->periph != NULL)
+			sbuf_printf(sb, "(%s%d:", path->periph->periph_name,
+				    path->periph->unit_number);
+		else
+			sbuf_printf(sb, "(noperiph:");
+
+		if (path->bus != NULL)
+			sbuf_printf(sb, "%s%d:%d:", path->bus->sim->sim_name,
+				    path->bus->sim->unit_number,
+				    path->bus->sim->bus_id);
+		else
+			sbuf_printf(sb, "nobus:");
+
+		if (path->target != NULL)
+			sbuf_printf(sb, "%d:", path->target->target_id);
+		else
+			sbuf_printf(sb, "X:");
+
+		if (path->device != NULL)
+			sbuf_printf(sb, "%jx): ",
+			    (uintmax_t)path->device->lun_id);
+		else
+			sbuf_printf(sb, "X): ");
+	}
+
+	return(sbuf_len(sb));
+}
+
+path_id_t
+xpt_path_path_id(struct cam_path *path)
+{
+	return(path->bus->path_id);
+}
+
+target_id_t
+xpt_path_target_id(struct cam_path *path)
+{
+	if (path->target != NULL)
+		return (path->target->target_id);
+	else
+		return (CAM_TARGET_WILDCARD);
+}
+
+lun_id_t
+xpt_path_lun_id(struct cam_path *path)
+{
+	if (path->device != NULL)
+		return (path->device->lun_id);
+	else
+		return (CAM_LUN_WILDCARD);
+}
+
+struct cam_sim *
+xpt_path_sim(struct cam_path *path)
+{
+
+	return (path->bus->sim);
+}
+
+struct cam_periph*
+xpt_path_periph(struct cam_path *path)
+{
+
+	return (path->periph);
+}
+
+/*
+ * Release a CAM control block for the caller.  Remit the cost of the structure
+ * to the device referenced by the path.  If the this device had no 'credits'
+ * and peripheral drivers have registered async callbacks for this notification
+ * call them now.
+ */
+void
+xpt_release_ccb(union ccb *free_ccb)
+{
+	struct	 cam_ed *device;
+	struct	 cam_periph *periph;
+
+	CAM_DEBUG_PRINT(CAM_DEBUG_XPT, ("xpt_release_ccb\n"));
+	xpt_path_assert(free_ccb->ccb_h.path, MA_OWNED);
+	device = free_ccb->ccb_h.path->device;
+	periph = free_ccb->ccb_h.path->periph;
+
+	xpt_free_ccb(free_ccb);
+	periph->periph_allocated--;
+	cam_ccbq_release_opening(&device->ccbq);
+	xpt_run_allocq(periph, 0);
+}
+
+/* Functions accessed by SIM drivers */
+
+static struct xpt_xport_ops xport_default_ops = {
+	.alloc_device = xpt_alloc_device_default,
+	.action = xpt_action_default,
+	.async = xpt_dev_async_default,
+};
+static struct xpt_xport xport_default = {
+	.xport = XPORT_UNKNOWN,
+	.name = "unknown",
+	.ops = &xport_default_ops,
+};
+
+CAM_XPT_XPORT(xport_default);
+
+/*
+ * A sim structure, listing the SIM entry points and instance
+ * identification info is passed to xpt_bus_register to hook the SIM
+ * into the CAM framework.  xpt_bus_register creates a cam_eb entry
+ * for this new bus and places it in the array of buses and assigns
+ * it a path_id.  The path_id may be influenced by "hard wiring"
+ * information specified by the user.  Once interrupt services are
+ * available, the bus will be probed.
+ */
+int32_t
+xpt_bus_register(struct cam_sim *sim, device_t parent, u_int32_t bus)
+{
+	struct cam_eb *new_bus;
+	struct cam_eb *old_bus;
+	struct ccb_pathinq cpi;
+	struct cam_path *path;
+	cam_status status;
+
+	sim->bus_id = bus;
+	new_bus = (struct cam_eb *)malloc(sizeof(*new_bus),
+					  M_CAMXPT, M_NOWAIT|M_ZERO);
+	if (new_bus == NULL) {
+		/* Couldn't satisfy request */
+		return (CAM_RESRC_UNAVAIL);
+	}
+
+	mtx_init(&new_bus->eb_mtx, "CAM bus lock", NULL, MTX_DEF);
+	TAILQ_INIT(&new_bus->et_entries);
+	cam_sim_hold(sim);
+	new_bus->sim = sim;
+	timevalclear(&new_bus->last_reset);
+	new_bus->flags = 0;
+	new_bus->refcount = 1;	/* Held until a bus_deregister event */
+	new_bus->generation = 0;
+
+	xpt_lock_buses();
+	sim->path_id = new_bus->path_id =
+	    xptpathid(sim->sim_name, sim->unit_number, sim->bus_id);
+	old_bus = TAILQ_FIRST(&xsoftc.xpt_busses);
+	while (old_bus != NULL
+	    && old_bus->path_id < new_bus->path_id)
+		old_bus = TAILQ_NEXT(old_bus, links);
+	if (old_bus != NULL)
+		TAILQ_INSERT_BEFORE(old_bus, new_bus, links);
+	else
+		TAILQ_INSERT_TAIL(&xsoftc.xpt_busses, new_bus, links);
+	xsoftc.bus_generation++;
+	xpt_unlock_buses();
+
+	/*
+	 * Set a default transport so that a PATH_INQ can be issued to
+	 * the SIM.  This will then allow for probing and attaching of
+	 * a more appropriate transport.
+	 */
+	new_bus->xport = &xport_default;
+
+	status = xpt_create_path(&path, /*periph*/NULL, sim->path_id,
+				  CAM_TARGET_WILDCARD, CAM_LUN_WILDCARD);
+	if (status != CAM_REQ_CMP) {
+		xpt_release_bus(new_bus);
+		free(path, M_CAMXPT);
+		return (CAM_RESRC_UNAVAIL);
+	}
+
+	xpt_setup_ccb(&cpi.ccb_h, path, CAM_PRIORITY_NORMAL);
+	cpi.ccb_h.func_code = XPT_PATH_INQ;
+	xpt_action((union ccb *)&cpi);
+
+	if (cpi.ccb_h.status == CAM_REQ_CMP) {
+		struct xpt_xport **xpt;
+
+		SET_FOREACH(xpt, cam_xpt_xport_set) {
+			if ((*xpt)->xport == cpi.transport) {
+				new_bus->xport = *xpt;
+				break;
+			}
+		}
+		if (new_bus->xport == NULL) {
+			xpt_print(path,
+			    "No transport found for %d\n", cpi.transport);
+			xpt_release_bus(new_bus);
+			free(path, M_CAMXPT);
+			return (CAM_RESRC_UNAVAIL);
+		}
+	}
+
+	/* Notify interested parties */
+	if (sim->path_id != CAM_XPT_PATH_ID) {
+
+		xpt_async(AC_PATH_REGISTERED, path, &cpi);
+		if ((cpi.hba_misc & PIM_NOSCAN) == 0) {
+			union	ccb *scan_ccb;
+
+			/* Initiate bus rescan. */
+			scan_ccb = xpt_alloc_ccb_nowait();
+			if (scan_ccb != NULL) {
+				scan_ccb->ccb_h.path = path;
+				scan_ccb->ccb_h.func_code = XPT_SCAN_BUS;
+				scan_ccb->crcn.flags = 0;
+				xpt_rescan(scan_ccb);
+			} else {
+				xpt_print(path,
+					  "Can't allocate CCB to scan bus\n");
+				xpt_free_path(path);
+			}
+		} else
+			xpt_free_path(path);
+	} else
+		xpt_free_path(path);
+	return (CAM_SUCCESS);
+}
+
+#ifndef __rtems__
+int32_t
+xpt_bus_deregister(path_id_t pathid)
+{
+	struct cam_path bus_path;
+	cam_status status;
+
+	status = xpt_compile_path(&bus_path, NULL, pathid,
+				  CAM_TARGET_WILDCARD, CAM_LUN_WILDCARD);
+	if (status != CAM_REQ_CMP)
+		return (status);
+
+	xpt_async(AC_LOST_DEVICE, &bus_path, NULL);
+	xpt_async(AC_PATH_DEREGISTERED, &bus_path, NULL);
+
+	/* Release the reference count held while registered. */
+	xpt_release_bus(bus_path.bus);
+	xpt_release_path(&bus_path);
+
+	return (CAM_REQ_CMP);
+}
+
+#endif
+static path_id_t
+xptnextfreepathid(void)
+{
+	struct cam_eb *bus;
+	path_id_t pathid;
+	const char *strval;
+
+	mtx_assert(&xsoftc.xpt_topo_lock, MA_OWNED);
+	pathid = 0;
+	bus = TAILQ_FIRST(&xsoftc.xpt_busses);
+retry:
+	/* Find an unoccupied pathid */
+	while (bus != NULL && bus->path_id <= pathid) {
+		if (bus->path_id == pathid)
+			pathid++;
+		bus = TAILQ_NEXT(bus, links);
+	}
+
+	/*
+	 * Ensure that this pathid is not reserved for
+	 * a bus that may be registered in the future.
+	 */
+	if (resource_string_value("scbus", pathid, "at", &strval) == 0) {
+		++pathid;
+		/* Start the search over */
+		goto retry;
+	}
+	return (pathid);
+}
+
+static path_id_t
+xptpathid(const char *sim_name, int sim_unit, int sim_bus)
+{
+	path_id_t pathid;
+	int i, dunit, val;
+	char buf[32];
+	const char *dname;
+
+	pathid = CAM_XPT_PATH_ID;
+	snprintf(buf, sizeof(buf), "%s%d", sim_name, sim_unit);
+	if (strcmp(buf, "xpt0") == 0 && sim_bus == 0)
+		return (pathid);
+	i = 0;
+	while ((resource_find_match(&i, &dname, &dunit, "at", buf)) == 0) {
+		if (strcmp(dname, "scbus")) {
+			/* Avoid a bit of foot shooting. */
+			continue;
+		}
+		if (dunit < 0)		/* unwired?! */
+			continue;
+		if (resource_int_value("scbus", dunit, "bus", &val) == 0) {
+			if (sim_bus == val) {
+				pathid = dunit;
+				break;
+			}
+		} else if (sim_bus == 0) {
+			/* Unspecified matches bus 0 */
+			pathid = dunit;
+			break;
+		} else {
+			printf("Ambiguous scbus configuration for %s%d "
+			       "bus %d, cannot wire down.  The kernel "
+			       "config entry for scbus%d should "
+			       "specify a controller bus.\n"
+			       "Scbus will be assigned dynamically.\n",
+			       sim_name, sim_unit, sim_bus, dunit);
+			break;
+		}
+	}
+
+	if (pathid == CAM_XPT_PATH_ID)
+		pathid = xptnextfreepathid();
+	return (pathid);
+}
+
+static const char *
+xpt_async_string(u_int32_t async_code)
+{
+
+	switch (async_code) {
+	case AC_BUS_RESET: return ("AC_BUS_RESET");
+	case AC_UNSOL_RESEL: return ("AC_UNSOL_RESEL");
+	case AC_SCSI_AEN: return ("AC_SCSI_AEN");
+	case AC_SENT_BDR: return ("AC_SENT_BDR");
+	case AC_PATH_REGISTERED: return ("AC_PATH_REGISTERED");
+	case AC_PATH_DEREGISTERED: return ("AC_PATH_DEREGISTERED");
+	case AC_FOUND_DEVICE: return ("AC_FOUND_DEVICE");
+	case AC_LOST_DEVICE: return ("AC_LOST_DEVICE");
+	case AC_TRANSFER_NEG: return ("AC_TRANSFER_NEG");
+	case AC_INQ_CHANGED: return ("AC_INQ_CHANGED");
+	case AC_GETDEV_CHANGED: return ("AC_GETDEV_CHANGED");
+	case AC_CONTRACT: return ("AC_CONTRACT");
+	case AC_ADVINFO_CHANGED: return ("AC_ADVINFO_CHANGED");
+	case AC_UNIT_ATTENTION: return ("AC_UNIT_ATTENTION");
+	}
+	return ("AC_UNKNOWN");
+}
+
+static int
+xpt_async_size(u_int32_t async_code)
+{
+
+	switch (async_code) {
+	case AC_BUS_RESET: return (0);
+	case AC_UNSOL_RESEL: return (0);
+	case AC_SCSI_AEN: return (0);
+	case AC_SENT_BDR: return (0);
+	case AC_PATH_REGISTERED: return (sizeof(struct ccb_pathinq));
+	case AC_PATH_DEREGISTERED: return (0);
+	case AC_FOUND_DEVICE: return (sizeof(struct ccb_getdev));
+	case AC_LOST_DEVICE: return (0);
+	case AC_TRANSFER_NEG: return (sizeof(struct ccb_trans_settings));
+	case AC_INQ_CHANGED: return (0);
+	case AC_GETDEV_CHANGED: return (0);
+	case AC_CONTRACT: return (sizeof(struct ac_contract));
+	case AC_ADVINFO_CHANGED: return (-1);
+	case AC_UNIT_ATTENTION: return (sizeof(struct ccb_scsiio));
+	}
+	return (0);
+}
+
+static int
+xpt_async_process_dev(struct cam_ed *device, void *arg)
+{
+	union ccb *ccb = arg;
+	struct cam_path *path = ccb->ccb_h.path;
+	void *async_arg = ccb->casync.async_arg_ptr;
+	u_int32_t async_code = ccb->casync.async_code;
+	int relock;
+
+	if (path->device != device
+	 && path->device->lun_id != CAM_LUN_WILDCARD
+	 && device->lun_id != CAM_LUN_WILDCARD)
+		return (1);
+
+	/*
+	 * The async callback could free the device.
+	 * If it is a broadcast async, it doesn't hold
+	 * device reference, so take our own reference.
+	 */
+	xpt_acquire_device(device);
+
+	/*
+	 * If async for specific device is to be delivered to
+	 * the wildcard client, take the specific device lock.
+	 * XXX: We may need a way for client to specify it.
+	 */
+	if ((device->lun_id == CAM_LUN_WILDCARD &&
+	     path->device->lun_id != CAM_LUN_WILDCARD) ||
+	    (device->target->target_id == CAM_TARGET_WILDCARD &&
+	     path->target->target_id != CAM_TARGET_WILDCARD) ||
+	    (device->target->bus->path_id == CAM_BUS_WILDCARD &&
+	     path->target->bus->path_id != CAM_BUS_WILDCARD)) {
+		mtx_unlock(&device->device_mtx);
+		xpt_path_lock(path);
+		relock = 1;
+	} else
+		relock = 0;
+
+	(*(device->target->bus->xport->ops->async))(async_code,
+	    device->target->bus, device->target, device, async_arg);
+	xpt_async_bcast(&device->asyncs, async_code, path, async_arg);
+
+	if (relock) {
+		xpt_path_unlock(path);
+		mtx_lock(&device->device_mtx);
+	}
+	xpt_release_device(device);
+	return (1);
+}
+
+static int
+xpt_async_process_tgt(struct cam_et *target, void *arg)
+{
+	union ccb *ccb = arg;
+	struct cam_path *path = ccb->ccb_h.path;
+
+	if (path->target != target
+	 && path->target->target_id != CAM_TARGET_WILDCARD
+	 && target->target_id != CAM_TARGET_WILDCARD)
+		return (1);
+
+	if (ccb->casync.async_code == AC_SENT_BDR) {
+		/* Update our notion of when the last reset occurred */
+		microtime(&target->last_reset);
+	}
+
+	return (xptdevicetraverse(target, NULL, xpt_async_process_dev, ccb));
+}
+
+static void
+xpt_async_process(struct cam_periph *periph, union ccb *ccb)
+{
+	struct cam_eb *bus;
+	struct cam_path *path;
+	void *async_arg;
+	u_int32_t async_code;
+
+	path = ccb->ccb_h.path;
+	async_code = ccb->casync.async_code;
+	async_arg = ccb->casync.async_arg_ptr;
+	CAM_DEBUG(path, CAM_DEBUG_TRACE | CAM_DEBUG_INFO,
+	    ("xpt_async(%s)\n", xpt_async_string(async_code)));
+	bus = path->bus;
+
+	if (async_code == AC_BUS_RESET) {
+		/* Update our notion of when the last reset occurred */
+		microtime(&bus->last_reset);
+	}
+
+	xpttargettraverse(bus, NULL, xpt_async_process_tgt, ccb);
+
+	/*
+	 * If this wasn't a fully wildcarded async, tell all
+	 * clients that want all async events.
+	 */
+	if (bus != xpt_periph->path->bus) {
+		xpt_path_lock(xpt_periph->path);
+		xpt_async_process_dev(xpt_periph->path->device, ccb);
+		xpt_path_unlock(xpt_periph->path);
+	}
+
+	if (path->device != NULL && path->device->lun_id != CAM_LUN_WILDCARD)
+		xpt_release_devq(path, 1, TRUE);
+	else
+		xpt_release_simq(path->bus->sim, TRUE);
+	if (ccb->casync.async_arg_size > 0)
+		free(async_arg, M_CAMXPT);
+	xpt_free_path(path);
+	xpt_free_ccb(ccb);
+}
+
+static void
+xpt_async_bcast(struct async_list *async_head,
+		u_int32_t async_code,
+		struct cam_path *path, void *async_arg)
+{
+	struct async_node *cur_entry;
+	struct mtx *mtx;
+
+	cur_entry = SLIST_FIRST(async_head);
+	while (cur_entry != NULL) {
+		struct async_node *next_entry;
+		/*
+		 * Grab the next list entry before we call the current
+		 * entry's callback.  This is because the callback function
+		 * can delete its async callback entry.
+		 */
+		next_entry = SLIST_NEXT(cur_entry, links);
+		if ((cur_entry->event_enable & async_code) != 0) {
+			mtx = cur_entry->event_lock ?
+			    path->device->sim->mtx : NULL;
+			if (mtx)
+				mtx_lock(mtx);
+			cur_entry->callback(cur_entry->callback_arg,
+					    async_code, path,
+					    async_arg);
+			if (mtx)
+				mtx_unlock(mtx);
+		}
+		cur_entry = next_entry;
+	}
+}
+
+void
+xpt_async(u_int32_t async_code, struct cam_path *path, void *async_arg)
+{
+	union ccb *ccb;
+	int size;
+
+	ccb = xpt_alloc_ccb_nowait();
+	if (ccb == NULL) {
+		xpt_print(path, "Can't allocate CCB to send %s\n",
+		    xpt_async_string(async_code));
+		return;
+	}
+
+	if (xpt_clone_path(&ccb->ccb_h.path, path) != CAM_REQ_CMP) {
+		xpt_print(path, "Can't allocate path to send %s\n",
+		    xpt_async_string(async_code));
+		xpt_free_ccb(ccb);
+		return;
+	}
+	ccb->ccb_h.path->periph = NULL;
+	ccb->ccb_h.func_code = XPT_ASYNC;
+	ccb->ccb_h.cbfcnp = xpt_async_process;
+	ccb->ccb_h.flags |= CAM_UNLOCKED;
+	ccb->casync.async_code = async_code;
+	ccb->casync.async_arg_size = 0;
+	size = xpt_async_size(async_code);
+	CAM_DEBUG(ccb->ccb_h.path, CAM_DEBUG_TRACE,
+	    ("xpt_async: func %#x %s aync_code %d %s\n",
+		ccb->ccb_h.func_code,
+		xpt_action_name(ccb->ccb_h.func_code),
+		async_code,
+		xpt_async_string(async_code)));
+	if (size > 0 && async_arg != NULL) {
+		ccb->casync.async_arg_ptr = malloc(size, M_CAMXPT, M_NOWAIT);
+		if (ccb->casync.async_arg_ptr == NULL) {
+			xpt_print(path, "Can't allocate argument to send %s\n",
+			    xpt_async_string(async_code));
+			xpt_free_path(ccb->ccb_h.path);
+			xpt_free_ccb(ccb);
+			return;
+		}
+		memcpy(ccb->casync.async_arg_ptr, async_arg, size);
+		ccb->casync.async_arg_size = size;
+	} else if (size < 0) {
+		ccb->casync.async_arg_ptr = async_arg;
+		ccb->casync.async_arg_size = size;
+	}
+	if (path->device != NULL && path->device->lun_id != CAM_LUN_WILDCARD)
+		xpt_freeze_devq(path, 1);
+	else
+		xpt_freeze_simq(path->bus->sim, 1);
+	xpt_done(ccb);
+}
+
+static void
+xpt_dev_async_default(u_int32_t async_code, struct cam_eb *bus,
+		      struct cam_et *target, struct cam_ed *device,
+		      void *async_arg)
+{
+
+	/*
+	 * We only need to handle events for real devices.
+	 */
+	if (target->target_id == CAM_TARGET_WILDCARD
+	 || device->lun_id == CAM_LUN_WILDCARD)
+		return;
+
+	printf("%s called\n", __func__);
+}
+
+static uint32_t
+xpt_freeze_devq_device(struct cam_ed *dev, u_int count)
+{
+	struct cam_devq	*devq;
+	uint32_t freeze;
+
+	devq = dev->sim->devq;
+	mtx_assert(&devq->send_mtx, MA_OWNED);
+	CAM_DEBUG_DEV(dev, CAM_DEBUG_TRACE,
+	    ("xpt_freeze_devq_device(%d) %u->%u\n", count,
+	    dev->ccbq.queue.qfrozen_cnt, dev->ccbq.queue.qfrozen_cnt + count));
+	freeze = (dev->ccbq.queue.qfrozen_cnt += count);
+	/* Remove frozen device from sendq. */
+	if (device_is_queued(dev))
+		camq_remove(&devq->send_queue, dev->devq_entry.index);
+	return (freeze);
+}
+
+u_int32_t
+xpt_freeze_devq(struct cam_path *path, u_int count)
+{
+	struct cam_ed	*dev = path->device;
+	struct cam_devq	*devq;
+	uint32_t	 freeze;
+
+	devq = dev->sim->devq;
+	mtx_lock(&devq->send_mtx);
+	CAM_DEBUG(path, CAM_DEBUG_TRACE, ("xpt_freeze_devq(%d)\n", count));
+	freeze = xpt_freeze_devq_device(dev, count);
+	mtx_unlock(&devq->send_mtx);
+	return (freeze);
+}
+
+u_int32_t
+xpt_freeze_simq(struct cam_sim *sim, u_int count)
+{
+	struct cam_devq	*devq;
+	uint32_t	 freeze;
+
+	devq = sim->devq;
+	mtx_lock(&devq->send_mtx);
+	freeze = (devq->send_queue.qfrozen_cnt += count);
+	mtx_unlock(&devq->send_mtx);
+	return (freeze);
+}
+
+static void
+xpt_release_devq_timeout(void *arg)
+{
+	struct cam_ed *dev;
+	struct cam_devq *devq;
+
+	dev = (struct cam_ed *)arg;
+	CAM_DEBUG_DEV(dev, CAM_DEBUG_TRACE, ("xpt_release_devq_timeout\n"));
+	devq = dev->sim->devq;
+	mtx_assert(&devq->send_mtx, MA_OWNED);
+	if (xpt_release_devq_device(dev, /*count*/1, /*run_queue*/TRUE))
+		xpt_run_devq(devq);
+}
+
+void
+xpt_release_devq(struct cam_path *path, u_int count, int run_queue)
+{
+	struct cam_ed *dev;
+	struct cam_devq *devq;
+
+	CAM_DEBUG(path, CAM_DEBUG_TRACE, ("xpt_release_devq(%d, %d)\n",
+	    count, run_queue));
+	dev = path->device;
+	devq = dev->sim->devq;
+	mtx_lock(&devq->send_mtx);
+	if (xpt_release_devq_device(dev, count, run_queue))
+		xpt_run_devq(dev->sim->devq);
+	mtx_unlock(&devq->send_mtx);
+}
+
+static int
+xpt_release_devq_device(struct cam_ed *dev, u_int count, int run_queue)
+{
+
+	mtx_assert(&dev->sim->devq->send_mtx, MA_OWNED);
+	CAM_DEBUG_DEV(dev, CAM_DEBUG_TRACE,
+	    ("xpt_release_devq_device(%d, %d) %u->%u\n", count, run_queue,
+	    dev->ccbq.queue.qfrozen_cnt, dev->ccbq.queue.qfrozen_cnt - count));
+	if (count > dev->ccbq.queue.qfrozen_cnt) {
+#ifdef INVARIANTS
+		printf("xpt_release_devq(): requested %u > present %u\n",
+		    count, dev->ccbq.queue.qfrozen_cnt);
+#endif
+		count = dev->ccbq.queue.qfrozen_cnt;
+	}
+	dev->ccbq.queue.qfrozen_cnt -= count;
+	if (dev->ccbq.queue.qfrozen_cnt == 0) {
+		/*
+		 * No longer need to wait for a successful
+		 * command completion.
+		 */
+		dev->flags &= ~CAM_DEV_REL_ON_COMPLETE;
+		/*
+		 * Remove any timeouts that might be scheduled
+		 * to release this queue.
+		 */
+		if ((dev->flags & CAM_DEV_REL_TIMEOUT_PENDING) != 0) {
+			callout_stop(&dev->callout);
+			dev->flags &= ~CAM_DEV_REL_TIMEOUT_PENDING;
+		}
+		/*
+		 * Now that we are unfrozen schedule the
+		 * device so any pending transactions are
+		 * run.
+		 */
+		xpt_schedule_devq(dev->sim->devq, dev);
+	} else
+		run_queue = 0;
+	return (run_queue);
+}
+
+void
+xpt_release_simq(struct cam_sim *sim, int run_queue)
+{
+	struct cam_devq	*devq;
+
+	devq = sim->devq;
+	mtx_lock(&devq->send_mtx);
+	if (devq->send_queue.qfrozen_cnt <= 0) {
+#ifdef INVARIANTS
+		printf("xpt_release_simq: requested 1 > present %u\n",
+		    devq->send_queue.qfrozen_cnt);
+#endif
+	} else
+		devq->send_queue.qfrozen_cnt--;
+	if (devq->send_queue.qfrozen_cnt == 0) {
+		/*
+		 * If there is a timeout scheduled to release this
+		 * sim queue, remove it.  The queue frozen count is
+		 * already at 0.
+		 */
+		if ((sim->flags & CAM_SIM_REL_TIMEOUT_PENDING) != 0){
+			callout_stop(&sim->callout);
+			sim->flags &= ~CAM_SIM_REL_TIMEOUT_PENDING;
+		}
+		if (run_queue) {
+			/*
+			 * Now that we are unfrozen run the send queue.
+			 */
+			xpt_run_devq(sim->devq);
+		}
+	}
+	mtx_unlock(&devq->send_mtx);
+}
+
+/*
+ * XXX Appears to be unused.
+ */
+static void
+xpt_release_simq_timeout(void *arg)
+{
+	struct cam_sim *sim;
+
+	sim = (struct cam_sim *)arg;
+	xpt_release_simq(sim, /* run_queue */ TRUE);
+}
+
+void
+xpt_done(union ccb *done_ccb)
+{
+	struct cam_doneq *queue;
+	int	run, hash;
+
+#if defined(BUF_TRACKING) || defined(FULL_BUF_TRACKING)
+	if (done_ccb->ccb_h.func_code == XPT_SCSI_IO &&
+	    done_ccb->csio.bio != NULL)
+		biotrack(done_ccb->csio.bio, __func__);
+#endif
+
+	CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_TRACE,
+	    ("xpt_done: func= %#x %s status %#x\n",
+		done_ccb->ccb_h.func_code,
+		xpt_action_name(done_ccb->ccb_h.func_code),
+		done_ccb->ccb_h.status));
+	if ((done_ccb->ccb_h.func_code & XPT_FC_QUEUED) == 0)
+		return;
+
+	/* Store the time the ccb was in the sim */
+	done_ccb->ccb_h.qos.sim_data = sbinuptime() - done_ccb->ccb_h.qos.sim_data;
+	hash = (done_ccb->ccb_h.path_id + done_ccb->ccb_h.target_id +
+	    done_ccb->ccb_h.target_lun) % cam_num_doneqs;
+	queue = &cam_doneqs[hash];
+	mtx_lock(&queue->cam_doneq_mtx);
+	run = (queue->cam_doneq_sleep && STAILQ_EMPTY(&queue->cam_doneq));
+	STAILQ_INSERT_TAIL(&queue->cam_doneq, &done_ccb->ccb_h, sim_links.stqe);
+	done_ccb->ccb_h.pinfo.index = CAM_DONEQ_INDEX;
+	mtx_unlock(&queue->cam_doneq_mtx);
+	if (run)
+		wakeup(&queue->cam_doneq);
+}
+
+void
+xpt_done_direct(union ccb *done_ccb)
+{
+
+	CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_TRACE,
+	    ("xpt_done_direct: status %#x\n", done_ccb->ccb_h.status));
+	if ((done_ccb->ccb_h.func_code & XPT_FC_QUEUED) == 0)
+		return;
+
+	/* Store the time the ccb was in the sim */
+	done_ccb->ccb_h.qos.sim_data = sbinuptime() - done_ccb->ccb_h.qos.sim_data;
+	xpt_done_process(&done_ccb->ccb_h);
+}
+
+union ccb *
+xpt_alloc_ccb()
+{
+	union ccb *new_ccb;
+
+	new_ccb = malloc(sizeof(*new_ccb), M_CAMCCB, M_ZERO|M_WAITOK);
+	return (new_ccb);
+}
+
+union ccb *
+xpt_alloc_ccb_nowait()
+{
+	union ccb *new_ccb;
+
+	new_ccb = malloc(sizeof(*new_ccb), M_CAMCCB, M_ZERO|M_NOWAIT);
+	return (new_ccb);
+}
+
+void
+xpt_free_ccb(union ccb *free_ccb)
+{
+	free(free_ccb, M_CAMCCB);
+}
+
+
+
+/* Private XPT functions */
+
+/*
+ * Get a CAM control block for the caller. Charge the structure to the device
+ * referenced by the path.  If we don't have sufficient resources to allocate
+ * more ccbs, we return NULL.
+ */
+/* Made non-static to be called by the mmc_da.c */
+union ccb *
+xpt_get_ccb_nowait(struct cam_periph *periph)
+{
+	union ccb *new_ccb;
+
+	new_ccb = malloc(sizeof(*new_ccb), M_CAMCCB, M_ZERO|M_NOWAIT);
+	if (new_ccb == NULL)
+		return (NULL);
+	periph->periph_allocated++;
+	cam_ccbq_take_opening(&periph->path->device->ccbq);
+	return (new_ccb);
+}
+
+static union ccb *
+xpt_get_ccb(struct cam_periph *periph)
+{
+	union ccb *new_ccb;
+
+	cam_periph_unlock(periph);
+	new_ccb = malloc(sizeof(*new_ccb), M_CAMCCB, M_ZERO|M_WAITOK);
+	cam_periph_lock(periph);
+	periph->periph_allocated++;
+	cam_ccbq_take_opening(&periph->path->device->ccbq);
+	return (new_ccb);
+}
+
+union ccb *
+cam_periph_getccb(struct cam_periph *periph, u_int32_t priority)
+{
+	struct ccb_hdr *ccb_h;
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("cam_periph_getccb\n"));
+	cam_periph_assert(periph, MA_OWNED);
+	while ((ccb_h = SLIST_FIRST(&periph->ccb_list)) == NULL ||
+	    ccb_h->pinfo.priority != priority) {
+		if (priority < periph->immediate_priority) {
+			periph->immediate_priority = priority;
+			xpt_run_allocq(periph, 0);
+		} else
+			cam_periph_sleep(periph, &periph->ccb_list, PRIBIO,
+			    "cgticb", 0);
+	}
+	SLIST_REMOVE_HEAD(&periph->ccb_list, periph_links.sle);
+	return ((union ccb *)ccb_h);
+}
+
+static void
+xpt_acquire_bus(struct cam_eb *bus)
+{
+
+	xpt_lock_buses();
+	bus->refcount++;
+	xpt_unlock_buses();
+}
+
+static void
+xpt_release_bus(struct cam_eb *bus)
+{
+
+	xpt_lock_buses();
+	KASSERT(bus->refcount >= 1, ("bus->refcount >= 1"));
+	if (--bus->refcount > 0) {
+		xpt_unlock_buses();
+		return;
+	}
+	TAILQ_REMOVE(&xsoftc.xpt_busses, bus, links);
+	xsoftc.bus_generation++;
+	xpt_unlock_buses();
+	KASSERT(TAILQ_EMPTY(&bus->et_entries),
+	    ("destroying bus, but target list is not empty"));
+	cam_sim_release(bus->sim);
+	mtx_destroy(&bus->eb_mtx);
+	free(bus, M_CAMXPT);
+}
+
+static struct cam_et *
+xpt_alloc_target(struct cam_eb *bus, target_id_t target_id)
+{
+	struct cam_et *cur_target, *target;
+
+	mtx_assert(&xsoftc.xpt_topo_lock, MA_OWNED);
+	mtx_assert(&bus->eb_mtx, MA_OWNED);
+	target = (struct cam_et *)malloc(sizeof(*target), M_CAMXPT,
+					 M_NOWAIT|M_ZERO);
+	if (target == NULL)
+		return (NULL);
+
+	TAILQ_INIT(&target->ed_entries);
+	target->bus = bus;
+	target->target_id = target_id;
+	target->refcount = 1;
+	target->generation = 0;
+	target->luns = NULL;
+	mtx_init(&target->luns_mtx, "CAM LUNs lock", NULL, MTX_DEF);
+	timevalclear(&target->last_reset);
+	/*
+	 * Hold a reference to our parent bus so it
+	 * will not go away before we do.
+	 */
+	bus->refcount++;
+
+	/* Insertion sort into our bus's target list */
+	cur_target = TAILQ_FIRST(&bus->et_entries);
+	while (cur_target != NULL && cur_target->target_id < target_id)
+		cur_target = TAILQ_NEXT(cur_target, links);
+	if (cur_target != NULL) {
+		TAILQ_INSERT_BEFORE(cur_target, target, links);
+	} else {
+		TAILQ_INSERT_TAIL(&bus->et_entries, target, links);
+	}
+	bus->generation++;
+	return (target);
+}
+
+static void
+xpt_acquire_target(struct cam_et *target)
+{
+	struct cam_eb *bus = target->bus;
+
+	mtx_lock(&bus->eb_mtx);
+	target->refcount++;
+	mtx_unlock(&bus->eb_mtx);
+}
+
+static void
+xpt_release_target(struct cam_et *target)
+{
+	struct cam_eb *bus = target->bus;
+
+	mtx_lock(&bus->eb_mtx);
+	if (--target->refcount > 0) {
+		mtx_unlock(&bus->eb_mtx);
+		return;
+	}
+	TAILQ_REMOVE(&bus->et_entries, target, links);
+	bus->generation++;
+	mtx_unlock(&bus->eb_mtx);
+	KASSERT(TAILQ_EMPTY(&target->ed_entries),
+	    ("destroying target, but device list is not empty"));
+	xpt_release_bus(bus);
+	mtx_destroy(&target->luns_mtx);
+	if (target->luns)
+		free(target->luns, M_CAMXPT);
+	free(target, M_CAMXPT);
+}
+
+static struct cam_ed *
+xpt_alloc_device_default(struct cam_eb *bus, struct cam_et *target,
+			 lun_id_t lun_id)
+{
+	struct cam_ed *device;
+
+	device = xpt_alloc_device(bus, target, lun_id);
+	if (device == NULL)
+		return (NULL);
+
+	device->mintags = 1;
+	device->maxtags = 1;
+	return (device);
+}
+
+static void
+xpt_destroy_device(void *context, int pending)
+{
+	struct cam_ed	*device = context;
+
+	mtx_lock(&device->device_mtx);
+	mtx_destroy(&device->device_mtx);
+	free(device, M_CAMDEV);
+}
+
+struct cam_ed *
+xpt_alloc_device(struct cam_eb *bus, struct cam_et *target, lun_id_t lun_id)
+{
+	struct cam_ed	*cur_device, *device;
+	struct cam_devq	*devq;
+	cam_status status;
+
+	mtx_assert(&bus->eb_mtx, MA_OWNED);
+	/* Make space for us in the device queue on our bus */
+	devq = bus->sim->devq;
+	mtx_lock(&devq->send_mtx);
+	status = cam_devq_resize(devq, devq->send_queue.array_size + 1);
+	mtx_unlock(&devq->send_mtx);
+	if (status != CAM_REQ_CMP)
+		return (NULL);
+
+	device = (struct cam_ed *)malloc(sizeof(*device),
+					 M_CAMDEV, M_NOWAIT|M_ZERO);
+	if (device == NULL)
+		return (NULL);
+
+	cam_init_pinfo(&device->devq_entry);
+	device->target = target;
+	device->lun_id = lun_id;
+	device->sim = bus->sim;
+	if (cam_ccbq_init(&device->ccbq,
+			  bus->sim->max_dev_openings) != 0) {
+		free(device, M_CAMDEV);
+		return (NULL);
+	}
+	SLIST_INIT(&device->asyncs);
+	SLIST_INIT(&device->periphs);
+	device->generation = 0;
+	device->flags = CAM_DEV_UNCONFIGURED;
+	device->tag_delay_count = 0;
+	device->tag_saved_openings = 0;
+	device->refcount = 1;
+	mtx_init(&device->device_mtx, "CAM device lock", NULL, MTX_DEF);
+	callout_init_mtx(&device->callout, &devq->send_mtx, 0);
+	TASK_INIT(&device->device_destroy_task, 0, xpt_destroy_device, device);
+	/*
+	 * Hold a reference to our parent bus so it
+	 * will not go away before we do.
+	 */
+	target->refcount++;
+
+	cur_device = TAILQ_FIRST(&target->ed_entries);
+	while (cur_device != NULL && cur_device->lun_id < lun_id)
+		cur_device = TAILQ_NEXT(cur_device, links);
+	if (cur_device != NULL)
+		TAILQ_INSERT_BEFORE(cur_device, device, links);
+	else
+		TAILQ_INSERT_TAIL(&target->ed_entries, device, links);
+	target->generation++;
+	return (device);
+}
+
+void
+xpt_acquire_device(struct cam_ed *device)
+{
+	struct cam_eb *bus = device->target->bus;
+
+	mtx_lock(&bus->eb_mtx);
+	device->refcount++;
+	mtx_unlock(&bus->eb_mtx);
+}
+
+void
+xpt_release_device(struct cam_ed *device)
+{
+	struct cam_eb *bus = device->target->bus;
+	struct cam_devq *devq;
+
+	mtx_lock(&bus->eb_mtx);
+	if (--device->refcount > 0) {
+		mtx_unlock(&bus->eb_mtx);
+		return;
+	}
+
+	TAILQ_REMOVE(&device->target->ed_entries, device,links);
+	device->target->generation++;
+	mtx_unlock(&bus->eb_mtx);
+
+	/* Release our slot in the devq */
+	devq = bus->sim->devq;
+	mtx_lock(&devq->send_mtx);
+	cam_devq_resize(devq, devq->send_queue.array_size - 1);
+	mtx_unlock(&devq->send_mtx);
+
+	KASSERT(SLIST_EMPTY(&device->periphs),
+	    ("destroying device, but periphs list is not empty"));
+	KASSERT(device->devq_entry.index == CAM_UNQUEUED_INDEX,
+	    ("destroying device while still queued for ccbs"));
+
+	if ((device->flags & CAM_DEV_REL_TIMEOUT_PENDING) != 0)
+		callout_stop(&device->callout);
+
+	xpt_release_target(device->target);
+
+	cam_ccbq_fini(&device->ccbq);
+	/*
+	 * Free allocated memory.  free(9) does nothing if the
+	 * supplied pointer is NULL, so it is safe to call without
+	 * checking.
+	 */
+	free(device->supported_vpds, M_CAMXPT);
+	free(device->device_id, M_CAMXPT);
+	free(device->ext_inq, M_CAMXPT);
+	free(device->physpath, M_CAMXPT);
+	free(device->rcap_buf, M_CAMXPT);
+	free(device->serial_num, M_CAMXPT);
+	taskqueue_enqueue(xsoftc.xpt_taskq, &device->device_destroy_task);
+}
+
+u_int32_t
+xpt_dev_ccbq_resize(struct cam_path *path, int newopenings)
+{
+	int	result;
+	struct	cam_ed *dev;
+
+	dev = path->device;
+	mtx_lock(&dev->sim->devq->send_mtx);
+	result = cam_ccbq_resize(&dev->ccbq, newopenings);
+	mtx_unlock(&dev->sim->devq->send_mtx);
+	if ((dev->flags & CAM_DEV_TAG_AFTER_COUNT) != 0
+	 || (dev->inq_flags & SID_CmdQue) != 0)
+		dev->tag_saved_openings = newopenings;
+	return (result);
+}
+
+static struct cam_eb *
+xpt_find_bus(path_id_t path_id)
+{
+	struct cam_eb *bus;
+
+	xpt_lock_buses();
+	for (bus = TAILQ_FIRST(&xsoftc.xpt_busses);
+	     bus != NULL;
+	     bus = TAILQ_NEXT(bus, links)) {
+		if (bus->path_id == path_id) {
+			bus->refcount++;
+			break;
+		}
+	}
+	xpt_unlock_buses();
+	return (bus);
+}
+
+static struct cam_et *
+xpt_find_target(struct cam_eb *bus, target_id_t	target_id)
+{
+	struct cam_et *target;
+
+	mtx_assert(&bus->eb_mtx, MA_OWNED);
+	for (target = TAILQ_FIRST(&bus->et_entries);
+	     target != NULL;
+	     target = TAILQ_NEXT(target, links)) {
+		if (target->target_id == target_id) {
+			target->refcount++;
+			break;
+		}
+	}
+	return (target);
+}
+
+static struct cam_ed *
+xpt_find_device(struct cam_et *target, lun_id_t lun_id)
+{
+	struct cam_ed *device;
+
+	mtx_assert(&target->bus->eb_mtx, MA_OWNED);
+	for (device = TAILQ_FIRST(&target->ed_entries);
+	     device != NULL;
+	     device = TAILQ_NEXT(device, links)) {
+		if (device->lun_id == lun_id) {
+			device->refcount++;
+			break;
+		}
+	}
+	return (device);
+}
+
+void
+xpt_start_tags(struct cam_path *path)
+{
+	struct ccb_relsim crs;
+	struct cam_ed *device;
+	struct cam_sim *sim;
+	int    newopenings;
+
+	device = path->device;
+	sim = path->bus->sim;
+	device->flags &= ~CAM_DEV_TAG_AFTER_COUNT;
+	xpt_freeze_devq(path, /*count*/1);
+	device->inq_flags |= SID_CmdQue;
+	if (device->tag_saved_openings != 0)
+		newopenings = device->tag_saved_openings;
+	else
+		newopenings = min(device->maxtags,
+				  sim->max_tagged_dev_openings);
+	xpt_dev_ccbq_resize(path, newopenings);
+	xpt_async(AC_GETDEV_CHANGED, path, NULL);
+	xpt_setup_ccb(&crs.ccb_h, path, CAM_PRIORITY_NORMAL);
+	crs.ccb_h.func_code = XPT_REL_SIMQ;
+	crs.release_flags = RELSIM_RELEASE_AFTER_QEMPTY;
+	crs.openings
+	    = crs.release_timeout
+	    = crs.qfrozen_cnt
+	    = 0;
+	xpt_action((union ccb *)&crs);
+}
+
+void
+xpt_stop_tags(struct cam_path *path)
+{
+	struct ccb_relsim crs;
+	struct cam_ed *device;
+	struct cam_sim *sim;
+
+	device = path->device;
+	sim = path->bus->sim;
+	device->flags &= ~CAM_DEV_TAG_AFTER_COUNT;
+	device->tag_delay_count = 0;
+	xpt_freeze_devq(path, /*count*/1);
+	device->inq_flags &= ~SID_CmdQue;
+	xpt_dev_ccbq_resize(path, sim->max_dev_openings);
+	xpt_async(AC_GETDEV_CHANGED, path, NULL);
+	xpt_setup_ccb(&crs.ccb_h, path, CAM_PRIORITY_NORMAL);
+	crs.ccb_h.func_code = XPT_REL_SIMQ;
+	crs.release_flags = RELSIM_RELEASE_AFTER_QEMPTY;
+	crs.openings
+	    = crs.release_timeout
+	    = crs.qfrozen_cnt
+	    = 0;
+	xpt_action((union ccb *)&crs);
+}
+
+static void
+xpt_boot_delay(void *arg)
+{
+
+	xpt_release_boot();
+}
+
+static void
+xpt_config(void *arg)
+{
+	/*
+	 * Now that interrupts are enabled, go find our devices
+	 */
+	if (taskqueue_start_threads(&xsoftc.xpt_taskq, 1, PRIBIO, "CAM taskq"))
+		printf("xpt_config: failed to create taskqueue thread.\n");
+
+	/* Setup debugging path */
+	if (cam_dflags != CAM_DEBUG_NONE) {
+		if (xpt_create_path(&cam_dpath, NULL,
+				    CAM_DEBUG_BUS, CAM_DEBUG_TARGET,
+				    CAM_DEBUG_LUN) != CAM_REQ_CMP) {
+			printf("xpt_config: xpt_create_path() failed for debug"
+			       " target %d:%d:%d, debugging disabled\n",
+			       CAM_DEBUG_BUS, CAM_DEBUG_TARGET, CAM_DEBUG_LUN);
+			cam_dflags = CAM_DEBUG_NONE;
+		}
+	} else
+		cam_dpath = NULL;
+
+	periphdriver_init(1);
+	xpt_hold_boot();
+	callout_init(&xsoftc.boot_callout, 1);
+	callout_reset_sbt(&xsoftc.boot_callout, SBT_1MS * xsoftc.boot_delay, 0,
+	    xpt_boot_delay, NULL, 0);
+	/* Fire up rescan thread. */
+	if (kproc_kthread_add(xpt_scanner_thread, NULL, &cam_proc, NULL, 0, 0,
+	    "cam", "scanner")) {
+		printf("xpt_config: failed to create rescan thread.\n");
+	}
+}
+
+void
+xpt_hold_boot(void)
+{
+	xpt_lock_buses();
+	xsoftc.buses_to_config++;
+	xpt_unlock_buses();
+}
+
+void
+xpt_release_boot(void)
+{
+	xpt_lock_buses();
+	xsoftc.buses_to_config--;
+	if (xsoftc.buses_to_config == 0 && xsoftc.buses_config_done == 0) {
+		struct	xpt_task *task;
+
+		xsoftc.buses_config_done = 1;
+		xpt_unlock_buses();
+		/* Call manually because we don't have any buses */
+		task = malloc(sizeof(struct xpt_task), M_CAMXPT, M_NOWAIT);
+		if (task != NULL) {
+			TASK_INIT(&task->task, 0, xpt_finishconfig_task, task);
+			taskqueue_enqueue(taskqueue_thread, &task->task);
+		}
+	} else
+		xpt_unlock_buses();
+}
+
+/*
+ * If the given device only has one peripheral attached to it, and if that
+ * peripheral is the passthrough driver, announce it.  This insures that the
+ * user sees some sort of announcement for every peripheral in their system.
+ */
+static int
+xptpassannouncefunc(struct cam_ed *device, void *arg)
+{
+	struct cam_periph *periph;
+	int i;
+
+	for (periph = SLIST_FIRST(&device->periphs), i = 0; periph != NULL;
+	     periph = SLIST_NEXT(periph, periph_links), i++);
+
+	periph = SLIST_FIRST(&device->periphs);
+	if ((i == 1)
+	 && (strncmp(periph->periph_name, "pass", 4) == 0))
+		xpt_announce_periph(periph, NULL);
+
+	return(1);
+}
+
+static void
+xpt_finishconfig_task(void *context, int pending)
+{
+
+	periphdriver_init(2);
+	/*
+	 * Check for devices with no "standard" peripheral driver
+	 * attached.  For any devices like that, announce the
+	 * passthrough driver so the user will see something.
+	 */
+	if (!bootverbose)
+		xpt_for_all_devices(xptpassannouncefunc, NULL);
+
+	/* Release our hook so that the boot can continue. */
+	config_intrhook_disestablish(xsoftc.xpt_config_hook);
+	free(xsoftc.xpt_config_hook, M_CAMXPT);
+	xsoftc.xpt_config_hook = NULL;
+
+	free(context, M_CAMXPT);
+}
+
+cam_status
+xpt_register_async(int event, ac_callback_t *cbfunc, void *cbarg,
+		   struct cam_path *path)
+{
+	struct ccb_setasync csa;
+	cam_status status;
+	int xptpath = 0;
+
+	if (path == NULL) {
+		status = xpt_create_path(&path, /*periph*/NULL, CAM_XPT_PATH_ID,
+					 CAM_TARGET_WILDCARD, CAM_LUN_WILDCARD);
+		if (status != CAM_REQ_CMP)
+			return (status);
+		xpt_path_lock(path);
+		xptpath = 1;
+	}
+
+	xpt_setup_ccb(&csa.ccb_h, path, CAM_PRIORITY_NORMAL);
+	csa.ccb_h.func_code = XPT_SASYNC_CB;
+	csa.event_enable = event;
+	csa.callback = cbfunc;
+	csa.callback_arg = cbarg;
+	xpt_action((union ccb *)&csa);
+	status = csa.ccb_h.status;
+
+	CAM_DEBUG(csa.ccb_h.path, CAM_DEBUG_TRACE,
+	    ("xpt_register_async: func %p\n", cbfunc));
+
+	if (xptpath) {
+		xpt_path_unlock(path);
+		xpt_free_path(path);
+	}
+
+	if ((status == CAM_REQ_CMP) &&
+	    (csa.event_enable & AC_FOUND_DEVICE)) {
+		/*
+		 * Get this peripheral up to date with all
+		 * the currently existing devices.
+		 */
+		xpt_for_all_devices(xptsetasyncfunc, &csa);
+	}
+	if ((status == CAM_REQ_CMP) &&
+	    (csa.event_enable & AC_PATH_REGISTERED)) {
+		/*
+		 * Get this peripheral up to date with all
+		 * the currently existing buses.
+		 */
+		xpt_for_all_busses(xptsetasyncbusfunc, &csa);
+	}
+
+	return (status);
+}
+
+static void
+xptaction(struct cam_sim *sim, union ccb *work_ccb)
+{
+	CAM_DEBUG(work_ccb->ccb_h.path, CAM_DEBUG_TRACE, ("xptaction\n"));
+
+	switch (work_ccb->ccb_h.func_code) {
+	/* Common cases first */
+	case XPT_PATH_INQ:		/* Path routing inquiry */
+	{
+		struct ccb_pathinq *cpi;
+
+		cpi = &work_ccb->cpi;
+		cpi->version_num = 1; /* XXX??? */
+		cpi->hba_inquiry = 0;
+		cpi->target_sprt = 0;
+		cpi->hba_misc = 0;
+		cpi->hba_eng_cnt = 0;
+		cpi->max_target = 0;
+		cpi->max_lun = 0;
+		cpi->initiator_id = 0;
+		strlcpy(cpi->sim_vid, "FreeBSD", SIM_IDLEN);
+		strlcpy(cpi->hba_vid, "", HBA_IDLEN);
+		strlcpy(cpi->dev_name, sim->sim_name, DEV_IDLEN);
+		cpi->unit_number = sim->unit_number;
+		cpi->bus_id = sim->bus_id;
+		cpi->base_transfer_speed = 0;
+		cpi->protocol = PROTO_UNSPECIFIED;
+		cpi->protocol_version = PROTO_VERSION_UNSPECIFIED;
+		cpi->transport = XPORT_UNSPECIFIED;
+		cpi->transport_version = XPORT_VERSION_UNSPECIFIED;
+		cpi->ccb_h.status = CAM_REQ_CMP;
+		xpt_done(work_ccb);
+		break;
+	}
+	default:
+		work_ccb->ccb_h.status = CAM_REQ_INVALID;
+		xpt_done(work_ccb);
+		break;
+	}
+}
+
+/*
+ * The xpt as a "controller" has no interrupt sources, so polling
+ * is a no-op.
+ */
+static void
+xptpoll(struct cam_sim *sim)
+{
+}
+
+void
+xpt_lock_buses(void)
+{
+	mtx_lock(&xsoftc.xpt_topo_lock);
+}
+
+void
+xpt_unlock_buses(void)
+{
+	mtx_unlock(&xsoftc.xpt_topo_lock);
+}
+
+struct mtx *
+xpt_path_mtx(struct cam_path *path)
+{
+
+	return (&path->device->device_mtx);
+}
+
+static void
+xpt_done_process(struct ccb_hdr *ccb_h)
+{
+	struct cam_sim *sim;
+	struct cam_devq *devq;
+	struct mtx *mtx = NULL;
+
+#if defined(BUF_TRACKING) || defined(FULL_BUF_TRACKING)
+	struct ccb_scsiio *csio;
+
+	if (ccb_h->func_code == XPT_SCSI_IO) {
+		csio = &((union ccb *)ccb_h)->csio;
+		if (csio->bio != NULL)
+			biotrack(csio->bio, __func__);
+	}
+#endif
+
+	if (ccb_h->flags & CAM_HIGH_POWER) {
+		struct highpowerlist	*hphead;
+		struct cam_ed		*device;
+
+		mtx_lock(&xsoftc.xpt_highpower_lock);
+		hphead = &xsoftc.highpowerq;
+
+		device = STAILQ_FIRST(hphead);
+
+		/*
+		 * Increment the count since this command is done.
+		 */
+		xsoftc.num_highpower++;
+
+		/*
+		 * Any high powered commands queued up?
+		 */
+		if (device != NULL) {
+
+			STAILQ_REMOVE_HEAD(hphead, highpowerq_entry);
+			mtx_unlock(&xsoftc.xpt_highpower_lock);
+
+			mtx_lock(&device->sim->devq->send_mtx);
+			xpt_release_devq_device(device,
+					 /*count*/1, /*runqueue*/TRUE);
+			mtx_unlock(&device->sim->devq->send_mtx);
+		} else
+			mtx_unlock(&xsoftc.xpt_highpower_lock);
+	}
+
+	sim = ccb_h->path->bus->sim;
+
+	if (ccb_h->status & CAM_RELEASE_SIMQ) {
+		xpt_release_simq(sim, /*run_queue*/FALSE);
+		ccb_h->status &= ~CAM_RELEASE_SIMQ;
+	}
+
+	if ((ccb_h->flags & CAM_DEV_QFRZDIS)
+	 && (ccb_h->status & CAM_DEV_QFRZN)) {
+		xpt_release_devq(ccb_h->path, /*count*/1, /*run_queue*/TRUE);
+		ccb_h->status &= ~CAM_DEV_QFRZN;
+	}
+
+	devq = sim->devq;
+	if ((ccb_h->func_code & XPT_FC_USER_CCB) == 0) {
+		struct cam_ed *dev = ccb_h->path->device;
+
+		mtx_lock(&devq->send_mtx);
+		devq->send_active--;
+		devq->send_openings++;
+		cam_ccbq_ccb_done(&dev->ccbq, (union ccb *)ccb_h);
+
+		if (((dev->flags & CAM_DEV_REL_ON_QUEUE_EMPTY) != 0
+		  && (dev->ccbq.dev_active == 0))) {
+			dev->flags &= ~CAM_DEV_REL_ON_QUEUE_EMPTY;
+			xpt_release_devq_device(dev, /*count*/1,
+					 /*run_queue*/FALSE);
+		}
+
+		if (((dev->flags & CAM_DEV_REL_ON_COMPLETE) != 0
+		  && (ccb_h->status&CAM_STATUS_MASK) != CAM_REQUEUE_REQ)) {
+			dev->flags &= ~CAM_DEV_REL_ON_COMPLETE;
+			xpt_release_devq_device(dev, /*count*/1,
+					 /*run_queue*/FALSE);
+		}
+
+		if (!device_is_queued(dev))
+			(void)xpt_schedule_devq(devq, dev);
+		xpt_run_devq(devq);
+		mtx_unlock(&devq->send_mtx);
+
+		if ((dev->flags & CAM_DEV_TAG_AFTER_COUNT) != 0) {
+			mtx = xpt_path_mtx(ccb_h->path);
+			mtx_lock(mtx);
+
+			if ((dev->flags & CAM_DEV_TAG_AFTER_COUNT) != 0
+			 && (--dev->tag_delay_count == 0))
+				xpt_start_tags(ccb_h->path);
+		}
+	}
+
+	if ((ccb_h->flags & CAM_UNLOCKED) == 0) {
+		if (mtx == NULL) {
+			mtx = xpt_path_mtx(ccb_h->path);
+			mtx_lock(mtx);
+		}
+	} else {
+		if (mtx != NULL) {
+			mtx_unlock(mtx);
+			mtx = NULL;
+		}
+	}
+
+	/* Call the peripheral driver's callback */
+	ccb_h->pinfo.index = CAM_UNQUEUED_INDEX;
+	(*ccb_h->cbfcnp)(ccb_h->path->periph, (union ccb *)ccb_h);
+	if (mtx != NULL)
+		mtx_unlock(mtx);
+}
+
+void
+xpt_done_td(void *arg)
+{
+	struct cam_doneq *queue = arg;
+	struct ccb_hdr *ccb_h;
+	STAILQ_HEAD(, ccb_hdr)	doneq;
+
+	STAILQ_INIT(&doneq);
+	mtx_lock(&queue->cam_doneq_mtx);
+	while (1) {
+		while (STAILQ_EMPTY(&queue->cam_doneq)) {
+			queue->cam_doneq_sleep = 1;
+			msleep(&queue->cam_doneq, &queue->cam_doneq_mtx,
+			    PRIBIO, "-", 0);
+			queue->cam_doneq_sleep = 0;
+		}
+		STAILQ_CONCAT(&doneq, &queue->cam_doneq);
+		mtx_unlock(&queue->cam_doneq_mtx);
+
+#ifndef __rtems__
+		THREAD_NO_SLEEPING();
+#endif
+		while ((ccb_h = STAILQ_FIRST(&doneq)) != NULL) {
+			STAILQ_REMOVE_HEAD(&doneq, sim_links.stqe);
+			xpt_done_process(ccb_h);
+		}
+#ifndef __rtems__
+		THREAD_SLEEPING_OK();
+
+#endif
+		mtx_lock(&queue->cam_doneq_mtx);
+	}
+}
+
+static void
+camisr_runqueue(void)
+{
+	struct	ccb_hdr *ccb_h;
+	struct cam_doneq *queue;
+	int i;
+
+	/* Process global queues. */
+	for (i = 0; i < cam_num_doneqs; i++) {
+		queue = &cam_doneqs[i];
+		mtx_lock(&queue->cam_doneq_mtx);
+		while ((ccb_h = STAILQ_FIRST(&queue->cam_doneq)) != NULL) {
+			STAILQ_REMOVE_HEAD(&queue->cam_doneq, sim_links.stqe);
+			mtx_unlock(&queue->cam_doneq_mtx);
+			xpt_done_process(ccb_h);
+			mtx_lock(&queue->cam_doneq_mtx);
+		}
+		mtx_unlock(&queue->cam_doneq_mtx);
+	}
+}
+
+struct kv 
+{
+	uint32_t v;
+	const char *name;
+};
+
+static struct kv map[] = {
+	{ XPT_NOOP, "XPT_NOOP" },
+	{ XPT_SCSI_IO, "XPT_SCSI_IO" },
+	{ XPT_GDEV_TYPE, "XPT_GDEV_TYPE" },
+	{ XPT_GDEVLIST, "XPT_GDEVLIST" },
+	{ XPT_PATH_INQ, "XPT_PATH_INQ" },
+	{ XPT_REL_SIMQ, "XPT_REL_SIMQ" },
+	{ XPT_SASYNC_CB, "XPT_SASYNC_CB" },
+	{ XPT_SDEV_TYPE, "XPT_SDEV_TYPE" },
+	{ XPT_SCAN_BUS, "XPT_SCAN_BUS" },
+	{ XPT_DEV_MATCH, "XPT_DEV_MATCH" },
+	{ XPT_DEBUG, "XPT_DEBUG" },
+	{ XPT_PATH_STATS, "XPT_PATH_STATS" },
+	{ XPT_GDEV_STATS, "XPT_GDEV_STATS" },
+	{ XPT_DEV_ADVINFO, "XPT_DEV_ADVINFO" },
+	{ XPT_ASYNC, "XPT_ASYNC" },
+	{ XPT_ABORT, "XPT_ABORT" },
+	{ XPT_RESET_BUS, "XPT_RESET_BUS" },
+	{ XPT_RESET_DEV, "XPT_RESET_DEV" },
+	{ XPT_TERM_IO, "XPT_TERM_IO" },
+	{ XPT_SCAN_LUN, "XPT_SCAN_LUN" },
+	{ XPT_GET_TRAN_SETTINGS, "XPT_GET_TRAN_SETTINGS" },
+	{ XPT_SET_TRAN_SETTINGS, "XPT_SET_TRAN_SETTINGS" },
+	{ XPT_CALC_GEOMETRY, "XPT_CALC_GEOMETRY" },
+	{ XPT_ATA_IO, "XPT_ATA_IO" },
+	{ XPT_GET_SIM_KNOB, "XPT_GET_SIM_KNOB" },
+	{ XPT_SET_SIM_KNOB, "XPT_SET_SIM_KNOB" },
+	{ XPT_NVME_IO, "XPT_NVME_IO" },
+	{ XPT_MMC_IO, "XPT_MMC_IO" },
+	{ XPT_SMP_IO, "XPT_SMP_IO" },
+	{ XPT_SCAN_TGT, "XPT_SCAN_TGT" },
+	{ XPT_ENG_INQ, "XPT_ENG_INQ" },
+	{ XPT_ENG_EXEC, "XPT_ENG_EXEC" },
+	{ XPT_EN_LUN, "XPT_EN_LUN" },
+	{ XPT_TARGET_IO, "XPT_TARGET_IO" },
+	{ XPT_ACCEPT_TARGET_IO, "XPT_ACCEPT_TARGET_IO" },
+	{ XPT_CONT_TARGET_IO, "XPT_CONT_TARGET_IO" },
+	{ XPT_IMMED_NOTIFY, "XPT_IMMED_NOTIFY" },
+	{ XPT_NOTIFY_ACK, "XPT_NOTIFY_ACK" },
+	{ XPT_IMMEDIATE_NOTIFY, "XPT_IMMEDIATE_NOTIFY" },
+	{ XPT_NOTIFY_ACKNOWLEDGE, "XPT_NOTIFY_ACKNOWLEDGE" },
+	{ 0, 0 }
+};
+
+const char *
+xpt_action_name(uint32_t action) 
+{
+	static char buffer[32];	/* Only for unknown messages -- racy */
+	struct kv *walker = map;
+
+	while (walker->name != NULL) {
+		if (walker->v == action)
+			return (walker->name);
+		walker++;
+	}
+
+	snprintf(buffer, sizeof(buffer), "%#x", action);
+	return (buffer);
+}
diff --git a/freebsd/sys/cam/cam_xpt.h b/freebsd/sys/cam/cam_xpt.h
index 8e6027e..a652309 100644
--- a/freebsd/sys/cam/cam_xpt.h
+++ b/freebsd/sys/cam/cam_xpt.h
@@ -34,6 +34,7 @@
 
 #ifdef _KERNEL
 #include <sys/cdefs.h>
+#include <cam/cam_ccb.h>
 #endif
 
 /* Forward Declarations */
@@ -141,6 +142,18 @@ void			xpt_copy_path(struct cam_path *new_path,
 
 void			xpt_release_path(struct cam_path *path);
 
+const char *		xpt_action_name(uint32_t action);
+
+static inline void
+xpt_path_inq(struct ccb_pathinq *cpi, struct cam_path *path)
+{
+
+	bzero(cpi, sizeof(*cpi));
+	xpt_setup_ccb(&cpi->ccb_h, path, CAM_PRIORITY_NORMAL);
+	cpi->ccb_h.func_code = XPT_PATH_INQ;
+	xpt_action((union ccb *)cpi);
+}
+
 #endif /* _KERNEL */
 
 #endif /* _CAM_CAM_XPT_H */
diff --git a/freebsd/sys/cam/cam_xpt_sim.h b/freebsd/sys/cam/cam_xpt_sim.h
index c3575de..1543645 100644
--- a/freebsd/sys/cam/cam_xpt_sim.h
+++ b/freebsd/sys/cam/cam_xpt_sim.h
@@ -42,11 +42,7 @@ int32_t		xpt_bus_register(struct cam_sim *sim, device_t parent,
 int32_t		xpt_bus_deregister(path_id_t path_id);
 u_int32_t	xpt_freeze_simq(struct cam_sim *sim, u_int count);
 void		xpt_release_simq(struct cam_sim *sim, int run_queue);
-#ifndef __rtems__
 u_int32_t	xpt_freeze_devq(struct cam_path *path, u_int count);
-#else /* __rtems__ */
-#define xpt_freeze_devq(path, count) do { } while (0)
-#endif /* __rtems__ */
 void		xpt_release_devq(struct cam_path *path,
 		    u_int count, int run_queue);
 void		xpt_done(union ccb *done_ccb);
diff --git a/freebsd/sys/cam/mmc/mmc.h b/freebsd/sys/cam/mmc/mmc.h
new file mode 100644
index 0000000..615e88f
--- /dev/null
+++ b/freebsd/sys/cam/mmc/mmc.h
@@ -0,0 +1,106 @@
+/*-
+ * Copyright (c) 2014-2016 Ilya Bakulin.  All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * Portions of this software may have been developed with reference to
+ * the SD Simplified Specification.  The following disclaimer may apply:
+ *
+ * The following conditions apply to the release of the simplified
+ * specification ("Simplified Specification") by the SD Card Association and
+ * the SD Group. The Simplified Specification is a subset of the complete SD
+ * Specification which is owned by the SD Card Association and the SD
+ * Group. This Simplified Specification is provided on a non-confidential
+ * basis subject to the disclaimers below. Any implementation of the
+ * Simplified Specification may require a license from the SD Card
+ * Association, SD Group, SD-3C LLC or other third parties.
+ *
+ * Disclaimers:
+ *
+ * The information contained in the Simplified Specification is presented only
+ * as a standard specification for SD Cards and SD Host/Ancillary products and
+ * is provided "AS-IS" without any representations or warranties of any
+ * kind. No responsibility is assumed by the SD Group, SD-3C LLC or the SD
+ * Card Association for any damages, any infringements of patents or other
+ * right of the SD Group, SD-3C LLC, the SD Card Association or any third
+ * parties, which may result from its use. No license is granted by
+ * implication, estoppel or otherwise under any patent or other rights of the
+ * SD Group, SD-3C LLC, the SD Card Association or any third party. Nothing
+ * herein shall be construed as an obligation by the SD Group, the SD-3C LLC
+ * or the SD Card Association to disclose or distribute any technical
+ * information, know-how or other confidential information to any third party.
+ *
+ * Inspired coded in sys/dev/mmc. Thanks to Warner Losh <imp@FreeBSD.org>,
+ * Bernd Walter <tisco@FreeBSD.org>, and other authors.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef CAM_MMC_H
+#define CAM_MMC_H
+
+#include <dev/mmc/mmcreg.h>
+/*
+ * This structure describes an MMC/SD card
+ */
+struct mmc_params {
+        u_int8_t	model[40]; /* Card model */
+
+        /* Card OCR */
+        uint32_t card_ocr;
+
+        /* OCR of the IO portion of the card */
+        uint32_t io_ocr;
+
+        /* Card CID -- raw and parsed */
+        uint32_t card_cid[4];
+        struct mmc_cid  cid;
+
+        /* Card CSD -- raw */
+        uint32_t card_csd[4];
+
+        /* Card RCA */
+        uint16_t card_rca;
+
+        /* What kind of card is it */
+        uint32_t card_features;
+#define CARD_FEATURE_MEMORY 0x1
+#define CARD_FEATURE_SDHC   0x1 << 1
+#define CARD_FEATURE_SDIO   0x1 << 2
+#define CARD_FEATURE_SD20   0x1 << 3
+#define CARD_FEATURE_MMC    0x1 << 4
+#define CARD_FEATURE_18V    0x1 << 5
+
+        uint8_t sdio_func_count;
+} __packed;
+
+/*
+ * Only one MMC card on bus is supported now.
+ * If we ever want to support multiple MMC cards on the same bus,
+ * mmc_xpt needs to be extended to issue new RCAs based on number
+ * of already probed cards. Furthermore, retuning and high-speed
+ * settings should also take all cards into account.
+ */
+#define MMC_PROPOSED_RCA    2
+#ifdef __rtems__
+#define PRIBIO 60
+#endif
+#endif
diff --git a/freebsd/sys/cam/mmc/mmc_all.h b/freebsd/sys/cam/mmc/mmc_all.h
new file mode 100644
index 0000000..c249489
--- /dev/null
+++ b/freebsd/sys/cam/mmc/mmc_all.h
@@ -0,0 +1,70 @@
+/*-
+ * Copyright (c) 2014-2016 Ilya Bakulin.  All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * Portions of this software may have been developed with reference to
+ * the SD Simplified Specification.  The following disclaimer may apply:
+ *
+ * The following conditions apply to the release of the simplified
+ * specification ("Simplified Specification") by the SD Card Association and
+ * the SD Group. The Simplified Specification is a subset of the complete SD
+ * Specification which is owned by the SD Card Association and the SD
+ * Group. This Simplified Specification is provided on a non-confidential
+ * basis subject to the disclaimers below. Any implementation of the
+ * Simplified Specification may require a license from the SD Card
+ * Association, SD Group, SD-3C LLC or other third parties.
+ *
+ * Disclaimers:
+ *
+ * The information contained in the Simplified Specification is presented only
+ * as a standard specification for SD Cards and SD Host/Ancillary products and
+ * is provided "AS-IS" without any representations or warranties of any
+ * kind. No responsibility is assumed by the SD Group, SD-3C LLC or the SD
+ * Card Association for any damages, any infringements of patents or other
+ * right of the SD Group, SD-3C LLC, the SD Card Association or any third
+ * parties, which may result from its use. No license is granted by
+ * implication, estoppel or otherwise under any patent or other rights of the
+ * SD Group, SD-3C LLC, the SD Card Association or any third party. Nothing
+ * herein shall be construed as an obligation by the SD Group, the SD-3C LLC
+ * or the SD Card Association to disclose or distribute any technical
+ * information, know-how or other confidential information to any third party.
+ *
+ * $FreeBSD$
+ */
+
+/*
+ * MMC function that should be visible to the CAM subsystem
+ * and are somehow useful should be declared here
+ *
+ * Like in other *_all.h, it's also a nice place to include
+ * some other transport-specific headers.
+ */
+
+#ifndef CAM_MMC_ALL_H
+#define CAM_MMC_ALL_H
+
+#include <cam/mmc/mmc.h>
+#include <dev/mmc/mmcreg.h>
+
+void	mmc_print_ident(struct mmc_params *ident_data);
+
+#endif
diff --git a/freebsd/sys/cam/mmc/mmc_bus.h b/freebsd/sys/cam/mmc/mmc_bus.h
new file mode 100644
index 0000000..db77da5
--- /dev/null
+++ b/freebsd/sys/cam/mmc/mmc_bus.h
@@ -0,0 +1,5 @@
+/*
+ * This file is in the public domain.
+ * $FreeBSD$
+ */
+#include <dev/mmc/bridge.h>
diff --git a/freebsd/sys/cam/mmc/mmc_da.c b/freebsd/sys/cam/mmc/mmc_da.c
new file mode 100644
index 0000000..75b7df8
--- /dev/null
+++ b/freebsd/sys/cam/mmc/mmc_da.c
@@ -0,0 +1,2187 @@
+#include <machine/rtems-bsd-kernel-space.h>
+
+/*-
+ * SPDX-License-Identifier: BSD-2-Clause-FreeBSD
+ *
+ * Copyright (c) 2006 Bernd Walter <tisco@FreeBSD.org>
+ * Copyright (c) 2006 M. Warner Losh <imp@FreeBSD.org>
+ * Copyright (c) 2009 Alexander Motin <mav@FreeBSD.org>
+ * Copyright (c) 2015-2017 Ilya Bakulin <kibab@FreeBSD.org>
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * Some code derived from the sys/dev/mmc and sys/cam/ata
+ * Thanks to Warner Losh <imp@FreeBSD.org>, Alexander Motin <mav@FreeBSD.org>
+ * Bernd Walter <tisco@FreeBSD.org>, and other authors.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+//#include <rtems/bsd/local/opt_sdda.h>
+
+#include <sys/param.h>
+
+#ifdef _KERNEL
+#include <sys/systm.h>
+#include <sys/kernel.h>
+#include <sys/bio.h>
+#include <sys/endian.h>
+#include <sys/taskqueue.h>
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/conf.h>
+#include <sys/devicestat.h>
+#include <sys/eventhandler.h>
+#include <sys/malloc.h>
+#include <sys/cons.h>
+#include <sys/proc.h>
+#include <sys/reboot.h>
+#include <geom/geom_disk.h>
+#include <machine/_inttypes.h>  /* for PRIu64 */
+#endif /* _KERNEL */
+
+#ifndef _KERNEL
+#include <stdio.h>
+#include <string.h>
+#endif /* _KERNEL */
+
+#include <cam/cam.h>
+#include <cam/cam_ccb.h>
+#include <cam/cam_queue.h>
+#include <cam/cam_periph.h>
+#include <cam/cam_sim.h>
+#include <cam/cam_xpt.h>
+#include <cam/cam_xpt_sim.h>
+#include <cam/cam_xpt_periph.h>
+#include <cam/cam_xpt_internal.h>
+#include <cam/cam_debug.h>
+
+#include <cam/mmc/mmc_all.h>
+#ifdef __rtems__
+#include <machine/rtems-bsd-support.h>
+#include <rtems/bdbuf.h>
+#include <rtems/diskdevs.h>
+#include <rtems/libio.h>
+#include <rtems/media.h>
+#endif /* __rtems__ */
+
+#include "md_var.h"	/* geometry translation */
+
+#ifdef _KERNEL
+
+typedef enum {
+	SDDA_FLAG_OPEN		= 0x0002,
+	SDDA_FLAG_DIRTY		= 0x0004
+} sdda_flags;
+
+typedef enum {
+	SDDA_STATE_INIT,
+	SDDA_STATE_INVALID,
+	SDDA_STATE_NORMAL,
+	SDDA_STATE_PART_SWITCH,
+} sdda_state;
+
+#define	SDDA_FMT_BOOT		"sdda%dboot"
+#define	SDDA_FMT_GP		"sdda%dgp"
+#define	SDDA_FMT_RPMB		"sdda%drpmb"
+#define	SDDA_LABEL_ENH		"enh"
+
+#define	SDDA_PART_NAMELEN	(16 + 1)
+
+struct sdda_softc;
+
+struct sdda_part {
+#ifndef __rtems__
+	struct disk *disk;
+	struct bio_queue_head bio_queue;
+#endif
+	sdda_flags flags;
+	struct sdda_softc *sc;
+	u_int cnt;
+	u_int type;
+	bool ro;
+	char name[SDDA_PART_NAMELEN];
+};
+
+union ccb *GLOBAL_START_CCB=NULL;
+
+struct sdda_softc {
+	int	 outstanding_cmds;	/* Number of active commands */
+	int	 refcount;		/* Active xpt_action() calls */
+	sdda_state state;
+	struct mmc_data *mmcdata;
+	struct cam_periph *periph;
+//	sdda_quirks quirks;
+	struct task start_init_task;
+	uint32_t raw_csd[4];
+	uint8_t raw_ext_csd[512]; /* MMC only? */
+	struct mmc_csd csd;
+	struct mmc_cid cid;
+	struct mmc_scr scr;
+	/* Calculated from CSD */
+	uint64_t sector_count;
+	uint64_t mediasize;
+
+	/* Calculated from CID */
+	char card_id_string[64];/* Formatted CID info (serial, MFG, etc) */
+	char card_sn_string[16];/* Formatted serial # for disk->d_ident */
+	/* Determined from CSD + is highspeed card*/
+	uint32_t card_f_max;
+
+	/* Generic switch timeout */
+	uint32_t cmd6_time;
+	/* MMC partitions support */
+	struct sdda_part *part[MMC_PART_MAX];
+	uint8_t part_curr;	/* Partition currently switched to */
+	uint8_t part_requested; /* What partition we're currently switching to */
+	uint32_t part_time;	/* Partition switch timeout [us] */
+	off_t enh_base;		/* Enhanced user data area slice base ... */
+	off_t enh_size;		/* ... and size [bytes] */
+	int log_count;
+	struct timeval log_time;
+};
+
+#define ccb_bp		ppriv_ptr1
+#ifndef __rtems__
+static	disk_strategy_t	sddastrategy;
+#endif
+periph_init_t	sddainit;
+static	void		sddaasync(void *callback_arg, u_int32_t code,
+				struct cam_path *path, void *arg);
+static	periph_ctor_t	sddaregister;
+static	periph_dtor_t	sddacleanup;
+static	periph_start_t	sddastart;
+static	periph_oninv_t	sddaoninvalidate;
+static	void		sddadone(struct cam_periph *periph,
+			       union ccb *done_ccb);
+static  int		sddaerror(union ccb *ccb, u_int32_t cam_flags,
+				u_int32_t sense_flags);
+void sddaschedule(struct cam_periph *periph);
+static uint16_t get_rca(struct cam_periph *periph);
+static void sdda_start_init(void *context, union ccb *start_ccb);
+static void sdda_start_init_task(void *context, int pending);
+static void sdda_process_mmc_partitions(struct cam_periph *periph, union ccb *start_ccb);
+static uint32_t sdda_get_host_caps(struct cam_periph *periph, union ccb *ccb);
+static void sdda_init_switch_part(struct cam_periph *periph, union ccb *start_ccb, u_int part);
+static int mmc_select_card(struct cam_periph *periph, union ccb *ccb, uint32_t rca);
+static inline uint32_t mmc_get_sector_size(struct cam_periph *periph) {return MMC_SECTOR_SIZE;}
+
+/* TODO: actually issue GET_TRAN_SETTINGS to get R/O status */
+static inline bool sdda_get_read_only(struct cam_periph *periph, union ccb *start_ccb)
+{
+
+	return (false);
+}
+
+static uint32_t mmc_get_spec_vers(struct cam_periph *periph);
+static uint64_t mmc_get_media_size(struct cam_periph *periph);
+static uint32_t mmc_get_cmd6_timeout(struct cam_periph *periph);
+static void sdda_add_part(struct cam_periph *periph, u_int type,
+    const char *name, u_int cnt, off_t media_size, bool ro);
+
+rtems_blkdev_sg_buffer *sg;
+
+static struct periph_driver sddadriver =
+{
+	sddainit, "sdda",
+	TAILQ_HEAD_INITIALIZER(sddadriver.units), /* generation */ 0
+};
+
+PERIPHDRIVER_DECLARE(sdda, sddadriver);
+
+static MALLOC_DEFINE(M_SDDA, "sd_da", "sd_da buffers");
+
+static const int exp[8] = {
+	1, 10, 100, 1000, 10000, 100000, 1000000, 10000000
+};
+
+static const int mant[16] = {
+	0, 10, 12, 13, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 70, 80
+};
+
+static const int cur_min[8] = {
+	500, 1000, 5000, 10000, 25000, 35000, 60000, 100000
+};
+
+static const int cur_max[8] = {
+	1000, 5000, 10000, 25000, 35000, 45000, 800000, 200000
+};
+
+#ifdef __rtems__
+static rtems_status_code
+rtems_bsd_sdda_set_block_size(device_t dev, uint32_t block_size)
+{
+	rtems_status_code status_code = RTEMS_SUCCESSFUL;
+	struct mmc_command cmd;
+	struct mmc_request req;
+
+	memset(&req, 0, sizeof(req));
+	memset(&cmd, 0, sizeof(cmd));
+
+	req.cmd = &cmd;
+	cmd.opcode = MMC_SET_BLOCKLEN;
+	cmd.flags = MMC_RSP_R1 | MMC_CMD_AC;
+	cmd.arg = block_size;
+	MMCBUS_WAIT_FOR_REQUEST(device_get_parent(dev), dev,
+	    &req);
+	if (req.cmd->error != MMC_ERR_NONE) {
+		status_code = RTEMS_IO_ERROR;
+	}
+
+	return status_code;
+}
+
+static int
+rtems_bsd_sdda_disk_read_write(struct sdda_part *part, rtems_blkdev_request *blkreq)
+{
+
+	rtems_status_code status_code = RTEMS_SUCCESSFUL;
+	struct sdda_softc *sc = part->sc;
+	struct cam_periph *periph = &sc->periph;
+	struct mmc_params *mmcp = &periph->path->device->mmc_ident_data;
+	int shift = (mmcp->card_features & CARD_FEATURE_SDHC) ? 0 : 9;
+	int rca = get_rca(sc->periph);
+	uint32_t buffer_count = blkreq->bufnum;
+	uint32_t transfer_bytes = blkreq->bufs[0].length;
+	uint32_t block_count = transfer_bytes / MMC_SECTOR_SIZE;
+	uint32_t opcode;
+	uint32_t data_flags;
+	uint32_t i;
+	uint32_t prio = min(periph->scheduled_priority,periph->immediate_priority);
+
+	if (blkreq->req == RTEMS_BLKDEV_REQ_WRITE) {
+		if (block_count > 1) {
+			opcode = MMC_WRITE_MULTIPLE_BLOCK;
+		} else {
+			opcode = MMC_WRITE_BLOCK;
+		}
+
+		data_flags = MMC_DATA_WRITE;
+	} else {
+		BSD_ASSERT(blkreq->req == RTEMS_BLKDEV_REQ_READ);
+
+		if (block_count > 1) {
+			opcode = MMC_READ_MULTIPLE_BLOCK;
+		} else {
+			opcode = MMC_READ_SINGLE_BLOCK;
+		}
+
+		data_flags = MMC_DATA_READ;
+	}
+
+	for (i = 0; i < buffer_count; ++i) {
+		sg = &blkreq->bufs [i];
+		struct cam_periph *periph;
+		struct sdda_softc *softc;
+
+		softc = part->sc;
+		periph = softc->periph;
+
+		cam_periph_lock(periph);
+
+		CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sddastrategy(%p)\n", bp));
+
+		/*
+		* If the device has been made invalid, error out
+		*/
+		if ((periph->flags & CAM_PERIPH_INVALID) != 0) {
+		    cam_periph_unlock(periph);
+		    return;
+		}
+
+		/*
+		* Schedule ourselves for performing the work.
+		*/
+		sddaschedule(periph);
+		cam_periph_unlock(periph);
+
+			}
+		error:
+			rtems_blkdev_request_done(blkreq, status_code);
+			return 0;
+}
+
+static int
+rtems_bsd_sdda_disk_ioctl(rtems_disk_device *dd, uint32_t req, void *arg)
+{
+
+
+	if (req == RTEMS_BLKIO_REQUEST) {
+		struct sdda_part *part = rtems_disk_get_driver_data(dd);
+		rtems_blkdev_request *blkreq = arg;
+
+		return rtems_bsd_sdda_disk_read_write(part, blkreq);
+	} else if (req == RTEMS_BLKIO_CAPABILITIES) {
+		*(uint32_t *) arg = RTEMS_BLKDEV_CAP_MULTISECTOR_CONT;
+		return 0;
+	} else {
+		return rtems_blkdev_ioctl(dd, req, arg);
+	}
+
+}
+
+static rtems_status_code
+rtems_bsd_sdda_attach_worker(rtems_media_state state, const char *src, char **dest, void *arg)
+{
+
+	rtems_status_code status_code = RTEMS_SUCCESSFUL;
+	struct sdda_part *part = arg;
+	char *disk = NULL;
+
+	if (state == RTEMS_MEDIA_STATE_READY) {
+		struct sdda_softc *sc = part->sc;
+		uint32_t block_count = sc->sector_count;
+		uint32_t block_size = 512;
+		disk = rtems_media_create_path("/dev", src, sc->periph->unit_number);
+		if (disk == NULL) {
+			printf("OOPS: create path failed\n");
+			goto error;
+		}
+
+		status_code = rtems_blkdev_create(disk, block_size,
+		    block_count, rtems_bsd_sdda_disk_ioctl, part);
+		if (status_code != RTEMS_SUCCESSFUL) {
+			goto error;
+		}
+
+		*dest = strdup(disk, M_RTEMS_HEAP);
+	}
+
+	return RTEMS_SUCCESSFUL;
+error:
+	free(disk, M_RTEMS_HEAP);
+
+	return RTEMS_IO_ERROR;
+}
+
+void rtems_sddastart(struct cam_periph *periph, union ccb *start_ccb){
+	struct sdda_softc *softc = (struct sdda_softc *)periph->softc;
+	struct sdda_part *part;
+	struct mmc_params *mmcp = &periph->path->device->mmc_ident_data;
+	int part_index;
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sddastart\n"));
+
+	if (softc->state != SDDA_STATE_NORMAL) {
+		CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("device is not in SDDA_STATE_NORMAL yet\n"));
+		xpt_release_ccb(start_ccb);
+		return;
+	}
+
+	/* Find partition that has outstanding commands.  Prefer current partition. */
+	part = softc->part[softc->part_curr];
+
+	if (part_index != softc->part_curr) {
+		CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH,
+		    ("Partition  %d -> %d\n", softc->part_curr, part_index));
+		/*
+		 * According to section "6.2.2 Command restrictions" of the eMMC
+		 * specification v5.1, CMD19/CMD21 aren't allowed to be used with
+		 * RPMB partitions.  So we pause re-tuning along with triggering
+		 * it up-front to decrease the likelihood of re-tuning becoming
+		 * necessary while accessing an RPMB partition.  Consequently, an
+		 * RPMB partition should immediately be switched away from again
+		 * after an access in order to allow for re-tuning to take place
+		 * anew.
+		 */
+		/* TODO: pause retune if switching to RPMB partition */
+		softc->state = SDDA_STATE_PART_SWITCH;
+		sdda_init_switch_part(periph, start_ccb, part_index);
+		return;
+	}
+	//Replace with RTEMS alternative
+	switch (bp->cmd) {
+	case BIO_WRITE:
+		CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("BIO_WRITE\n"));
+		part->flags |= SDDA_FLAG_DIRTY;
+		/* FALLTHROUGH */
+	case BIO_READ:
+	{
+		struct ccb_mmcio *mmcio;
+		uint64_t blockno = sg->block;
+		uint16_t count = 1; //not sure, should i divide by 512?
+		uint16_t opcode = MMC_READ_SINGLE_BLOCK;
+
+		start_ccb->ccb_h.func_code = XPT_MMC_IO;
+		start_ccb->ccb_h.flags = CAM_DIR_IN;
+		start_ccb->ccb_h.retry_count = 0;
+		start_ccb->ccb_h.timeout = 15 * 1000;
+		start_ccb->ccb_h.cbfcnp = sddadone;
+
+		mmcio = &start_ccb->mmcio;
+		mmcio->cmd.opcode = opcode;
+		mmcio->cmd.arg = blockno;
+		if (!(mmcp->card_features & CARD_FEATURE_SDHC))
+			mmcio->cmd.arg <<= 9;
+
+		mmcio->cmd.flags = MMC_RSP_R1 | MMC_CMD_ADTC;
+		mmcio->cmd.data = softc->mmcdata;
+		mmcio->cmd.data->data = sg->buffer;
+		mmcio->cmd.data->len = sg->length;
+		mmcio->cmd.data->flags = MMC_DATA_READ;
+		/* Direct h/w to issue CMD12 upon completion */
+		if (count > 1) {
+			mmcio->cmd.data->flags |= MMC_DATA_MULTI;
+			mmcio->stop.opcode = MMC_STOP_TRANSMISSION;
+			mmcio->stop.flags = MMC_RSP_R1B | MMC_CMD_AC;
+			mmcio->stop.arg = 0;
+		}
+		break;
+	}
+	case BIO_FLUSH:
+		CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("BIO_FLUSH\n"));
+		sddaschedule(periph);
+		break;
+	case BIO_DELETE:
+		CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("BIO_DELETE\n"));
+		sddaschedule(periph);
+		break;
+	}
+	softc->outstanding_cmds++;
+	softc->refcount++;
+	cam_periph_unlock(periph);
+	xpt_action(start_ccb);
+	cam_periph_lock(periph);
+
+}
+#endif /* __rtems__ */
+
+static uint16_t
+get_rca(struct cam_periph *periph) {
+	return periph->path->device->mmc_ident_data.card_rca;
+}
+
+static uint32_t
+mmc_get_bits(uint32_t *bits, int bit_len, int start, int size)
+{
+	const int i = (bit_len / 32) - (start / 32) - 1;
+	const int shift = start & 31;
+	uint32_t retval = bits[i] >> shift;
+	if (size + shift > 32)
+		retval |= bits[i - 1] << (32 - shift);
+	return (retval & ((1llu << size) - 1));
+}
+
+
+static void
+mmc_decode_csd_sd(uint32_t *raw_csd, struct mmc_csd *csd)
+{
+	int v;
+	int m;
+	int e;
+
+	memset(csd, 0, sizeof(*csd));
+	csd->csd_structure = v = mmc_get_bits(raw_csd, 128, 126, 2);
+	if (v == 0) {
+		m = mmc_get_bits(raw_csd, 128, 115, 4);
+		e = mmc_get_bits(raw_csd, 128, 112, 3);
+		csd->tacc = (exp[e] * mant[m] + 9) / 10;
+		csd->nsac = mmc_get_bits(raw_csd, 128, 104, 8) * 100;
+		m = mmc_get_bits(raw_csd, 128, 99, 4);
+		e = mmc_get_bits(raw_csd, 128, 96, 3);
+		csd->tran_speed = exp[e] * 10000 * mant[m];
+		csd->ccc = mmc_get_bits(raw_csd, 128, 84, 12);
+		csd->read_bl_len = 1 << mmc_get_bits(raw_csd, 128, 80, 4);
+		csd->read_bl_partial = mmc_get_bits(raw_csd, 128, 79, 1);
+		csd->write_blk_misalign = mmc_get_bits(raw_csd, 128, 78, 1);
+		csd->read_blk_misalign = mmc_get_bits(raw_csd, 128, 77, 1);
+		csd->dsr_imp = mmc_get_bits(raw_csd, 128, 76, 1);
+		csd->vdd_r_curr_min = cur_min[mmc_get_bits(raw_csd, 128, 59, 3)];
+		csd->vdd_r_curr_max = cur_max[mmc_get_bits(raw_csd, 128, 56, 3)];
+		csd->vdd_w_curr_min = cur_min[mmc_get_bits(raw_csd, 128, 53, 3)];
+		csd->vdd_w_curr_max = cur_max[mmc_get_bits(raw_csd, 128, 50, 3)];
+		m = mmc_get_bits(raw_csd, 128, 62, 12);
+		e = mmc_get_bits(raw_csd, 128, 47, 3);
+		csd->capacity = ((1 + m) << (e + 2)) * csd->read_bl_len;
+		csd->erase_blk_en = mmc_get_bits(raw_csd, 128, 46, 1);
+		csd->erase_sector = mmc_get_bits(raw_csd, 128, 39, 7) + 1;
+		csd->wp_grp_size = mmc_get_bits(raw_csd, 128, 32, 7);
+		csd->wp_grp_enable = mmc_get_bits(raw_csd, 128, 31, 1);
+		csd->r2w_factor = 1 << mmc_get_bits(raw_csd, 128, 26, 3);
+		csd->write_bl_len = 1 << mmc_get_bits(raw_csd, 128, 22, 4);
+		csd->write_bl_partial = mmc_get_bits(raw_csd, 128, 21, 1);
+	} else if (v == 1) {
+		m = mmc_get_bits(raw_csd, 128, 115, 4);
+		e = mmc_get_bits(raw_csd, 128, 112, 3);
+		csd->tacc = (exp[e] * mant[m] + 9) / 10;
+		csd->nsac = mmc_get_bits(raw_csd, 128, 104, 8) * 100;
+		m = mmc_get_bits(raw_csd, 128, 99, 4);
+		e = mmc_get_bits(raw_csd, 128, 96, 3);
+		csd->tran_speed = exp[e] * 10000 * mant[m];
+		csd->ccc = mmc_get_bits(raw_csd, 128, 84, 12);
+		csd->read_bl_len = 1 << mmc_get_bits(raw_csd, 128, 80, 4);
+		csd->read_bl_partial = mmc_get_bits(raw_csd, 128, 79, 1);
+		csd->write_blk_misalign = mmc_get_bits(raw_csd, 128, 78, 1);
+		csd->read_blk_misalign = mmc_get_bits(raw_csd, 128, 77, 1);
+		csd->dsr_imp = mmc_get_bits(raw_csd, 128, 76, 1);
+		csd->capacity = ((uint64_t)mmc_get_bits(raw_csd, 128, 48, 22) + 1) *
+		    512 * 1024;
+		csd->erase_blk_en = mmc_get_bits(raw_csd, 128, 46, 1);
+		csd->erase_sector = mmc_get_bits(raw_csd, 128, 39, 7) + 1;
+		csd->wp_grp_size = mmc_get_bits(raw_csd, 128, 32, 7);
+		csd->wp_grp_enable = mmc_get_bits(raw_csd, 128, 31, 1);
+		csd->r2w_factor = 1 << mmc_get_bits(raw_csd, 128, 26, 3);
+		csd->write_bl_len = 1 << mmc_get_bits(raw_csd, 128, 22, 4);
+		csd->write_bl_partial = mmc_get_bits(raw_csd, 128, 21, 1);
+	} else
+		panic("unknown SD CSD version");
+}
+
+static void
+mmc_decode_csd_mmc(uint32_t *raw_csd, struct mmc_csd *csd)
+{
+	int m;
+	int e;
+
+	memset(csd, 0, sizeof(*csd));
+	csd->csd_structure = mmc_get_bits(raw_csd, 128, 126, 2);
+	csd->spec_vers = mmc_get_bits(raw_csd, 128, 122, 4);
+	m = mmc_get_bits(raw_csd, 128, 115, 4);
+	e = mmc_get_bits(raw_csd, 128, 112, 3);
+	csd->tacc = exp[e] * mant[m] + 9 / 10;
+	csd->nsac = mmc_get_bits(raw_csd, 128, 104, 8) * 100;
+	m = mmc_get_bits(raw_csd, 128, 99, 4);
+	e = mmc_get_bits(raw_csd, 128, 96, 3);
+	csd->tran_speed = exp[e] * 10000 * mant[m];
+	csd->ccc = mmc_get_bits(raw_csd, 128, 84, 12);
+	csd->read_bl_len = 1 << mmc_get_bits(raw_csd, 128, 80, 4);
+	csd->read_bl_partial = mmc_get_bits(raw_csd, 128, 79, 1);
+	csd->write_blk_misalign = mmc_get_bits(raw_csd, 128, 78, 1);
+	csd->read_blk_misalign = mmc_get_bits(raw_csd, 128, 77, 1);
+	csd->dsr_imp = mmc_get_bits(raw_csd, 128, 76, 1);
+	csd->vdd_r_curr_min = cur_min[mmc_get_bits(raw_csd, 128, 59, 3)];
+	csd->vdd_r_curr_max = cur_max[mmc_get_bits(raw_csd, 128, 56, 3)];
+	csd->vdd_w_curr_min = cur_min[mmc_get_bits(raw_csd, 128, 53, 3)];
+	csd->vdd_w_curr_max = cur_max[mmc_get_bits(raw_csd, 128, 50, 3)];
+	m = mmc_get_bits(raw_csd, 128, 62, 12);
+	e = mmc_get_bits(raw_csd, 128, 47, 3);
+	csd->capacity = ((1 + m) << (e + 2)) * csd->read_bl_len;
+	csd->erase_blk_en = 0;
+	csd->erase_sector = (mmc_get_bits(raw_csd, 128, 42, 5) + 1) *
+	    (mmc_get_bits(raw_csd, 128, 37, 5) + 1);
+	csd->wp_grp_size = mmc_get_bits(raw_csd, 128, 32, 5);
+	csd->wp_grp_enable = mmc_get_bits(raw_csd, 128, 31, 1);
+	csd->r2w_factor = 1 << mmc_get_bits(raw_csd, 128, 26, 3);
+	csd->write_bl_len = 1 << mmc_get_bits(raw_csd, 128, 22, 4);
+	csd->write_bl_partial = mmc_get_bits(raw_csd, 128, 21, 1);
+}
+
+static void
+mmc_decode_cid_sd(uint32_t *raw_cid, struct mmc_cid *cid)
+{
+	int i;
+
+	/* There's no version info, so we take it on faith */
+	memset(cid, 0, sizeof(*cid));
+	cid->mid = mmc_get_bits(raw_cid, 128, 120, 8);
+	cid->oid = mmc_get_bits(raw_cid, 128, 104, 16);
+	for (i = 0; i < 5; i++)
+		cid->pnm[i] = mmc_get_bits(raw_cid, 128, 96 - i * 8, 8);
+	cid->pnm[5] = 0;
+	cid->prv = mmc_get_bits(raw_cid, 128, 56, 8);
+	cid->psn = mmc_get_bits(raw_cid, 128, 24, 32);
+	cid->mdt_year = mmc_get_bits(raw_cid, 128, 12, 8) + 2000;
+	cid->mdt_month = mmc_get_bits(raw_cid, 128, 8, 4);
+}
+
+static void
+mmc_decode_cid_mmc(uint32_t *raw_cid, struct mmc_cid *cid)
+{
+	int i;
+
+	/* There's no version info, so we take it on faith */
+	memset(cid, 0, sizeof(*cid));
+	cid->mid = mmc_get_bits(raw_cid, 128, 120, 8);
+	cid->oid = mmc_get_bits(raw_cid, 128, 104, 8);
+	for (i = 0; i < 6; i++)
+		cid->pnm[i] = mmc_get_bits(raw_cid, 128, 96 - i * 8, 8);
+	cid->pnm[6] = 0;
+	cid->prv = mmc_get_bits(raw_cid, 128, 48, 8);
+	cid->psn = mmc_get_bits(raw_cid, 128, 16, 32);
+	cid->mdt_month = mmc_get_bits(raw_cid, 128, 12, 4);
+	cid->mdt_year = mmc_get_bits(raw_cid, 128, 8, 4) + 1997;
+}
+
+static void
+mmc_format_card_id_string(struct sdda_softc *sc, struct mmc_params *mmcp)
+{
+	char oidstr[8];
+	uint8_t c1;
+	uint8_t c2;
+
+	/*
+	 * Format a card ID string for use by the mmcsd driver, it's what
+	 * appears between the <> in the following:
+	 * mmcsd0: 968MB <SD SD01G 8.0 SN 2686905 Mfg 08/2008 by 3 TN> at mmc0
+	 * 22.5MHz/4bit/128-block
+	 *
+	 * Also format just the card serial number, which the mmcsd driver will
+	 * use as the disk->d_ident string.
+	 *
+	 * The card_id_string in mmc_ivars is currently allocated as 64 bytes,
+	 * and our max formatted length is currently 55 bytes if every field
+	 * contains the largest value.
+	 *
+	 * Sometimes the oid is two printable ascii chars; when it's not,
+	 * format it as 0xnnnn instead.
+	 */
+	c1 = (sc->cid.oid >> 8) & 0x0ff;
+	c2 = sc->cid.oid & 0x0ff;
+	if (c1 > 0x1f && c1 < 0x7f && c2 > 0x1f && c2 < 0x7f)
+		snprintf(oidstr, sizeof(oidstr), "%c%c", c1, c2);
+	else
+		snprintf(oidstr, sizeof(oidstr), "0x%04x", sc->cid.oid);
+	snprintf(sc->card_sn_string, sizeof(sc->card_sn_string),
+	    "%08X", sc->cid.psn);
+	snprintf(sc->card_id_string, sizeof(sc->card_id_string),
+                 "%s%s %s %d.%d SN %08X MFG %02d/%04d by %d %s",
+                 mmcp->card_features & CARD_FEATURE_MMC ? "MMC" : "SD",
+                 mmcp->card_features & CARD_FEATURE_SDHC ? "HC" : "",
+                 sc->cid.pnm, sc->cid.prv >> 4, sc->cid.prv & 0x0f,
+                 sc->cid.psn, sc->cid.mdt_month, sc->cid.mdt_year,
+                 sc->cid.mid, oidstr);
+}
+
+#ifndef __rtems__
+static int
+sddaopen(struct disk *dp)
+{
+	struct sdda_part *part;
+	struct cam_periph *periph;
+	struct sdda_softc *softc;
+	int error;
+
+	part = (struct sdda_part *)dp->d_drv1;
+	softc = part->sc;
+	periph = softc->periph;
+	if (cam_periph_acquire(periph) != CAM_REQ_CMP) {
+		return(ENXIO);
+	}
+
+	cam_periph_lock(periph);
+	if ((error = cam_periph_hold(periph, PRIBIO|PCATCH)) != 0) {
+		cam_periph_unlock(periph);
+		cam_periph_release(periph);
+		return (error);
+	}
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sddaopen\n"));
+
+	part->flags |= SDDA_FLAG_OPEN;
+
+	cam_periph_unhold(periph);
+	cam_periph_unlock(periph);
+	return (0);
+}
+
+static int
+sddaclose(struct disk *dp)
+{
+	struct sdda_part *part;
+	struct	cam_periph *periph;
+	struct	sdda_softc *softc;
+
+	part = (struct sdda_part *)dp->d_drv1;
+	softc = part->sc;
+	periph = softc->periph;
+	part->flags &= ~SDDA_FLAG_OPEN;
+
+	cam_periph_lock(periph);
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sddaclose\n"));
+
+	while (softc->refcount != 0)
+		cam_periph_sleep(periph, &softc->refcount, PRIBIO, "sddaclose", 1);
+	cam_periph_unlock(periph);
+	cam_periph_release(periph);
+	return (0);
+}
+#endif /* __rtems__ */
+
+void
+sddaschedule(struct cam_periph *periph)
+{
+	struct sdda_softc *softc = (struct sdda_softc *)periph->softc;
+	struct sdda_part *part;
+#ifndef __rtems__
+	struct bio *bp;
+	int i;
+
+	/* Check if we have more work to do. */
+	/* Find partition that has outstanding commands. Prefer current partition. */
+	bp = bioq_first(&softc->part[softc->part_curr]->bio_queue);
+	if (bp == NULL) {
+		for (i = 0; i < MMC_PART_MAX; i++) {
+			if ((part = softc->part[i]) != NULL &&
+			    (bp = bioq_first(&softc->part[i]->bio_queue)) != NULL)
+				break;
+		}
+	}
+	if (bp != NULL) {
+		xpt_schedule(periph, CAM_PRIORITY_NORMAL);
+	}
+#else /* __rtems__ */
+    xpt_schedule(periph, CAM_PRIORITY_NORMAL);
+#endif /* __rtems__ */
+}
+
+/*
+ * Actually translate the requested transfer into one the physical driver
+ * can understand.  The transfer is described by a buf and will include
+ * only one physical transfer.
+ */
+#ifndef __rtems__
+static void
+sddastrategy(struct bio *bp)
+{
+	struct cam_periph *periph;
+	struct sdda_part *part;
+	struct sdda_softc *softc;
+
+	part = (struct sdda_part *)bp->bio_disk->d_drv1;
+	softc = part->sc;
+	periph = softc->periph;
+
+	cam_periph_lock(periph);
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sddastrategy(%p)\n", bp));
+
+	/*
+	 * If the device has been made invalid, error out
+	 */
+	if ((periph->flags & CAM_PERIPH_INVALID) != 0) {
+		cam_periph_unlock(periph);
+		biofinish(bp, NULL, ENXIO);
+		return;
+	}
+
+	/*
+	 * Place it in the queue of disk activities for this disk
+	 */
+	bioq_disksort(&part->bio_queue, bp);
+
+	/*
+	 * Schedule ourselves for performing the work.
+	 */
+	sddaschedule(periph);
+	cam_periph_unlock(periph);
+
+	return;
+}
+#endif
+void
+sddainit(void)
+{
+	cam_status status;
+
+	/*
+	 * Install a global async callback.  This callback will
+	 * receive async callbacks like "new device found".
+	 */
+	status = xpt_register_async(AC_FOUND_DEVICE, sddaasync, NULL, NULL);
+
+	if (status != CAM_REQ_CMP) {
+		printf("sdda: Failed to attach master async callback "
+		       "due to status 0x%x!\n", status);
+	}
+}
+
+/*
+ * Callback from GEOM, called when it has finished cleaning up its
+ * resources.
+ */
+#ifndef __rtems__
+static void
+sddadiskgonecb(struct disk *dp)
+{
+	struct cam_periph *periph;
+	struct sdda_part *part;
+
+	part = (struct sdda_part *)dp->d_drv1;
+	periph = part->sc->periph;
+        CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sddadiskgonecb\n"));
+
+	cam_periph_release(periph);
+}
+#endif
+static void
+sddaoninvalidate(struct cam_periph *periph)
+{
+	struct sdda_softc *softc;
+	struct sdda_part *part;
+
+	softc = (struct sdda_softc *)periph->softc;
+
+        CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sddaoninvalidate\n"));
+
+	/*
+	 * De-register any async callbacks.
+	 */
+	xpt_register_async(0, sddaasync, periph, periph->path);
+
+	/*
+	 * Return all queued I/O with ENXIO.
+	 * XXX Handle any transactions queued to the card
+	 *     with XPT_ABORT_CCB.
+	 */
+        CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("bioq_flush start\n"));
+#ifndef __rtems__
+	for (int i = 0; i < MMC_PART_MAX; i++) {
+		if ((part = softc->part[i]) != NULL) {
+			bioq_flush(&part->bio_queue, NULL, ENXIO);
+			disk_gone(part->disk);
+		}
+	}
+#endif
+        CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("bioq_flush end\n"));
+
+}
+
+static void
+sddacleanup(struct cam_periph *periph)
+{
+	struct sdda_softc *softc;
+	struct sdda_part *part;
+	int i;
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sddacleanup\n"));
+	softc = (struct sdda_softc *)periph->softc;
+
+	cam_periph_unlock(periph);
+
+	for (i = 0; i < MMC_PART_MAX; i++) {
+		if ((part = softc->part[i]) != NULL) {
+/* TODO: Add RTEMS speciifc disk cleanup routine */
+#ifndef __rtems__
+			disk_destroy(part->disk);
+#endif
+			free(part, M_DEVBUF);
+			softc->part[i] = NULL;
+		}
+	}
+	free(softc, M_DEVBUF);
+	cam_periph_lock(periph);
+}
+
+static void
+sddaasync(void *callback_arg, u_int32_t code,
+	struct cam_path *path, void *arg)
+{
+	struct ccb_getdev cgd;
+	struct cam_periph *periph;
+	struct sdda_softc *softc;
+
+	periph = (struct cam_periph *)callback_arg;
+        CAM_DEBUG(path, CAM_DEBUG_TRACE, ("sddaasync(code=%d)\n", code));
+	switch (code) {
+	case AC_FOUND_DEVICE:
+	{
+                CAM_DEBUG(path, CAM_DEBUG_TRACE, ("=> AC_FOUND_DEVICE\n"));
+		struct ccb_getdev *cgd;
+		cam_status status;
+
+		cgd = (struct ccb_getdev *)arg;
+		if (cgd == NULL)
+			break;
+
+		if (cgd->protocol != PROTO_MMCSD)
+			break;
+
+                if (!(path->device->mmc_ident_data.card_features & CARD_FEATURE_MEMORY)) {
+                        CAM_DEBUG(path, CAM_DEBUG_TRACE, ("No memory on the card!\n"));
+                        break;
+                }
+
+		/*
+		 * Allocate a peripheral instance for
+		 * this device and start the probe
+		 * process.
+		 */
+		status = cam_periph_alloc(sddaregister, sddaoninvalidate,
+					  sddacleanup, sddastart,
+					  "sdda", CAM_PERIPH_BIO,
+					  path, sddaasync,
+					  AC_FOUND_DEVICE, cgd);
+
+		if (status != CAM_REQ_CMP
+		 && status != CAM_REQ_INPROG)
+			printf("sddaasync: Unable to attach to new device "
+				"due to status 0x%x\n", status);
+		break;
+	}
+	case AC_GETDEV_CHANGED:
+	{
+		CAM_DEBUG(path, CAM_DEBUG_TRACE, ("=> AC_GETDEV_CHANGED\n"));
+		softc = (struct sdda_softc *)periph->softc;
+		xpt_setup_ccb(&cgd.ccb_h, periph->path, CAM_PRIORITY_NORMAL);
+		cgd.ccb_h.func_code = XPT_GDEV_TYPE;
+		xpt_action((union ccb *)&cgd);
+		cam_periph_async(periph, code, path, arg);
+		break;
+	}
+	case AC_ADVINFO_CHANGED:
+	{
+		uintptr_t buftype;
+		int i;
+
+		CAM_DEBUG(path, CAM_DEBUG_TRACE, ("=> AC_ADVINFO_CHANGED\n"));
+		buftype = (uintptr_t)arg;
+		if (buftype == CDAI_TYPE_PHYS_PATH) {
+			struct sdda_softc *softc;
+			struct sdda_part *part;
+
+			softc = periph->softc;
+#ifndef __rtems__
+			for (i = 0; i < MMC_PART_MAX; i++) {
+				if ((part = softc->part[i]) != NULL) {
+					disk_attr_changed(part->disk, "GEOM::physpath",
+					    M_NOWAIT);
+				}
+			}
+#endif
+		}
+		break;
+	}
+	default:
+		CAM_DEBUG(path, CAM_DEBUG_TRACE, ("=> default?!\n"));
+		cam_periph_async(periph, code, path, arg);
+		break;
+	}
+}
+
+#ifndef __rtems__
+static int
+sddagetattr(struct bio *bp)
+{
+	struct cam_periph *periph;
+	struct sdda_softc *softc;
+	struct sdda_part *part;
+	int ret;
+
+	part = (struct sdda_part *)bp->bio_disk->d_drv1;
+	softc = part->sc;
+	periph = softc->periph;
+	cam_periph_lock(periph);
+	ret = xpt_getattr(bp->bio_data, bp->bio_length, bp->bio_attribute,
+	    periph->path);
+	cam_periph_unlock(periph);
+	if (ret == 0)
+		bp->bio_completed = bp->bio_length;
+	return (ret);
+}
+#endif /* __rtems__ */
+
+static cam_status
+sddaregister(struct cam_periph *periph, void *arg)
+{
+	struct sdda_softc *softc;
+	struct ccb_getdev *cgd;
+	union ccb *request_ccb;	/* CCB representing the probe request */
+
+        CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sddaregister\n"));
+	cgd = (struct ccb_getdev *)arg;
+	if (cgd == NULL) {
+		printf("sddaregister: no getdev CCB, can't register device\n");
+		return (CAM_REQ_CMP_ERR);
+	}
+
+	softc = (struct sdda_softc *)malloc(sizeof(*softc), M_DEVBUF,
+	    M_NOWAIT|M_ZERO);
+
+	if (softc == NULL) {
+		printf("sddaregister: Unable to probe new device. "
+		    "Unable to allocate softc\n");
+		return (CAM_REQ_CMP_ERR);
+	}
+
+	softc->state = SDDA_STATE_INIT;
+	softc->mmcdata =
+		(struct mmc_data *)malloc(sizeof(struct mmc_data), M_DEVBUF, M_NOWAIT|M_ZERO);
+	periph->softc = softc;
+	softc->periph = periph;
+
+	request_ccb = (union ccb*) arg;
+	xpt_schedule(periph, CAM_PRIORITY_XPT);
+	TASK_INIT(&softc->start_init_task, 0, sdda_start_init_task, periph);
+	taskqueue_enqueue(taskqueue_thread, &softc->start_init_task);
+
+	return (CAM_REQ_CMP);
+}
+
+static int
+mmc_exec_app_cmd(struct cam_periph *periph, union ccb *ccb,
+	struct mmc_command *cmd) {
+	int err;
+
+	/* Send APP_CMD first */
+	memset(&ccb->mmcio.cmd, 0, sizeof(struct mmc_command));
+	memset(&ccb->mmcio.stop, 0, sizeof(struct mmc_command));
+	cam_fill_mmcio(&ccb->mmcio,
+		       /*retries*/ 0,
+		       /*cbfcnp*/ NULL,
+		       /*flags*/ CAM_DIR_NONE,
+		       /*mmc_opcode*/ MMC_APP_CMD,
+		       /*mmc_arg*/ get_rca(periph) << 16,
+		       /*mmc_flags*/ MMC_RSP_R1 | MMC_CMD_AC,
+		       /*mmc_data*/ NULL,
+		       /*timeout*/ 0);
+
+	err = cam_periph_runccb(ccb, sddaerror, CAM_FLAG_NONE, /*sense_flags*/0, NULL);
+	if (err != 0)
+		return err;
+	if (!(ccb->mmcio.cmd.resp[0] & R1_APP_CMD))
+		return MMC_ERR_FAILED;
+
+	/* Now exec actual command */
+	int flags = 0;
+	if (cmd->data != NULL) {
+		ccb->mmcio.cmd.data = cmd->data;
+		if (cmd->data->flags & MMC_DATA_READ)
+			flags |= CAM_DIR_IN;
+		if (cmd->data->flags & MMC_DATA_WRITE)
+			flags |= CAM_DIR_OUT;
+	} else flags = CAM_DIR_NONE;
+
+	cam_fill_mmcio(&ccb->mmcio,
+		       /*retries*/ 0,
+		       /*cbfcnp*/ NULL,
+		       /*flags*/ flags,
+		       /*mmc_opcode*/ cmd->opcode,
+		       /*mmc_arg*/ cmd->arg,
+		       /*mmc_flags*/ cmd->flags,
+		       /*mmc_data*/ cmd->data,
+		       /*timeout*/ 0);
+
+	err = cam_periph_runccb(ccb, sddaerror, CAM_FLAG_NONE, /*sense_flags*/0, NULL);
+	memcpy(cmd->resp, ccb->mmcio.cmd.resp, sizeof(cmd->resp));
+	cmd->error = ccb->mmcio.cmd.error;
+	if (err != 0)
+		return err;
+	return 0;
+}
+
+static int
+mmc_app_get_scr(struct cam_periph *periph, union ccb *ccb, uint32_t *rawscr) {
+	int err;
+	struct mmc_command cmd;
+	struct mmc_data d;
+
+	memset(&cmd, 0, sizeof(cmd));
+	memset(&d, 0, sizeof(d));
+
+	memset(rawscr, 0, 8);
+	cmd.opcode = ACMD_SEND_SCR;
+	cmd.flags = MMC_RSP_R1 | MMC_CMD_ADTC;
+	cmd.arg = 0;
+
+	d.data = rawscr;
+	d.len = 8;
+	d.flags = MMC_DATA_READ;
+	cmd.data = &d;
+
+	err = mmc_exec_app_cmd(periph, ccb, &cmd);
+	rawscr[0] = be32toh(rawscr[0]);
+	rawscr[1] = be32toh(rawscr[1]);
+	return (err);
+}
+
+static int
+mmc_send_ext_csd(struct cam_periph *periph, union ccb *ccb,
+		 uint8_t *rawextcsd, size_t buf_len) {
+	int err;
+	struct mmc_data d;
+
+	KASSERT(buf_len == 512, ("Buffer for ext csd must be 512 bytes"));
+	d.data = rawextcsd;
+	d.len = buf_len;
+	d.flags = MMC_DATA_READ;
+	memset(d.data, 0, d.len);
+
+	cam_fill_mmcio(&ccb->mmcio,
+		       /*retries*/ 0,
+		       /*cbfcnp*/ NULL,
+		       /*flags*/ CAM_DIR_IN,
+		       /*mmc_opcode*/ MMC_SEND_EXT_CSD,
+		       /*mmc_arg*/ 0,
+		       /*mmc_flags*/ MMC_RSP_R1 | MMC_CMD_ADTC,
+		       /*mmc_data*/ &d,
+		       /*timeout*/ 0);
+
+	err = cam_periph_runccb(ccb, sddaerror, CAM_FLAG_NONE, /*sense_flags*/0, NULL);
+	if (err != 0)
+		return (err);
+	return (MMC_ERR_NONE);
+}
+
+static void
+mmc_app_decode_scr(uint32_t *raw_scr, struct mmc_scr *scr)
+{
+	unsigned int scr_struct;
+
+	memset(scr, 0, sizeof(*scr));
+
+	scr_struct = mmc_get_bits(raw_scr, 64, 60, 4);
+	if (scr_struct != 0) {
+		printf("Unrecognised SCR structure version %d\n",
+		    scr_struct);
+		return;
+	}
+	scr->sda_vsn = mmc_get_bits(raw_scr, 64, 56, 4);
+	scr->bus_widths = mmc_get_bits(raw_scr, 64, 48, 4);
+}
+
+static inline void
+mmc_switch_fill_mmcio(union ccb *ccb,
+    uint8_t set, uint8_t index, uint8_t value, u_int timeout)
+{
+	int arg = (MMC_SWITCH_FUNC_WR << 24) |
+	    (index << 16) |
+	    (value << 8) |
+	    set;
+
+	cam_fill_mmcio(&ccb->mmcio,
+		       /*retries*/ 0,
+		       /*cbfcnp*/ NULL,
+		       /*flags*/ CAM_DIR_NONE,
+		       /*mmc_opcode*/ MMC_SWITCH_FUNC,
+		       /*mmc_arg*/ arg,
+		       /*mmc_flags*/ MMC_RSP_R1B | MMC_CMD_AC,
+		       /*mmc_data*/ NULL,
+		       /*timeout*/ timeout);
+}
+
+static int
+mmc_select_card(struct cam_periph *periph, union ccb *ccb, uint32_t rca)
+{
+	int flags;
+
+	flags = (rca ? MMC_RSP_R1B : MMC_RSP_NONE) | MMC_CMD_AC;
+	cam_fill_mmcio(&ccb->mmcio,
+		       /*retries*/ 0,
+		       /*cbfcnp*/ NULL,
+		       /*flags*/ CAM_DIR_IN,
+		       /*mmc_opcode*/ MMC_SELECT_CARD,
+		       /*mmc_arg*/ rca << 16,
+		       /*mmc_flags*/ flags,
+		       /*mmc_data*/ NULL,
+		       /*timeout*/ 0);
+
+	cam_periph_runccb(ccb, sddaerror, CAM_FLAG_NONE, /*sense_flags*/0, NULL);
+
+	if (((ccb->ccb_h.status & CAM_STATUS_MASK) == CAM_REQ_CMP)) {
+		if (ccb->mmcio.cmd.error != 0) {
+			CAM_DEBUG(ccb->ccb_h.path, CAM_DEBUG_PERIPH,
+				  ("%s: MMC_SELECT command failed", __func__));
+			return EIO;
+		}
+		return 0; /* Normal return */
+	} else {
+		CAM_DEBUG(ccb->ccb_h.path, CAM_DEBUG_PERIPH,
+			  ("%s: CAM request failed\n", __func__));
+		return EIO;
+	}
+}
+
+static int
+mmc_switch(struct cam_periph *periph, union ccb *ccb,
+    uint8_t set, uint8_t index, uint8_t value, u_int timeout)
+{
+
+	mmc_switch_fill_mmcio(ccb, set, index, value, timeout);
+	cam_periph_runccb(ccb, sddaerror, CAM_FLAG_NONE, /*sense_flags*/0, NULL);
+
+	if (((ccb->ccb_h.status & CAM_STATUS_MASK) == CAM_REQ_CMP)) {
+		if (ccb->mmcio.cmd.error != 0) {
+			CAM_DEBUG(ccb->ccb_h.path, CAM_DEBUG_PERIPH,
+				  ("%s: MMC command failed", __func__));
+			return (EIO);
+		}
+		return (0); /* Normal return */
+	} else {
+		CAM_DEBUG(ccb->ccb_h.path, CAM_DEBUG_PERIPH,
+			  ("%s: CAM request failed\n", __func__));
+		return (EIO);
+	}
+
+}
+
+static uint32_t
+mmc_get_spec_vers(struct cam_periph *periph) {
+	struct sdda_softc *softc = (struct sdda_softc *)periph->softc;
+
+	return (softc->csd.spec_vers);
+}
+
+static uint64_t
+mmc_get_media_size(struct cam_periph *periph) {
+	struct sdda_softc *softc = (struct sdda_softc *)periph->softc;
+
+	return (softc->mediasize);
+}
+
+static uint32_t
+mmc_get_cmd6_timeout(struct cam_periph *periph)
+{
+	struct sdda_softc *softc = (struct sdda_softc *)periph->softc;
+
+	if (mmc_get_spec_vers(periph) >= 6)
+		return (softc->raw_ext_csd[EXT_CSD_GEN_CMD6_TIME] * 10);
+	return (500 * 1000);
+}
+
+static int
+mmc_sd_switch(struct cam_periph *periph, union ccb *ccb,
+	      uint8_t mode, uint8_t grp, uint8_t value,
+	      uint8_t *res) {
+
+	struct mmc_data mmc_d;
+	uint32_t arg;
+
+	memset(res, 0, 64);
+	mmc_d.len = 64;
+	mmc_d.data = res;
+	mmc_d.flags = MMC_DATA_READ;
+
+	arg = mode << 31;			/* 0 - check, 1 - set */
+	arg |= 0x00FFFFFF;
+	arg &= ~(0xF << (grp * 4));
+	arg |= value << (grp * 4);
+
+	cam_fill_mmcio(&ccb->mmcio,
+		       /*retries*/ 0,
+		       /*cbfcnp*/ NULL,
+		       /*flags*/ CAM_DIR_IN,
+		       /*mmc_opcode*/ SD_SWITCH_FUNC,
+		       /*mmc_arg*/ arg,
+		       /*mmc_flags*/ MMC_RSP_R1 | MMC_CMD_ADTC,
+		       /*mmc_data*/ &mmc_d,
+		       /*timeout*/ 0);
+
+	cam_periph_runccb(ccb, sddaerror, CAM_FLAG_NONE, /*sense_flags*/0, NULL);
+
+	if (((ccb->ccb_h.status & CAM_STATUS_MASK) == CAM_REQ_CMP)) {
+		if (ccb->mmcio.cmd.error != 0) {
+			CAM_DEBUG(ccb->ccb_h.path, CAM_DEBUG_PERIPH,
+				  ("%s: MMC command failed", __func__));
+			return EIO;
+		}
+		return 0; /* Normal return */
+	} else {
+		CAM_DEBUG(ccb->ccb_h.path, CAM_DEBUG_PERIPH,
+			  ("%s: CAM request failed\n", __func__));
+		return EIO;
+	}
+}
+
+static int
+mmc_set_timing(struct cam_periph *periph,
+	       union ccb *ccb,
+	       enum mmc_bus_timing timing)
+{
+	u_char switch_res[64];
+	int err;
+	uint8_t	value;
+	struct sdda_softc *softc = (struct sdda_softc *)periph->softc;
+	struct mmc_params *mmcp = &periph->path->device->mmc_ident_data;
+
+	CAM_DEBUG(ccb->ccb_h.path, CAM_DEBUG_TRACE,
+		  ("mmc_set_timing(timing=%d)", timing));
+	switch (timing) {
+	case bus_timing_normal:
+		value = 0;
+		break;
+	case bus_timing_hs:
+		value = 1;
+		break;
+	default:
+		return (MMC_ERR_INVALID);
+	}
+	if (mmcp->card_features & CARD_FEATURE_MMC) {
+		err = mmc_switch(periph, ccb, EXT_CSD_CMD_SET_NORMAL,
+		    EXT_CSD_HS_TIMING, value, softc->cmd6_time);
+	} else {
+		err = mmc_sd_switch(periph, ccb, SD_SWITCH_MODE_SET, SD_SWITCH_GROUP1, value, switch_res);
+	}
+
+	/* Set high-speed timing on the host */
+	struct ccb_trans_settings_mmc *cts;
+	cts = &ccb->cts.proto_specific.mmc;
+	ccb->ccb_h.func_code = XPT_SET_TRAN_SETTINGS;
+	ccb->ccb_h.flags = CAM_DIR_NONE;
+	ccb->ccb_h.retry_count = 0;
+	ccb->ccb_h.timeout = 100;
+	ccb->ccb_h.cbfcnp = NULL;
+	cts->ios.timing = timing;
+	cts->ios_valid = MMC_BT;
+	xpt_action(ccb);
+
+	return (err);
+}
+
+static void
+sdda_start_init_task(void *context, int pending) {
+	union ccb *new_ccb;
+	struct cam_periph *periph;
+
+	periph = (struct cam_periph *)context;
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sdda_start_init_task\n"));
+	new_ccb = xpt_alloc_ccb();
+	xpt_setup_ccb(&new_ccb->ccb_h, periph->path,
+		      CAM_PRIORITY_NONE);
+
+	cam_periph_lock(periph);
+	sdda_start_init(context, new_ccb);
+	cam_periph_unlock(periph);
+	xpt_free_ccb(new_ccb);
+}
+
+static void
+sdda_set_bus_width(struct cam_periph *periph, union ccb *ccb, int width) {
+	struct sdda_softc *softc = (struct sdda_softc *)periph->softc;
+	struct mmc_params *mmcp = &periph->path->device->mmc_ident_data;
+	int err;
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sdda_set_bus_width\n"));
+
+	/* First set for the card, then for the host */
+	if (mmcp->card_features & CARD_FEATURE_MMC) {
+		uint8_t	value;
+		switch (width) {
+		case bus_width_1:
+			value = EXT_CSD_BUS_WIDTH_1;
+			break;
+		case bus_width_4:
+			value = EXT_CSD_BUS_WIDTH_4;
+			break;
+		case bus_width_8:
+			value = EXT_CSD_BUS_WIDTH_8;
+			break;
+		default:
+			panic("Invalid bus width %d", width);
+		}
+		err = mmc_switch(periph, ccb, EXT_CSD_CMD_SET_NORMAL,
+		    EXT_CSD_BUS_WIDTH, value, softc->cmd6_time);
+	} else {
+		/* For SD cards we send ACMD6 with the required bus width in arg */
+		struct mmc_command cmd;
+		memset(&cmd, 0, sizeof(struct mmc_command));
+		cmd.opcode = ACMD_SET_BUS_WIDTH;
+		cmd.arg = width;
+		cmd.flags = MMC_RSP_R1 | MMC_CMD_AC;
+		err = mmc_exec_app_cmd(periph, ccb, &cmd);
+	}
+
+	if (err != MMC_ERR_NONE) {
+		CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH, ("Error %d when setting bus width on the card\n", err));
+		return;
+	}
+	/* Now card is done, set the host to the same width */
+	struct ccb_trans_settings_mmc *cts;
+	cts = &ccb->cts.proto_specific.mmc;
+	ccb->ccb_h.func_code = XPT_SET_TRAN_SETTINGS;
+	ccb->ccb_h.flags = CAM_DIR_NONE;
+	ccb->ccb_h.retry_count = 0;
+	ccb->ccb_h.timeout = 100;
+	ccb->ccb_h.cbfcnp = NULL;
+	cts->ios.bus_width = width;
+	cts->ios_valid = MMC_BW;
+	xpt_action(ccb);
+}
+
+static inline const char
+*part_type(u_int type)
+{
+
+	switch (type) {
+	case EXT_CSD_PART_CONFIG_ACC_RPMB:
+		return ("RPMB");
+	case EXT_CSD_PART_CONFIG_ACC_DEFAULT:
+		return ("default");
+	case EXT_CSD_PART_CONFIG_ACC_BOOT0:
+		return ("boot0");
+	case EXT_CSD_PART_CONFIG_ACC_BOOT1:
+		return ("boot1");
+	case EXT_CSD_PART_CONFIG_ACC_GP0:
+	case EXT_CSD_PART_CONFIG_ACC_GP1:
+	case EXT_CSD_PART_CONFIG_ACC_GP2:
+	case EXT_CSD_PART_CONFIG_ACC_GP3:
+		return ("general purpose");
+	default:
+		return ("(unknown type)");
+	}
+}
+
+static inline const char
+*bus_width_str(enum mmc_bus_width w)
+{
+
+	switch (w) {
+	case bus_width_1:
+		return ("1-bit");
+	case bus_width_4:
+		return ("4-bit");
+	case bus_width_8:
+		return ("8-bit");
+	}
+}
+
+static uint32_t
+sdda_get_host_caps(struct cam_periph *periph, union ccb *ccb)
+{
+	struct ccb_trans_settings_mmc *cts;
+
+	cts = &ccb->cts.proto_specific.mmc;
+
+	ccb->ccb_h.func_code = XPT_GET_TRAN_SETTINGS;
+	ccb->ccb_h.flags = CAM_DIR_NONE;
+	ccb->ccb_h.retry_count = 0;
+	ccb->ccb_h.timeout = 100;
+	ccb->ccb_h.cbfcnp = NULL;
+	xpt_action(ccb);
+
+	if (ccb->ccb_h.status != CAM_REQ_CMP)
+		panic("Cannot get host caps");
+	return (cts->host_caps);
+}
+
+static void
+sdda_start_init(void *context, union ccb *start_ccb)
+{
+	struct cam_periph *periph = (struct cam_periph *)context;
+	struct ccb_trans_settings_mmc *cts;
+	uint32_t host_caps;
+	uint32_t sec_count;
+	int err;
+	int host_f_max;
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sdda_start_init\n"));
+	/* periph was held for us when this task was enqueued */
+	if ((periph->flags & CAM_PERIPH_INVALID) != 0) {
+		cam_periph_release(periph);
+		return;
+	}
+
+	struct sdda_softc *softc = (struct sdda_softc *)periph->softc;
+	//struct ccb_mmcio *mmcio = &start_ccb->mmcio;
+	struct mmc_params *mmcp = &periph->path->device->mmc_ident_data;
+	struct cam_ed *device = periph->path->device;
+
+	if (mmcp->card_features & CARD_FEATURE_MMC) {
+		mmc_decode_csd_mmc(mmcp->card_csd, &softc->csd);
+		mmc_decode_cid_mmc(mmcp->card_cid, &softc->cid);
+		if (mmc_get_spec_vers(periph) >= 4) {
+			err = mmc_send_ext_csd(periph, start_ccb,
+					       (uint8_t *)&softc->raw_ext_csd,
+					       sizeof(softc->raw_ext_csd));
+			if (err != 0) {
+				CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH,
+				    ("Cannot read EXT_CSD, err %d", err));
+				return;
+			}
+		}
+	} else {
+		mmc_decode_csd_sd(mmcp->card_csd, &softc->csd);
+		mmc_decode_cid_sd(mmcp->card_cid, &softc->cid);
+	}
+
+	softc->sector_count = softc->csd.capacity / 512;
+	softc->mediasize = softc->csd.capacity;
+	softc->cmd6_time = mmc_get_cmd6_timeout(periph);
+
+	/* MMC >= 4.x have EXT_CSD that has its own opinion about capacity */
+	if (mmc_get_spec_vers(periph) >= 4) {
+		sec_count = softc->raw_ext_csd[EXT_CSD_SEC_CNT] +
+		    (softc->raw_ext_csd[EXT_CSD_SEC_CNT + 1] << 8) +
+		    (softc->raw_ext_csd[EXT_CSD_SEC_CNT + 2] << 16) +
+		    (softc->raw_ext_csd[EXT_CSD_SEC_CNT + 3] << 24);
+		if (sec_count != 0) {
+			softc->sector_count = sec_count;
+			softc->mediasize = softc->sector_count * 512;
+			/* FIXME: there should be a better name for this option...*/
+			mmcp->card_features |= CARD_FEATURE_SDHC;
+		}
+
+	}
+	CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH,
+	    ("Capacity: %"PRIu64", sectors: %"PRIu64"\n",
+		softc->mediasize,
+		softc->sector_count));
+	mmc_format_card_id_string(softc, mmcp);
+
+	/* Update info for CAM */
+	device->serial_num_len = strlen(softc->card_sn_string);
+	device->serial_num = (u_int8_t *)malloc((device->serial_num_len + 1),
+	    M_CAMXPT, M_NOWAIT);
+	strlcpy(device->serial_num, softc->card_sn_string, device->serial_num_len);
+
+	device->device_id_len = strlen(softc->card_id_string);
+	device->device_id = (u_int8_t *)malloc((device->device_id_len + 1),
+	    M_CAMXPT, M_NOWAIT);
+	strlcpy(device->device_id, softc->card_id_string, device->device_id_len);
+
+	strlcpy(mmcp->model, softc->card_id_string, sizeof(mmcp->model));
+
+	/* Set the clock frequency that the card can handle */
+	cts = &start_ccb->cts.proto_specific.mmc;
+
+	/* First, get the host's max freq */
+	start_ccb->ccb_h.func_code = XPT_GET_TRAN_SETTINGS;
+	start_ccb->ccb_h.flags = CAM_DIR_NONE;
+	start_ccb->ccb_h.retry_count = 0;
+	start_ccb->ccb_h.timeout = 100;
+	start_ccb->ccb_h.cbfcnp = NULL;
+	xpt_action(start_ccb);
+
+	if (start_ccb->ccb_h.status != CAM_REQ_CMP)
+		panic("Cannot get max host freq");
+	host_f_max = cts->host_f_max;
+	host_caps = cts->host_caps;
+	if (cts->ios.bus_width != bus_width_1)
+		panic("Bus width in ios is not 1-bit");
+
+	/* Now check if the card supports High-speed */
+	softc->card_f_max = softc->csd.tran_speed;
+
+	if (host_caps & MMC_CAP_HSPEED) {
+		/* Find out if the card supports High speed timing */
+		if (mmcp->card_features & CARD_FEATURE_SD20) {
+			/* Get and decode SCR */
+			uint32_t rawscr[2];
+			uint8_t res[64];
+			if (mmc_app_get_scr(periph, start_ccb, rawscr)) {
+				CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH, ("Cannot get SCR\n"));
+				goto finish_hs_tests;
+			}
+			mmc_app_decode_scr(rawscr, &softc->scr);
+
+			if ((softc->scr.sda_vsn >= 1) && (softc->csd.ccc & (1<<10))) {
+				mmc_sd_switch(periph, start_ccb, SD_SWITCH_MODE_CHECK,
+					      SD_SWITCH_GROUP1, SD_SWITCH_NOCHANGE, res);
+				if (res[13] & 2) {
+					CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH, ("Card supports HS\n"));
+					softc->card_f_max = SD_HS_MAX;
+				}
+
+				/*
+				 * We deselect then reselect the card here.  Some cards
+				 * become unselected and timeout with the above two
+				 * commands, although the state tables / diagrams in the
+				 * standard suggest they go back to the transfer state.
+				 * Other cards don't become deselected, and if we
+				 * attempt to blindly re-select them, we get timeout
+				 * errors from some controllers.  So we deselect then
+				 * reselect to handle all situations.
+				 */
+				mmc_select_card(periph, start_ccb, 0);
+				mmc_select_card(periph, start_ccb, get_rca(periph));
+			} else {
+				CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH, ("Not trying the switch\n"));
+				goto finish_hs_tests;
+			}
+		}
+
+		if (mmcp->card_features & CARD_FEATURE_MMC && mmc_get_spec_vers(periph) >= 4) {
+			if (softc->raw_ext_csd[EXT_CSD_CARD_TYPE]
+			    & EXT_CSD_CARD_TYPE_HS_52)
+				softc->card_f_max = MMC_TYPE_HS_52_MAX;
+			else if (softc->raw_ext_csd[EXT_CSD_CARD_TYPE]
+				 & EXT_CSD_CARD_TYPE_HS_26)
+				softc->card_f_max = MMC_TYPE_HS_26_MAX;
+		}
+	}
+	int f_max;
+finish_hs_tests:
+	f_max = min(host_f_max, softc->card_f_max);
+	CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH, ("Set SD freq to %d MHz (min out of host f=%d MHz and card f=%d MHz)\n", f_max  / 1000000, host_f_max / 1000000, softc->card_f_max / 1000000));
+
+	/* Enable high-speed timing on the card */
+	if (f_max > 25000000) {
+		err = mmc_set_timing(periph, start_ccb, bus_timing_hs);
+		if (err != MMC_ERR_NONE) {
+			CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("Cannot switch card to high-speed mode"));
+			f_max = 25000000;
+		}
+	}
+	/* Set frequency on the controller */
+	start_ccb->ccb_h.func_code = XPT_SET_TRAN_SETTINGS;
+	start_ccb->ccb_h.flags = CAM_DIR_NONE;
+	start_ccb->ccb_h.retry_count = 0;
+	start_ccb->ccb_h.timeout = 100;
+	start_ccb->ccb_h.cbfcnp = NULL;
+	cts->ios.clock = f_max;
+	cts->ios_valid = MMC_CLK;
+	xpt_action(start_ccb);
+
+	/* Set bus width */
+	enum mmc_bus_width desired_bus_width = bus_width_1;
+	enum mmc_bus_width max_host_bus_width =
+		(host_caps & MMC_CAP_8_BIT_DATA ? bus_width_8 :
+		 host_caps & MMC_CAP_4_BIT_DATA ? bus_width_4 : bus_width_1);
+	enum mmc_bus_width max_card_bus_width = bus_width_1;
+	if (mmcp->card_features & CARD_FEATURE_SD20 &&
+	    softc->scr.bus_widths & SD_SCR_BUS_WIDTH_4)
+		max_card_bus_width = bus_width_4;
+	/*
+	 * Unlike SD, MMC cards don't have any information about supported bus width...
+	 * So we need to perform read/write test to find out the width.
+	 */
+	/* TODO: figure out bus width for MMC; use 8-bit for now (to test on BBB) */
+	if (mmcp->card_features & CARD_FEATURE_MMC)
+		max_card_bus_width = bus_width_8;
+
+	desired_bus_width = min(max_host_bus_width, max_card_bus_width);
+	CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH,
+		  ("Set bus width to %s (min of host %s and card %s)\n",
+		   bus_width_str(desired_bus_width),
+		   bus_width_str(max_host_bus_width),
+		   bus_width_str(max_card_bus_width)));
+	sdda_set_bus_width(periph, start_ccb, desired_bus_width);
+
+	softc->state = SDDA_STATE_NORMAL;
+
+	/* MMC partitions support */
+	if (mmcp->card_features & CARD_FEATURE_MMC && mmc_get_spec_vers(periph) >= 4) {
+		sdda_process_mmc_partitions(periph, start_ccb);
+	} else if (mmcp->card_features & CARD_FEATURE_SD20) {
+		/* For SD[HC] cards, just add one partition that is the whole card */
+		sdda_add_part(periph, 0, "sdda",
+		    periph->unit_number,
+		    mmc_get_media_size(periph),
+		    sdda_get_read_only(periph, start_ccb));
+		softc->part_curr = 0;
+	}
+
+	xpt_announce_periph(periph, softc->card_id_string);
+	/*
+	 * Add async callbacks for bus reset and bus device reset calls.
+	 * I don't bother checking if this fails as, in most cases,
+	 * the system will function just fine without them and the only
+	 * alternative would be to not attach the device on failure.
+	 */
+	xpt_register_async(AC_LOST_DEVICE | AC_GETDEV_CHANGED |
+	    AC_ADVINFO_CHANGED, sddaasync, periph, periph->path);
+}
+
+static void
+sdda_add_part(struct cam_periph *periph, u_int type, const char *name,
+    u_int cnt, off_t media_size, bool ro)
+{
+	struct sdda_softc *sc = (struct sdda_softc *)periph->softc;
+	struct sdda_part *part;
+	struct ccb_pathinq cpi;
+	u_int maxio;
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH,
+	    ("Partition type '%s', size %ju %s\n",
+	    part_type(type),
+	    media_size,
+	    ro ? "(read-only)" : ""));
+
+	part = sc->part[type] = malloc(sizeof(*part), M_DEVBUF,
+	    M_WAITOK | M_ZERO);
+
+	part->cnt = cnt;
+	part->type = type;
+	part->ro = ro;
+	part->sc = sc;
+	snprintf(part->name, sizeof(part->name), name, periph->unit_number);
+
+	/*
+	 * Due to the nature of RPMB partition it doesn't make much sense
+	 * to add it as a disk. It would be more appropriate to create a
+	 * userland tool to operate on the partition or leverage the existing
+	 * tools from sysutils/mmc-utils.
+	 */
+	if (type == EXT_CSD_PART_CONFIG_ACC_RPMB) {
+		/* TODO: Create device, assign IOCTL handler */
+		CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH,
+		    ("Don't know what to do with RPMB partitions yet\n"));
+		return;
+	}
+#ifndef __rtems__
+	bioq_init(&part->bio_queue);
+#endif /* __rtems__ */
+
+	bzero(&cpi, sizeof(cpi));
+	xpt_setup_ccb(&cpi.ccb_h, periph->path, CAM_PRIORITY_NONE);
+	cpi.ccb_h.func_code = XPT_PATH_INQ;
+	xpt_action((union ccb *)&cpi);
+
+	/*
+	 * Register this media as a disk
+	 */
+	(void)cam_periph_hold(periph, PRIBIO);
+	cam_periph_unlock(periph);
+
+#ifndef __rtems__
+	part->disk = disk_alloc();
+	part->disk->d_rotation_rate = DISK_RR_NON_ROTATING;
+	part->disk->d_devstat = devstat_new_entry(part->name,
+	    cnt, 512,
+	    DEVSTAT_ALL_SUPPORTED,
+	    DEVSTAT_TYPE_DIRECT | XPORT_DEVSTAT_TYPE(cpi.transport),
+	    DEVSTAT_PRIORITY_DISK);
+
+	part->disk->d_open = sddaopen;
+	part->disk->d_close = sddaclose;
+	part->disk->d_strategy = sddastrategy;
+	part->disk->d_getattr = sddagetattr;
+//	sc->disk->d_dump = sddadump;
+	part->disk->d_gone = sddadiskgonecb;
+	part->disk->d_name = part->name;
+	part->disk->d_drv1 = part;
+	maxio = cpi.maxio;		/* Honor max I/O size of SIM */
+	if (maxio == 0)
+		maxio = DFLTPHYS;	/* traditional default */
+	else if (maxio > MAXPHYS)
+		maxio = MAXPHYS;	/* for safety */
+	part->disk->d_maxsize = maxio;
+	part->disk->d_unit = cnt;
+	part->disk->d_flags = 0;
+	strlcpy(part->disk->d_descr, sc->card_id_string,
+	    MIN(sizeof(part->disk->d_descr), sizeof(sc->card_id_string)));
+	strlcpy(part->disk->d_ident, sc->card_sn_string,
+	    MIN(sizeof(part->disk->d_ident), sizeof(sc->card_sn_string)));
+	part->disk->d_hba_vendor = cpi.hba_vendor;
+	part->disk->d_hba_device = cpi.hba_device;
+	part->disk->d_hba_subvendor = cpi.hba_subvendor;
+	part->disk->d_hba_subdevice = cpi.hba_subdevice;
+
+	part->disk->d_sectorsize = mmc_get_sector_size(periph);
+	part->disk->d_mediasize = media_size;
+	part->disk->d_stripesize = 0;
+	part->disk->d_fwsectors = 0;
+	part->disk->d_fwheads = 0;
+#endif /* __rtems__ */
+
+	/*
+	 * Acquire a reference to the periph before we register with GEOM.
+	 * We'll release this reference once GEOM calls us back (via
+	 * sddadiskgonecb()) telling us that our provider has been freed.
+	 */
+	if (cam_periph_acquire(periph) != CAM_REQ_CMP) {
+		xpt_print(periph->path, "%s: lost periph during "
+		    "registration!\n", __func__);
+		cam_periph_lock(periph);
+		return;
+	}
+#ifdef __rtems__
+	rtems_status_code status_code = rtems_media_server_disk_attach(
+        part->name, rtems_bsd_sdda_attach_worker, part);
+	BSD_ASSERT(status_code == RTEMS_SUCCESSFUL);
+#else /* __rtems__ */
+	disk_create(part->disk, DISK_VERSION);
+#endif /*__rtems__ */
+
+	cam_periph_lock(periph);
+	cam_periph_unhold(periph);
+}
+
+/*
+ * For MMC cards, process EXT_CSD and add partitions that are supported by
+ * this device.
+ */
+static void
+sdda_process_mmc_partitions(struct cam_periph *periph, union ccb *ccb)
+{
+	struct sdda_softc *sc = (struct sdda_softc *)periph->softc;
+	struct mmc_params *mmcp = &periph->path->device->mmc_ident_data;
+	off_t erase_size, sector_size, size, wp_size;
+	int i;
+	const uint8_t *ext_csd;
+	uint8_t rev;
+	bool comp, ro;
+
+	ext_csd = sc->raw_ext_csd;
+
+	/*
+	 * Enhanced user data area and general purpose partitions are only
+	 * supported in revision 1.4 (EXT_CSD_REV == 4) and later, the RPMB
+	 * partition in revision 1.5 (MMC v4.41, EXT_CSD_REV == 5) and later.
+	 */
+	rev = ext_csd[EXT_CSD_REV];
+
+	/*
+	 * Ignore user-creatable enhanced user data area and general purpose
+	 * partitions partitions as long as partitioning hasn't been finished.
+	 */
+	comp = (ext_csd[EXT_CSD_PART_SET] & EXT_CSD_PART_SET_COMPLETED) != 0;
+
+	/*
+	 * Add enanced user data area slice, unless it spans the entirety of
+	 * the user data area.  The enhanced area is of a multiple of high
+	 * capacity write protect groups ((ERASE_GRP_SIZE + HC_WP_GRP_SIZE) *
+	 * 512 KB) and its offset given in either sectors or bytes, depending
+	 * on whether it's a high capacity device or not.
+	 * NB: The slicer and its slices need to be registered before adding
+	 *     the disk for the corresponding user data area as re-tasting is
+	 *     racy.
+	 */
+	sector_size = mmc_get_sector_size(periph);
+	size = ext_csd[EXT_CSD_ENH_SIZE_MULT] +
+		(ext_csd[EXT_CSD_ENH_SIZE_MULT + 1] << 8) +
+		(ext_csd[EXT_CSD_ENH_SIZE_MULT + 2] << 16);
+	if (rev >= 4 && comp == TRUE && size > 0 &&
+	    (ext_csd[EXT_CSD_PART_SUPPORT] &
+		EXT_CSD_PART_SUPPORT_ENH_ATTR_EN) != 0 &&
+	    (ext_csd[EXT_CSD_PART_ATTR] & (EXT_CSD_PART_ATTR_ENH_USR)) != 0) {
+		erase_size = ext_csd[EXT_CSD_ERASE_GRP_SIZE] * 1024 *
+			MMC_SECTOR_SIZE;
+		wp_size = ext_csd[EXT_CSD_HC_WP_GRP_SIZE];
+		size *= erase_size * wp_size;
+		if (size != mmc_get_media_size(periph) * sector_size) {
+			sc->enh_size = size;
+			sc->enh_base = (ext_csd[EXT_CSD_ENH_START_ADDR] +
+			    (ext_csd[EXT_CSD_ENH_START_ADDR + 1] << 8) +
+			    (ext_csd[EXT_CSD_ENH_START_ADDR + 2] << 16) +
+			    (ext_csd[EXT_CSD_ENH_START_ADDR + 3] << 24)) *
+				((mmcp->card_features & CARD_FEATURE_SDHC) ? 1: MMC_SECTOR_SIZE);
+		} else
+			CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH,
+			    ("enhanced user data area spans entire device"));
+	}
+
+	/*
+	 * Add default partition.  This may be the only one or the user
+	 * data area in case partitions are supported.
+	 */
+	ro = sdda_get_read_only(periph, ccb);
+	sdda_add_part(periph, EXT_CSD_PART_CONFIG_ACC_DEFAULT, "sdda",
+	    periph->unit_number, mmc_get_media_size(periph), ro);
+	sc->part_curr = EXT_CSD_PART_CONFIG_ACC_DEFAULT;
+
+	if (mmc_get_spec_vers(periph) < 3)
+		return;
+
+	/* Belatedly announce enhanced user data slice. */
+	if (sc->enh_size != 0) {
+		CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH,
+		    ("enhanced user data area off 0x%jx size %ju bytes\n",
+			sc->enh_base, sc->enh_size));
+	}
+
+	/*
+	 * Determine partition switch timeout (provided in units of 10 ms)
+	 * and ensure it's at least 300 ms as some eMMC chips lie.
+	 */
+	sc->part_time = max(ext_csd[EXT_CSD_PART_SWITCH_TO] * 10 * 1000,
+	    300 * 1000);
+
+	/* Add boot partitions, which are of a fixed multiple of 128 KB. */
+	size = ext_csd[EXT_CSD_BOOT_SIZE_MULT] * MMC_BOOT_RPMB_BLOCK_SIZE;
+	if (size > 0 && (sdda_get_host_caps(periph, ccb) & MMC_CAP_BOOT_NOACC) == 0) {
+		sdda_add_part(periph, EXT_CSD_PART_CONFIG_ACC_BOOT0,
+		    SDDA_FMT_BOOT, 0, size,
+		    ro | ((ext_csd[EXT_CSD_BOOT_WP_STATUS] &
+		    EXT_CSD_BOOT_WP_STATUS_BOOT0_MASK) != 0));
+		sdda_add_part(periph, EXT_CSD_PART_CONFIG_ACC_BOOT1,
+		    SDDA_FMT_BOOT, 1, size,
+		    ro | ((ext_csd[EXT_CSD_BOOT_WP_STATUS] &
+		    EXT_CSD_BOOT_WP_STATUS_BOOT1_MASK) != 0));
+	}
+
+	/* Add RPMB partition, which also is of a fixed multiple of 128 KB. */
+	size = ext_csd[EXT_CSD_RPMB_MULT] * MMC_BOOT_RPMB_BLOCK_SIZE;
+	if (rev >= 5 && size > 0)
+		sdda_add_part(periph, EXT_CSD_PART_CONFIG_ACC_RPMB,
+		    SDDA_FMT_RPMB, 0, size, ro);
+
+	if (rev <= 3 || comp == FALSE)
+		return;
+
+	/*
+	 * Add general purpose partitions, which are of a multiple of high
+	 * capacity write protect groups, too.
+	 */
+	if ((ext_csd[EXT_CSD_PART_SUPPORT] & EXT_CSD_PART_SUPPORT_EN) != 0) {
+		erase_size = ext_csd[EXT_CSD_ERASE_GRP_SIZE] * 1024 *
+			MMC_SECTOR_SIZE;
+		wp_size = ext_csd[EXT_CSD_HC_WP_GRP_SIZE];
+		for (i = 0; i < MMC_PART_GP_MAX; i++) {
+			size = ext_csd[EXT_CSD_GP_SIZE_MULT + i * 3] +
+				(ext_csd[EXT_CSD_GP_SIZE_MULT + i * 3 + 1] << 8) +
+				(ext_csd[EXT_CSD_GP_SIZE_MULT + i * 3 + 2] << 16);
+			if (size == 0)
+				continue;
+			sdda_add_part(periph, EXT_CSD_PART_CONFIG_ACC_GP0 + i,
+			    SDDA_FMT_GP, i, size * erase_size * wp_size, ro);
+		}
+	}
+}
+
+/*
+ * We cannot just call mmc_switch() since it will sleep, and we are in
+ * GEOM context and cannot sleep. Instead, create an MMCIO request to switch
+ * partitions and send it to h/w, and upon completion resume processing
+ * the I/O que This function cannot fail, instead check switch errors in sddadone().
+ */
+static void
+sdda_init_switch_part(struct cam_periph *periph, union ccb *start_ccb, u_int part) {
+	struct sdda_softc *sc = (struct sdda_softc *)periph->softc;
+	uint8_t value;
+
+	sc->part_requested = part;
+
+	value = (sc->raw_ext_csd[EXT_CSD_PART_CONFIG] &
+	    ~EXT_CSD_PART_CONFIG_ACC_MASK) | part;
+
+	mmc_switch_fill_mmcio(start_ccb, EXT_CSD_CMD_SET_NORMAL,
+	    EXT_CSD_PART_CONFIG, value, sc->part_time);
+	start_ccb->ccb_h.cbfcnp = sddadone;
+
+	sc->outstanding_cmds++;
+	cam_periph_unlock(periph);
+	xpt_action(start_ccb);
+	cam_periph_lock(periph);
+}
+
+/* Called with periph lock held! */
+static void
+sddastart(struct cam_periph *periph, union ccb *start_ccb)
+{
+#ifndef __rtems__
+	struct bio *bp;
+#else
+	rtems_sddastart(periph,start_ccb);
+#endif /* __rtems__ */
+#ifndef __rtems__
+	struct sdda_softc *softc = (struct sdda_softc *)periph->softc;
+	struct sdda_part *part;
+	struct mmc_params *mmcp = &periph->path->device->mmc_ident_data;
+	int part_index;
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("sddastart\n"));
+
+	if (softc->state != SDDA_STATE_NORMAL) {
+		CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("device is not in SDDA_STATE_NORMAL yet\n"));
+		xpt_release_ccb(start_ccb);
+		return;
+	}
+	/* Find partition that has outstanding commands.  Prefer current partition. */
+	part = softc->part[softc->part_curr];
+	bp = bioq_first(&part->bio_queue);
+	if (bp == NULL) {
+		for (part_index = 0; part_index < MMC_PART_MAX; part_index++) {
+			if ((part = softc->part[part_index]) != NULL &&
+			    (bp = bioq_first(&softc->part[part_index]->bio_queue)) != NULL)
+				break;
+		}
+	}
+	if (bp == NULL) {
+		xpt_release_ccb(start_ccb);
+		return;
+	}
+	if (part_index != softc->part_curr) {
+		CAM_DEBUG(periph->path, CAM_DEBUG_PERIPH,
+		    ("Partition  %d -> %d\n", softc->part_curr, part_index));
+		/*
+		 * According to section "6.2.2 Command restrictions" of the eMMC
+		 * specification v5.1, CMD19/CMD21 aren't allowed to be used with
+		 * RPMB partitions.  So we pause re-tuning along with triggering
+		 * it up-front to decrease the likelihood of re-tuning becoming
+		 * necessary while accessing an RPMB partition.  Consequently, an
+		 * RPMB partition should immediately be switched away from again
+		 * after an access in order to allow for re-tuning to take place
+		 * anew.
+		 */
+		/* TODO: pause retune if switching to RPMB partition */
+		softc->state = SDDA_STATE_PART_SWITCH;
+		sdda_init_switch_part(periph, start_ccb, part_index);
+		return;
+	}
+
+	bioq_remove(&part->bio_queue, bp);
+
+	switch (bp->bio_cmd) {
+	case BIO_WRITE:
+		CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("BIO_WRITE\n"));
+		part->flags |= SDDA_FLAG_DIRTY;
+		/* FALLTHROUGH */
+	case BIO_READ:
+	{
+		struct ccb_mmcio *mmcio;
+		uint64_t blockno = bp->bio_pblkno;
+		uint16_t count = bp->bio_bcount / 512;
+		uint16_t opcode;
+
+		if (bp->bio_cmd == BIO_READ)
+			CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("BIO_READ\n"));
+		CAM_DEBUG(periph->path, CAM_DEBUG_TRACE,
+		    ("Block %"PRIu64" cnt %u\n", blockno, count));
+
+		/* Construct new MMC command */
+		if (bp->bio_cmd == BIO_READ) {
+			if (count > 1)
+				opcode = MMC_READ_MULTIPLE_BLOCK;
+			else
+				opcode = MMC_READ_SINGLE_BLOCK;
+		} else {
+			if (count > 1)
+				opcode = MMC_WRITE_MULTIPLE_BLOCK;
+			else
+				opcode = MMC_WRITE_BLOCK;
+		}
+
+		start_ccb->ccb_h.func_code = XPT_MMC_IO;
+		start_ccb->ccb_h.flags = (bp->bio_cmd == BIO_READ ? CAM_DIR_IN : CAM_DIR_OUT);
+		start_ccb->ccb_h.retry_count = 0;
+		start_ccb->ccb_h.timeout = 15 * 1000;
+		start_ccb->ccb_h.cbfcnp = sddadone;
+
+		mmcio = &start_ccb->mmcio;
+		mmcio->cmd.opcode = opcode;
+		mmcio->cmd.arg = blockno;
+		if (!(mmcp->card_features & CARD_FEATURE_SDHC))
+			mmcio->cmd.arg <<= 9;
+
+		mmcio->cmd.flags = MMC_RSP_R1 | MMC_CMD_ADTC;
+		mmcio->cmd.data = softc->mmcdata;
+		mmcio->cmd.data->data = bp->bio_data;
+		mmcio->cmd.data->len = 512 * count;
+		mmcio->cmd.data->flags = (bp->bio_cmd == BIO_READ ? MMC_DATA_READ : MMC_DATA_WRITE);
+		/* Direct h/w to issue CMD12 upon completion */
+		if (count > 1) {
+			mmcio->cmd.data->flags |= MMC_DATA_MULTI;
+			mmcio->stop.opcode = MMC_STOP_TRANSMISSION;
+			mmcio->stop.flags = MMC_RSP_R1B | MMC_CMD_AC;
+			mmcio->stop.arg = 0;
+		}
+		break;
+	}
+	case BIO_FLUSH:
+		CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("BIO_FLUSH\n"));
+		sddaschedule(periph);
+		break;
+	case BIO_DELETE:
+		CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("BIO_DELETE\n"));
+		sddaschedule(periph);
+		break;
+	}
+	start_ccb->ccb_h.ccb_bp = bp;
+	softc->outstanding_cmds++;
+	softc->refcount++;
+	cam_periph_unlock(periph);
+	xpt_action(start_ccb);
+	cam_periph_lock(periph);
+
+	/* May have more work to do, so ensure we stay scheduled */
+	sddaschedule(periph);
+#endif /* __rtems__ */
+}
+
+static void
+sddadone(struct cam_periph *periph, union ccb *done_ccb)
+{
+	struct bio *bp;
+	struct sdda_softc *softc;
+	struct ccb_mmcio *mmcio;
+	struct cam_path *path;
+	uint32_t card_status;
+	int error = 0;
+
+	softc = (struct sdda_softc *)periph->softc;
+	mmcio = &done_ccb->mmcio;
+#ifndef __rtems__
+	path = done_ccb->ccb_h.path;
+#endif
+	CAM_DEBUG(path, CAM_DEBUG_TRACE, ("sddadone\n"));
+//        cam_periph_lock(periph);
+	if ((done_ccb->ccb_h.status & CAM_STATUS_MASK) != CAM_REQ_CMP) {
+		CAM_DEBUG(path, CAM_DEBUG_TRACE, ("Error!!!\n"));
+		if ((done_ccb->ccb_h.status & CAM_DEV_QFRZN) != 0)
+			cam_release_devq(path,
+			    /*relsim_flags*/0,
+			    /*reduction*/0,
+			    /*timeout*/0,
+			    /*getcount_only*/0);
+		error = 5; /* EIO */
+	} else {
+		if ((done_ccb->ccb_h.status & CAM_DEV_QFRZN) != 0)
+			panic("REQ_CMP with QFRZN");
+		error = 0;
+	}
+
+	card_status = mmcio->cmd.resp[0];
+	CAM_DEBUG(path, CAM_DEBUG_TRACE,
+	    ("Card status: %08x\n", R1_STATUS(card_status)));
+	CAM_DEBUG(path, CAM_DEBUG_TRACE,
+	    ("Current state: %d\n", R1_CURRENT_STATE(card_status)));
+
+	/* Process result of switching MMC partitions */
+	if (softc->state == SDDA_STATE_PART_SWITCH) {
+		CAM_DEBUG(path, CAM_DEBUG_TRACE,
+		    ("Compteting partition switch to %d\n", softc->part_requested));
+		softc->outstanding_cmds--;
+		/* Complete partition switch */
+		softc->state = SDDA_STATE_NORMAL;
+		if (error != MMC_ERR_NONE) {
+			/* TODO: Unpause retune if accessing RPMB */
+			xpt_release_ccb(done_ccb);
+			xpt_schedule(periph, CAM_PRIORITY_NORMAL);
+			return;
+		}
+
+		softc->raw_ext_csd[EXT_CSD_PART_CONFIG] =
+		    (softc->raw_ext_csd[EXT_CSD_PART_CONFIG] &
+			~EXT_CSD_PART_CONFIG_ACC_MASK) | softc->part_requested;
+		/* TODO: Unpause retune if accessing RPMB */
+		softc->part_curr = softc->part_requested;
+		xpt_release_ccb(done_ccb);
+
+		/* Return to processing BIO requests */
+		xpt_schedule(periph, CAM_PRIORITY_NORMAL);
+		return;
+	}
+#ifndef __rtems__
+	bp = (struct bio *)done_ccb->ccb_h.ccb_bp;
+	bp->bio_error = error;
+	if (error != 0) {
+		bp->bio_resid = bp->bio_bcount;
+		bp->bio_flags |= BIO_ERROR;
+	} else {
+		/* XXX: How many bytes remaining? */
+		bp->bio_resid = 0;
+		if (bp->bio_resid > 0)
+			bp->bio_flags |= BIO_ERROR;
+	}
+#endif
+
+	softc->outstanding_cmds--;
+	xpt_release_ccb(done_ccb);
+	/*
+	 * Release the periph refcount taken in sddastart() for each CCB.
+	 */
+	KASSERT(softc->refcount >= 1, ("sddadone softc %p refcount %d", softc, softc->refcount));
+	softc->refcount--;
+#ifndef __rtems__
+	biodone(bp);
+#endif
+}
+
+static int
+sddaerror(union ccb *ccb, u_int32_t cam_flags, u_int32_t sense_flags)
+{
+	return(cam_periph_error(ccb, cam_flags, sense_flags));
+}
+#endif /* _KERNEL */
diff --git a/freebsd/sys/cam/mmc/mmc_xpt.c b/freebsd/sys/cam/mmc/mmc_xpt.c
new file mode 100644
index 0000000..7c30e76
--- /dev/null
+++ b/freebsd/sys/cam/mmc/mmc_xpt.c
@@ -0,0 +1,1110 @@
+#include <machine/rtems-bsd-kernel-space.h>
+
+/*-
+ * SPDX-License-Identifier: BSD-2-Clause-FreeBSD
+ *
+ * Copyright (c) 2013,2014 Ilya Bakulin <ilya@bakulin.de>
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/bus.h>
+#include <sys/endian.h>
+#include <sys/systm.h>
+#include <sys/types.h>
+#include <sys/malloc.h>
+#include <sys/kernel.h>
+#include <sys/time.h>
+#include <sys/conf.h>
+#include <sys/fcntl.h>
+#include <sys/interrupt.h>
+#include <sys/sbuf.h>
+
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/sysctl.h>
+#include <sys/condvar.h>
+
+#include <cam/cam.h>
+#include <cam/cam_ccb.h>
+#include <cam/cam_queue.h>
+#include <cam/cam_periph.h>
+#include <cam/cam_sim.h>
+#include <cam/cam_xpt.h>
+#include <cam/cam_xpt_sim.h>
+#include <cam/cam_xpt_periph.h>
+#include <cam/cam_xpt_internal.h>
+#include <cam/cam_debug.h>
+
+#include <cam/mmc/mmc.h>
+#include <cam/mmc/mmc_bus.h>
+
+#include <machine/stdarg.h>	/* for xpt_print below */
+#include <machine/_inttypes.h>  /* for PRIu64 */
+#include <rtems/bsd/local/opt_cam.h>
+
+FEATURE(mmccam, "CAM-based MMC/SD/SDIO stack");
+
+static struct cam_ed * mmc_alloc_device(struct cam_eb *bus,
+    struct cam_et *target, lun_id_t lun_id);
+static void mmc_dev_async(u_int32_t async_code, struct cam_eb *bus,
+    struct cam_et *target, struct cam_ed *device, void *async_arg);
+static void	 mmc_action(union ccb *start_ccb);
+static void	 mmc_dev_advinfo(union ccb *start_ccb);
+static void	 mmc_announce_periph(struct cam_periph *periph);
+static void	 mmc_scan_lun(struct cam_periph *periph,
+    struct cam_path *path, cam_flags flags, union ccb *ccb);
+
+/* mmcprobe methods */
+static cam_status mmcprobe_register(struct cam_periph *periph, void *arg);
+static void	 mmcprobe_start(struct cam_periph *periph, union ccb *start_ccb);
+static void	 mmcprobe_cleanup(struct cam_periph *periph);
+static void	 mmcprobe_done(struct cam_periph *periph, union ccb *done_ccb);
+
+static void mmc_proto_announce(struct cam_ed *device);
+static void mmc_proto_denounce(struct cam_ed *device);
+static void mmc_proto_debug_out(union ccb *ccb);
+
+typedef enum {
+	PROBE_RESET,
+	PROBE_IDENTIFY,
+        PROBE_SDIO_RESET,
+	PROBE_SEND_IF_COND,
+        PROBE_SDIO_INIT,
+        PROBE_MMC_INIT,
+	PROBE_SEND_APP_OP_COND,
+        PROBE_GET_CID,
+        PROBE_GET_CSD,
+        PROBE_SEND_RELATIVE_ADDR,
+	PROBE_MMC_SET_RELATIVE_ADDR,
+        PROBE_SELECT_CARD,
+	PROBE_DONE,
+	PROBE_INVALID
+} probe_action;
+
+static char *probe_action_text[] = {
+	"PROBE_RESET",
+	"PROBE_IDENTIFY",
+        "PROBE_SDIO_RESET",
+	"PROBE_SEND_IF_COND",
+        "PROBE_SDIO_INIT",
+        "PROBE_MMC_INIT",
+	"PROBE_SEND_APP_OP_COND",
+        "PROBE_GET_CID",
+        "PROBE_GET_CSD",
+        "PROBE_SEND_RELATIVE_ADDR",
+	"PROBE_MMC_SET_RELATIVE_ADDR",
+        "PROBE_SELECT_CARD",
+	"PROBE_DONE",
+	"PROBE_INVALID"
+};
+
+#define PROBE_SET_ACTION(softc, newaction)	\
+do {									\
+	char **text;							\
+	text = probe_action_text;					\
+	CAM_DEBUG((softc)->periph->path, CAM_DEBUG_PROBE,		\
+	    ("Probe %s to %s\n", text[(softc)->action],			\
+	    text[(newaction)]));					\
+	(softc)->action = (newaction);					\
+} while(0)
+
+static struct xpt_xport_ops mmc_xport_ops = {
+	.alloc_device = mmc_alloc_device,
+	.action = mmc_action,
+	.async = mmc_dev_async,
+	.announce = mmc_announce_periph,
+};
+
+#define MMC_XPT_XPORT(x, X)				\
+	static struct xpt_xport mmc_xport_ ## x = {	\
+		.xport = XPORT_ ## X,			\
+		.name = #x,				\
+		.ops = &mmc_xport_ops,			\
+	};						\
+	CAM_XPT_XPORT(mmc_xport_ ## x);
+
+MMC_XPT_XPORT(mmc, MMCSD);
+
+static struct xpt_proto_ops mmc_proto_ops = {
+	.announce = mmc_proto_announce,
+	.denounce = mmc_proto_denounce,
+	.debug_out = mmc_proto_debug_out,
+};
+
+static struct xpt_proto mmc_proto = {
+	.proto = PROTO_MMCSD,
+	.name = "mmcsd",
+	.ops = &mmc_proto_ops,
+};
+CAM_XPT_PROTO(mmc_proto);
+
+typedef struct {
+	probe_action	action;
+	int             restart;
+	union ccb	saved_ccb;
+	uint32_t	flags;
+#define PROBE_FLAG_ACMD_SENT	0x1 /* CMD55 is sent, card expects ACMD */
+	uint8_t         acmd41_count; /* how many times ACMD41 has been issued */
+	struct cam_periph *periph;
+} mmcprobe_softc;
+
+/* XPort functions -- an interface to CAM at periph side */
+
+static struct cam_ed *
+mmc_alloc_device(struct cam_eb *bus, struct cam_et *target, lun_id_t lun_id)
+{
+	struct cam_ed *device;
+
+	printf("mmc_alloc_device()\n");
+	device = xpt_alloc_device(bus, target, lun_id);
+	if (device == NULL)
+		return (NULL);
+
+	device->quirk = NULL;
+	device->mintags = 0;
+	device->maxtags = 0;
+	bzero(&device->inq_data, sizeof(device->inq_data));
+	device->inq_flags = 0;
+	device->queue_flags = 0;
+	device->serial_num = NULL;
+	device->serial_num_len = 0;
+	return (device);
+}
+
+static void
+mmc_dev_async(u_int32_t async_code, struct cam_eb *bus, struct cam_et *target,
+	      struct cam_ed *device, void *async_arg)
+{
+
+	printf("mmc_dev_async(async_code=0x%x, path_id=%d, target_id=%x, lun_id=%" SCNx64 "\n",
+	       async_code,
+	       bus->path_id,
+	       target->target_id,
+	       device->lun_id);
+	/*
+	 * We only need to handle events for real devices.
+	 */
+	if (target->target_id == CAM_TARGET_WILDCARD
+            || device->lun_id == CAM_LUN_WILDCARD)
+		return;
+
+        if (async_code == AC_LOST_DEVICE) {
+                if ((device->flags & CAM_DEV_UNCONFIGURED) == 0) {
+                        printf("AC_LOST_DEVICE -> set to unconfigured\n");
+                        device->flags |= CAM_DEV_UNCONFIGURED;
+                        xpt_release_device(device);
+                } else {
+                        printf("AC_LOST_DEVICE on unconfigured device\n");
+                }
+        } else if (async_code == AC_FOUND_DEVICE) {
+                printf("Got AC_FOUND_DEVICE -- whatever...\n");
+        } else if (async_code == AC_PATH_REGISTERED) {
+                printf("Got AC_PATH_REGISTERED -- whatever...\n");
+        } else if (async_code == AC_PATH_DEREGISTERED ) {
+                        printf("Got AC_PATH_DEREGISTERED -- whatever...\n");
+	} else if (async_code == AC_UNIT_ATTENTION) {
+		printf("Got interrupt generated by the card and ignored it\n");
+	} else
+		panic("Unknown async code\n");
+}
+
+/* Taken from nvme_scan_lun, thanks to bsdimp@ */
+static void
+mmc_scan_lun(struct cam_periph *periph, struct cam_path *path,
+	     cam_flags flags, union ccb *request_ccb)
+{
+	struct ccb_pathinq cpi;
+	cam_status status;
+	struct cam_periph *old_periph;
+	int lock;
+
+	CAM_DEBUG(path, CAM_DEBUG_TRACE, ("mmc_scan_lun\n"));
+
+	xpt_path_inq(&cpi, path);
+
+	if (cpi.ccb_h.status != CAM_REQ_CMP) {
+		if (request_ccb != NULL) {
+			request_ccb->ccb_h.status = cpi.ccb_h.status;
+			xpt_done(request_ccb);
+		}
+		return;
+	}
+
+	if (xpt_path_lun_id(path) == CAM_LUN_WILDCARD) {
+		CAM_DEBUG(path, CAM_DEBUG_TRACE, ("mmd_scan_lun ignoring bus\n"));
+		request_ccb->ccb_h.status = CAM_REQ_CMP;	/* XXX signal error ? */
+		xpt_done(request_ccb);
+		return;
+	}
+
+	lock = (xpt_path_owned(path) == 0);
+	if (lock)
+		xpt_path_lock(path);
+
+	if ((old_periph = cam_periph_find(path, "mmcprobe")) != NULL) {
+		if ((old_periph->flags & CAM_PERIPH_INVALID) == 0) {
+//			mmcprobe_softc *softc;
+//			softc = (mmcprobe_softc *)old_periph->softc;
+//                      Not sure if we need request ccb queue for mmc
+//			TAILQ_INSERT_TAIL(&softc->request_ccbs,
+//				&request_ccb->ccb_h, periph_links.tqe);
+//			softc->restart = 1;
+                        CAM_DEBUG(path, CAM_DEBUG_INFO,
+                                  ("Got scan request, but mmcprobe already exists\n"));
+			request_ccb->ccb_h.status = CAM_REQ_CMP_ERR;
+                        xpt_done(request_ccb);
+		} else {
+			request_ccb->ccb_h.status = CAM_REQ_CMP_ERR;
+			xpt_done(request_ccb);
+		}
+	} else {
+		xpt_print(path, " Set up the mmcprobe device...\n");
+
+                status = cam_periph_alloc(mmcprobe_register, NULL,
+					  mmcprobe_cleanup,
+					  mmcprobe_start,
+					  "mmcprobe",
+					  CAM_PERIPH_BIO,
+					  path, NULL, 0,
+					  request_ccb);
+                if (status != CAM_REQ_CMP) {
+			xpt_print(path, "xpt_scan_lun: cam_alloc_periph "
+                                  "returned an error, can't continue probe\n");
+		}
+		request_ccb->ccb_h.status = status;
+		xpt_done(request_ccb);
+	}
+
+	if (lock)
+		xpt_path_unlock(path);
+}
+
+static void
+mmc_action(union ccb *start_ccb)
+{
+	CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_TRACE,
+		  ("mmc_action! func_code=%x, action %s\n", start_ccb->ccb_h.func_code,
+		   xpt_action_name(start_ccb->ccb_h.func_code)));
+	switch (start_ccb->ccb_h.func_code) {
+
+	case XPT_SCAN_BUS:
+                /* FALLTHROUGH */
+	case XPT_SCAN_TGT:
+                /* FALLTHROUGH */
+	case XPT_SCAN_LUN:
+		CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_INFO,
+			  ("XPT_SCAN_{BUS,TGT,LUN}\n"));
+		mmc_scan_lun(start_ccb->ccb_h.path->periph,
+			     start_ccb->ccb_h.path, start_ccb->crcn.flags,
+			     start_ccb);
+		break;
+
+	case XPT_DEV_ADVINFO:
+	{
+		mmc_dev_advinfo(start_ccb);
+		break;
+	}
+
+	default:
+		xpt_action_default(start_ccb);
+		break;
+	}
+}
+
+static void
+mmc_dev_advinfo(union ccb *start_ccb)
+{
+	struct cam_ed *device;
+	struct ccb_dev_advinfo *cdai;
+	off_t amt;
+
+	start_ccb->ccb_h.status = CAM_REQ_INVALID;
+	device = start_ccb->ccb_h.path->device;
+	cdai = &start_ccb->cdai;
+	CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_TRACE,
+		  ("%s: request %x\n", __func__, cdai->buftype));
+
+        /* We don't support writing any data */
+        if (cdai->flags & CDAI_FLAG_STORE)
+                panic("Attempt to store data?!");
+
+	switch(cdai->buftype) {
+	case CDAI_TYPE_SCSI_DEVID:
+		cdai->provsiz = device->device_id_len;
+		if (device->device_id_len == 0)
+			break;
+		amt = MIN(cdai->provsiz, cdai->bufsiz);
+		memcpy(cdai->buf, device->device_id, amt);
+		break;
+	case CDAI_TYPE_SERIAL_NUM:
+		cdai->provsiz = device->serial_num_len;
+		if (device->serial_num_len == 0)
+			break;
+		amt = MIN(cdai->provsiz, cdai->bufsiz);
+		memcpy(cdai->buf, device->serial_num, amt);
+		break;
+        case CDAI_TYPE_PHYS_PATH: /* pass(4) wants this */
+                cdai->provsiz = 0;
+                break;
+	case CDAI_TYPE_MMC_PARAMS:
+		cdai->provsiz = sizeof(struct mmc_params);
+		amt = MIN(cdai->provsiz, cdai->bufsiz);
+		memcpy(cdai->buf, &device->mmc_ident_data, amt);
+		break;
+	default:
+                panic("Unknown buftype");
+		return;
+	}
+	start_ccb->ccb_h.status = CAM_REQ_CMP;
+}
+
+static void
+mmc_announce_periph(struct cam_periph *periph)
+{
+	struct	ccb_pathinq cpi;
+	struct	ccb_trans_settings cts;
+	struct	cam_path *path = periph->path;
+
+	cam_periph_assert(periph, MA_OWNED);
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_INFO,
+		  ("mmc_announce_periph: called\n"));
+
+	xpt_setup_ccb(&cts.ccb_h, path, CAM_PRIORITY_NORMAL);
+	cts.ccb_h.func_code = XPT_GET_TRAN_SETTINGS;
+	cts.type = CTS_TYPE_CURRENT_SETTINGS;
+	xpt_action((union ccb*)&cts);
+	if ((cts.ccb_h.status & CAM_STATUS_MASK) != CAM_REQ_CMP)
+		return;
+	xpt_path_inq(&cpi, periph->path);
+	printf("XPT info: CLK %04X, ...\n", cts.proto_specific.mmc.ios.clock);
+}
+
+/* This func is called per attached device :-( */
+void
+mmc_print_ident(struct mmc_params *ident_data)
+{
+        printf("Relative addr: %08x\n", ident_data->card_rca);
+        printf("Card features: <");
+        if (ident_data->card_features & CARD_FEATURE_MMC)
+                printf("MMC ");
+        if (ident_data->card_features & CARD_FEATURE_MEMORY)
+                printf("Memory ");
+        if (ident_data->card_features & CARD_FEATURE_SDHC)
+                printf("High-Capacity ");
+        if (ident_data->card_features & CARD_FEATURE_SD20)
+                printf("SD2.0-Conditions ");
+        if (ident_data->card_features & CARD_FEATURE_SDIO)
+                printf("SDIO ");
+        printf(">\n");
+
+        if (ident_data->card_features & CARD_FEATURE_MEMORY)
+                printf("Card memory OCR: %08x\n", ident_data->card_ocr);
+
+        if (ident_data->card_features & CARD_FEATURE_SDIO) {
+                printf("Card IO OCR: %08x\n", ident_data->io_ocr);
+                printf("Number of funcitions: %u\n", ident_data->sdio_func_count);
+        }
+}
+
+static void
+mmc_proto_announce(struct cam_ed *device)
+{
+	mmc_print_ident(&device->mmc_ident_data);
+}
+
+static void
+mmc_proto_denounce(struct cam_ed *device)
+{
+	mmc_print_ident(&device->mmc_ident_data);
+}
+
+static void
+mmc_proto_debug_out(union ccb *ccb)
+{
+	if (ccb->ccb_h.func_code != XPT_MMC_IO)
+		return;
+
+	CAM_DEBUG(ccb->ccb_h.path,
+	    CAM_DEBUG_CDB,("mmc_proto_debug_out\n"));
+}
+
+static periph_init_t probe_periph_init;
+
+static struct periph_driver probe_driver =
+{
+	probe_periph_init, "mmcprobe",
+	TAILQ_HEAD_INITIALIZER(probe_driver.units), /* generation */ 0,
+	CAM_PERIPH_DRV_EARLY
+};
+
+PERIPHDRIVER_DECLARE(mmcprobe, probe_driver);
+
+#define	CARD_ID_FREQUENCY 400000 /* Spec requires 400kHz max during ID phase. */
+
+static void
+probe_periph_init()
+{
+sddainit();
+}
+
+static cam_status
+mmcprobe_register(struct cam_periph *periph, void *arg)
+{
+	mmcprobe_softc *softc;
+	union ccb *request_ccb;	/* CCB representing the probe request */
+	cam_status status;
+
+	CAM_DEBUG(periph->path, CAM_DEBUG_TRACE, ("mmcprobe_register\n"));
+
+	request_ccb = (union ccb *)arg;
+	if (request_ccb == NULL) {
+		printf("mmcprobe_register: no probe CCB, "
+		       "can't register device\n");
+		return(CAM_REQ_CMP_ERR);
+	}
+
+	softc = (mmcprobe_softc *)malloc(sizeof(*softc), M_CAMXPT, M_NOWAIT);
+
+	if (softc == NULL) {
+		printf("proberegister: Unable to probe new device. "
+		       "Unable to allocate softc\n");
+		return(CAM_REQ_CMP_ERR);
+	}
+
+	softc->flags = 0;
+	softc->acmd41_count = 0;
+	periph->softc = softc;
+	softc->periph = periph;
+	softc->action = PROBE_INVALID;
+        softc->restart = 0;
+	status = cam_periph_acquire(periph);
+
+        memset(&periph->path->device->mmc_ident_data, 0, sizeof(struct mmc_params));
+	if (status != CAM_REQ_CMP) {
+		printf("proberegister: cam_periph_acquire failed (status=%d)\n",
+			status);
+		return (status);
+	}
+	CAM_DEBUG(periph->path, CAM_DEBUG_PROBE, ("Probe started\n"));
+
+	if (periph->path->device->flags & CAM_DEV_UNCONFIGURED)
+		PROBE_SET_ACTION(softc, PROBE_RESET);
+	else
+		PROBE_SET_ACTION(softc, PROBE_IDENTIFY);
+
+	/* This will kick the ball */
+	xpt_schedule(periph, CAM_PRIORITY_XPT);
+
+	return(CAM_REQ_CMP);
+}
+
+static int
+mmc_highest_voltage(uint32_t ocr)
+{
+	int i;
+
+	for (i = MMC_OCR_MAX_VOLTAGE_SHIFT;
+	    i >= MMC_OCR_MIN_VOLTAGE_SHIFT; i--)
+		if (ocr & (1 << i))
+			return (i);
+	return (-1);
+}
+
+static inline void
+init_standard_ccb(union ccb *ccb, uint32_t cmd)
+{
+	ccb->ccb_h.func_code = cmd;
+	ccb->ccb_h.flags = CAM_DIR_OUT;
+	ccb->ccb_h.retry_count = 0;
+	ccb->ccb_h.timeout = 15 * 1000;
+	ccb->ccb_h.cbfcnp = mmcprobe_done;
+}
+
+static void
+mmcprobe_start(struct cam_periph *periph, union ccb *start_ccb)
+{
+	mmcprobe_softc *softc;
+	struct cam_path *path;
+	struct ccb_mmcio *mmcio;
+	struct mtx *p_mtx = cam_periph_mtx(periph);
+	struct ccb_trans_settings_mmc *cts;
+
+	CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_PROBE, ("mmcprobe_start\n"));
+	softc = (mmcprobe_softc *)periph->softc;
+	path = start_ccb->ccb_h.path;
+	mmcio = &start_ccb->mmcio;
+	cts = &start_ccb->cts.proto_specific.mmc;
+	struct mmc_params *mmcp = &path->device->mmc_ident_data;
+
+	memset(&mmcio->cmd, 0, sizeof(struct mmc_command));
+
+	if (softc->restart) {
+		softc->restart = 0;
+		if (path->device->flags & CAM_DEV_UNCONFIGURED)
+			softc->action = PROBE_RESET;
+		else
+			softc->action = PROBE_IDENTIFY;
+
+	}
+
+	/* Here is the place where the identify fun begins */
+	switch (softc->action) {
+	case PROBE_RESET:
+		/* FALLTHROUGH */
+	case PROBE_IDENTIFY:
+		xpt_path_inq(&start_ccb->cpi, periph->path);
+
+		CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_PROBE, ("Start with PROBE_RESET\n"));
+		init_standard_ccb(start_ccb, XPT_GET_TRAN_SETTINGS);
+		xpt_action(start_ccb);
+		if(cts->ios.power_mode != power_off) {
+			init_standard_ccb(start_ccb, XPT_SET_TRAN_SETTINGS);
+			cts->ios.power_mode = power_off;
+			cts->ios_valid = MMC_PM;
+			xpt_action(start_ccb);
+			mtx_sleep(periph, p_mtx, 0, "mmcios", 100);
+		}
+
+		/* mmc_power_up */
+		/* Get the host OCR */
+		init_standard_ccb(start_ccb, XPT_GET_TRAN_SETTINGS);
+		xpt_action(start_ccb);
+
+		uint32_t hv = mmc_highest_voltage(cts->host_ocr);
+		init_standard_ccb(start_ccb, XPT_SET_TRAN_SETTINGS);
+		cts->ios.vdd = hv;
+		cts->ios.bus_mode = opendrain;
+		cts->ios.chip_select = cs_dontcare;
+		cts->ios.power_mode = power_up;
+		cts->ios.bus_width = bus_width_1;
+		cts->ios.clock = 0;
+		cts->ios_valid = MMC_VDD | MMC_PM | MMC_BM |
+			MMC_CS | MMC_BW | MMC_CLK;
+		xpt_action(start_ccb);
+		mtx_sleep(periph, p_mtx, 0, "mmcios", 100);
+
+		init_standard_ccb(start_ccb, XPT_SET_TRAN_SETTINGS);
+		cts->ios.power_mode = power_on;
+		cts->ios.clock = CARD_ID_FREQUENCY;
+		cts->ios.timing = bus_timing_normal;
+		cts->ios_valid = MMC_PM | MMC_CLK | MMC_BT;
+		xpt_action(start_ccb);
+		mtx_sleep(periph, p_mtx, 0, "mmcios", 100);
+		/* End for mmc_power_on */
+
+		/* Begin mmc_idle_cards() */
+		init_standard_ccb(start_ccb, XPT_SET_TRAN_SETTINGS);
+		cts->ios.chip_select = cs_high;
+		cts->ios_valid = MMC_CS;
+		xpt_action(start_ccb);
+		mtx_sleep(periph, p_mtx, 0, "mmcios", 1);
+
+		CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_PROBE, ("Send first XPT_MMC_IO\n"));
+		init_standard_ccb(start_ccb, XPT_MMC_IO);
+		mmcio->cmd.opcode = MMC_GO_IDLE_STATE; /* CMD 0 */
+		mmcio->cmd.arg = 0;
+		mmcio->cmd.flags = MMC_RSP_NONE | MMC_CMD_BC;
+		mmcio->cmd.data = NULL;
+		mmcio->stop.opcode = 0;
+
+		/* XXX Reset I/O portion as well */
+		break;
+	case PROBE_SDIO_RESET:
+		CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+			  ("Start with PROBE_SDIO_RESET\n"));
+		uint32_t mmc_arg = SD_IO_RW_ADR(SD_IO_CCCR_CTL)
+			| SD_IO_RW_DAT(CCCR_CTL_RES) | SD_IO_RW_WR | SD_IO_RW_RAW;
+		cam_fill_mmcio(&start_ccb->mmcio,
+			       /*retries*/ 0,
+			       /*cbfcnp*/ mmcprobe_done,
+			       /*flags*/ CAM_DIR_NONE,
+			       /*mmc_opcode*/ SD_IO_RW_DIRECT,
+			       /*mmc_arg*/ mmc_arg,
+			       /*mmc_flags*/ MMC_RSP_R5 | MMC_CMD_AC,
+			       /*mmc_data*/ NULL,
+			       /*timeout*/ 1000);
+		break;
+	case PROBE_SEND_IF_COND:
+		CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+			  ("Start with PROBE_SEND_IF_COND\n"));
+		init_standard_ccb(start_ccb, XPT_MMC_IO);
+		mmcio->cmd.opcode = SD_SEND_IF_COND; /* CMD 8 */
+		mmcio->cmd.arg = (1 << 8) + 0xAA;
+		mmcio->cmd.flags = MMC_RSP_R7 | MMC_CMD_BCR;
+		mmcio->stop.opcode = 0;
+		break;
+
+	case PROBE_SDIO_INIT:
+		CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+			  ("Start with PROBE_SDIO_INIT\n"));
+		init_standard_ccb(start_ccb, XPT_MMC_IO);
+		mmcio->cmd.opcode = IO_SEND_OP_COND; /* CMD 5 */
+		mmcio->cmd.arg = mmcp->io_ocr;
+		mmcio->cmd.flags = MMC_RSP_R4;
+		mmcio->stop.opcode = 0;
+		break;
+
+	case PROBE_MMC_INIT:
+		CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+			  ("Start with PROBE_MMC_INIT\n"));
+		init_standard_ccb(start_ccb, XPT_MMC_IO);
+		mmcio->cmd.opcode = MMC_SEND_OP_COND; /* CMD 1 */
+		mmcio->cmd.arg = MMC_OCR_CCS | mmcp->card_ocr; /* CCS + ocr */;
+		mmcio->cmd.flags = MMC_RSP_R3 | MMC_CMD_BCR;
+		mmcio->stop.opcode = 0;
+		break;
+
+	case PROBE_SEND_APP_OP_COND:
+		init_standard_ccb(start_ccb, XPT_MMC_IO);
+		if (softc->flags & PROBE_FLAG_ACMD_SENT) {
+			mmcio->cmd.opcode = ACMD_SD_SEND_OP_COND; /* CMD 41 */
+			/*
+			 * We set CCS bit because we do support SDHC cards.
+			 * XXX: Don't set CCS if no response to CMD8.
+			 */
+			uint32_t cmd_arg = MMC_OCR_CCS | mmcp->card_ocr; /* CCS + ocr */
+			if (softc->acmd41_count < 10 && mmcp->card_ocr != 0 )
+				cmd_arg |= MMC_OCR_S18R;
+			mmcio->cmd.arg = cmd_arg;
+			mmcio->cmd.flags = MMC_RSP_R3 | MMC_CMD_BCR;
+			softc->acmd41_count++;
+		} else {
+			mmcio->cmd.opcode = MMC_APP_CMD; /* CMD 55 */
+			mmcio->cmd.arg = 0; /* rca << 16 */
+			mmcio->cmd.flags = MMC_RSP_R1 | MMC_CMD_AC;
+		}
+		mmcio->stop.opcode = 0;
+		break;
+
+	case PROBE_GET_CID: /* XXX move to mmc_da */
+		init_standard_ccb(start_ccb, XPT_MMC_IO);
+		mmcio->cmd.opcode = MMC_ALL_SEND_CID;
+		mmcio->cmd.arg = 0;
+		mmcio->cmd.flags = MMC_RSP_R2 | MMC_CMD_BCR;
+		mmcio->stop.opcode = 0;
+		break;
+	case PROBE_SEND_RELATIVE_ADDR:
+		init_standard_ccb(start_ccb, XPT_MMC_IO);
+		mmcio->cmd.opcode = SD_SEND_RELATIVE_ADDR;
+		mmcio->cmd.arg = 0;
+		mmcio->cmd.flags = MMC_RSP_R6 | MMC_CMD_BCR;
+		mmcio->stop.opcode = 0;
+		break;
+	case PROBE_MMC_SET_RELATIVE_ADDR:
+		init_standard_ccb(start_ccb, XPT_MMC_IO);
+		mmcio->cmd.opcode = MMC_SET_RELATIVE_ADDR;
+		mmcio->cmd.arg = MMC_PROPOSED_RCA << 16;
+		mmcio->cmd.flags = MMC_RSP_R1 | MMC_CMD_AC;
+		mmcio->stop.opcode = 0;
+		break;
+	case PROBE_SELECT_CARD:
+		init_standard_ccb(start_ccb, XPT_MMC_IO);
+		mmcio->cmd.opcode = MMC_SELECT_CARD;
+		mmcio->cmd.arg = (uint32_t)path->device->mmc_ident_data.card_rca << 16;
+		mmcio->cmd.flags = MMC_RSP_R1B | MMC_CMD_AC;
+		mmcio->stop.opcode = 0;
+		break;
+	case PROBE_GET_CSD: /* XXX move to mmc_da */
+		init_standard_ccb(start_ccb, XPT_MMC_IO);
+		mmcio->cmd.opcode = MMC_SEND_CSD;
+		mmcio->cmd.arg = (uint32_t)path->device->mmc_ident_data.card_rca << 16;
+		mmcio->cmd.flags = MMC_RSP_R2 | MMC_CMD_BCR;
+		mmcio->stop.opcode = 0;
+		break;
+	case PROBE_DONE:
+		CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_PROBE, ("Start with PROBE_DONE\n"));
+		init_standard_ccb(start_ccb, XPT_SET_TRAN_SETTINGS);
+		cts->ios.bus_mode = pushpull;
+		cts->ios_valid = MMC_BM;
+		xpt_action(start_ccb);
+		return;
+		/* NOTREACHED */
+		break;
+	case PROBE_INVALID:
+		break;
+	default:
+		CAM_DEBUG(start_ccb->ccb_h.path, CAM_DEBUG_PROBE, ("probestart: invalid action state 0x%x\n", softc->action));
+		panic("default: case in mmc_probe_start()");
+	}
+
+	start_ccb->ccb_h.flags |= CAM_DEV_QFREEZE;
+	xpt_action(start_ccb);
+}
+
+static void mmcprobe_cleanup(struct cam_periph *periph)
+{
+	free(periph->softc, M_CAMXPT);
+}
+
+static void
+mmcprobe_done(struct cam_periph *periph, union ccb *done_ccb)
+{
+	mmcprobe_softc *softc;
+	struct cam_path *path;
+
+	int err;
+	struct ccb_mmcio *mmcio;
+	u_int32_t  priority;
+
+	CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE, ("mmcprobe_done\n"));
+	softc = (mmcprobe_softc *)periph->softc;
+	path = done_ccb->ccb_h.path;
+	priority = done_ccb->ccb_h.pinfo.priority;
+
+	switch (softc->action) {
+	case PROBE_RESET:
+		/* FALLTHROUGH */
+	case PROBE_IDENTIFY:
+	{
+		printf("Starting completion of PROBE_RESET\n");
+		CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE, ("done with PROBE_RESET\n"));
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+
+		if (err != MMC_ERR_NONE) {
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+				  ("GO_IDLE_STATE failed with error %d\n",
+				   err));
+
+			/* There was a device there, but now it's gone... */
+			if ((path->device->flags & CAM_DEV_UNCONFIGURED) == 0) {
+				xpt_async(AC_LOST_DEVICE, path, NULL);
+			}
+			PROBE_SET_ACTION(softc, PROBE_INVALID);
+			break;
+		}
+		path->device->protocol = PROTO_MMCSD;
+		PROBE_SET_ACTION(softc, PROBE_SEND_IF_COND);
+		break;
+	}
+	case PROBE_SEND_IF_COND:
+	{
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+		struct mmc_params *mmcp = &path->device->mmc_ident_data;
+
+		if (err != MMC_ERR_NONE || mmcio->cmd.resp[0] != 0x1AA) {
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+				  ("IF_COND: error %d, pattern %08x\n",
+				   err, mmcio->cmd.resp[0]));
+		} else {
+			mmcp->card_features |= CARD_FEATURE_SD20;
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+				  ("SD 2.0 interface conditions: OK\n"));
+
+		}
+                PROBE_SET_ACTION(softc, PROBE_SDIO_RESET);
+		break;
+	}
+        case PROBE_SDIO_RESET:
+        {
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+
+                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                          ("SDIO_RESET: error %d, CCCR CTL register: %08x\n",
+                           err, mmcio->cmd.resp[0]));
+                PROBE_SET_ACTION(softc, PROBE_SDIO_INIT);
+                break;
+        }
+        case PROBE_SDIO_INIT:
+        {
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+                struct mmc_params *mmcp = &path->device->mmc_ident_data;
+
+                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                          ("SDIO_INIT: error %d, %08x %08x %08x %08x\n",
+                           err, mmcio->cmd.resp[0],
+                           mmcio->cmd.resp[1],
+                           mmcio->cmd.resp[2],
+                           mmcio->cmd.resp[3]));
+
+                /*
+                 * Error here means that this card is not SDIO,
+                 * so proceed with memory init as if nothing has happened
+                 */
+		if (err != MMC_ERR_NONE) {
+                        PROBE_SET_ACTION(softc, PROBE_SEND_APP_OP_COND);
+                        break;
+		}
+                mmcp->card_features |= CARD_FEATURE_SDIO;
+                uint32_t ioifcond = mmcio->cmd.resp[0];
+                uint32_t io_ocr = ioifcond & R4_IO_OCR_MASK;
+
+                mmcp->sdio_func_count = R4_IO_NUM_FUNCTIONS(ioifcond);
+                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                          ("SDIO card: %d functions\n", mmcp->sdio_func_count));
+                if (io_ocr == 0) {
+                    CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                              ("SDIO OCR invalid?!\n"));
+                    break; /* Retry */
+                }
+
+                if (io_ocr != 0 && mmcp->io_ocr == 0) {
+                        mmcp->io_ocr = io_ocr;
+                        break; /* Retry, this time with non-0 OCR */
+                }
+                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                          ("SDIO OCR: %08x\n", mmcp->io_ocr));
+
+                if (ioifcond & R4_IO_MEM_PRESENT) {
+                        /* Combo card -- proceed to memory initialization */
+                        PROBE_SET_ACTION(softc, PROBE_SEND_APP_OP_COND);
+                } else {
+                        /* No memory portion -- get RCA and select card */
+                        PROBE_SET_ACTION(softc, PROBE_SEND_RELATIVE_ADDR);
+                }
+                break;
+        }
+        case PROBE_MMC_INIT:
+        {
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+                struct mmc_params *mmcp = &path->device->mmc_ident_data;
+
+		if (err != MMC_ERR_NONE) {
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+				  ("MMC_INIT: error %d, resp %08x\n",
+				   err, mmcio->cmd.resp[0]));
+			PROBE_SET_ACTION(softc, PROBE_INVALID);
+                        break;
+                }
+                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                          ("MMC card, OCR %08x\n", mmcio->cmd.resp[0]));
+
+                if (mmcp->card_ocr == 0) {
+                        /* We haven't sent the OCR to the card yet -- do it */
+                        mmcp->card_ocr = mmcio->cmd.resp[0];
+                        CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                                  ("-> sending OCR to card\n"));
+                        break;
+                }
+
+                if (!(mmcio->cmd.resp[0] & MMC_OCR_CARD_BUSY)) {
+                        CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                                  ("Card is still powering up\n"));
+                        break;
+                }
+
+                mmcp->card_features |= CARD_FEATURE_MMC | CARD_FEATURE_MEMORY;
+                PROBE_SET_ACTION(softc, PROBE_GET_CID);
+                break;
+        }
+	case PROBE_SEND_APP_OP_COND:
+	{
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+
+		if (err != MMC_ERR_NONE) {
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+				  ("APP_OP_COND: error %d, resp %08x\n",
+				   err, mmcio->cmd.resp[0]));
+			PROBE_SET_ACTION(softc, PROBE_MMC_INIT);
+                        break;
+                }
+
+                if (!(softc->flags & PROBE_FLAG_ACMD_SENT)) {
+                        /* Don't change the state */
+                        softc->flags |= PROBE_FLAG_ACMD_SENT;
+                        break;
+                }
+
+                softc->flags &= ~PROBE_FLAG_ACMD_SENT;
+                if ((mmcio->cmd.resp[0] & MMC_OCR_CARD_BUSY) ||
+                    (mmcio->cmd.arg & MMC_OCR_VOLTAGE) == 0) {
+                        struct mmc_params *mmcp = &path->device->mmc_ident_data;
+                        CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                                  ("Card OCR: %08x\n",  mmcio->cmd.resp[0]));
+                        if (mmcp->card_ocr == 0) {
+                                mmcp->card_ocr = mmcio->cmd.resp[0];
+                                /* Now when we know OCR that we want -- send it to card */
+                                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                                          ("-> sending OCR to card\n"));
+                        } else {
+                                /* We already know the OCR and despite of that we
+                                 * are processing the answer to ACMD41 -> move on
+                                 */
+                                PROBE_SET_ACTION(softc, PROBE_GET_CID);
+                        }
+                        /* Getting an answer to ACMD41 means the card has memory */
+                        mmcp->card_features |= CARD_FEATURE_MEMORY;
+
+                        /* Standard capacity vs High Capacity memory card */
+                        if (mmcio->cmd.resp[0] & MMC_OCR_CCS) {
+                                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                                          ("Card is SDHC\n"));
+                                mmcp->card_features |= CARD_FEATURE_SDHC;
+                        }
+
+			/* Whether the card supports 1.8V signaling */
+			if (mmcio->cmd.resp[0] & MMC_OCR_S18A) {
+				CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+					  ("Card supports 1.8V signaling\n"));
+				mmcp->card_features |= CARD_FEATURE_18V;
+			}
+		} else {
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+				  ("Card not ready: %08x\n",  mmcio->cmd.resp[0]));
+			/* Send CMD55+ACMD41 once again  */
+			PROBE_SET_ACTION(softc, PROBE_SEND_APP_OP_COND);
+		}
+
+                break;
+	}
+        case PROBE_GET_CID: /* XXX move to mmc_da */
+        {
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+
+		if (err != MMC_ERR_NONE) {
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+				  ("PROBE_GET_CID: error %d\n", err));
+			PROBE_SET_ACTION(softc, PROBE_INVALID);
+                        break;
+                }
+
+                struct mmc_params *mmcp = &path->device->mmc_ident_data;
+                memcpy(mmcp->card_cid, mmcio->cmd.resp, 4 * sizeof(uint32_t));
+                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                          ("CID %08x%08x%08x%08x\n",
+                           mmcp->card_cid[0],
+                           mmcp->card_cid[1],
+                           mmcp->card_cid[2],
+                           mmcp->card_cid[3]));
+		if (mmcp->card_features & CARD_FEATURE_MMC)
+			PROBE_SET_ACTION(softc, PROBE_MMC_SET_RELATIVE_ADDR);
+		else
+			PROBE_SET_ACTION(softc, PROBE_SEND_RELATIVE_ADDR);
+                break;
+        }
+        case PROBE_SEND_RELATIVE_ADDR: {
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+                struct mmc_params *mmcp = &path->device->mmc_ident_data;
+                uint16_t rca = mmcio->cmd.resp[0] >> 16;
+                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                          ("Card published RCA: %u\n", rca));
+                path->device->mmc_ident_data.card_rca = rca;
+		if (err != MMC_ERR_NONE) {
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+				  ("PROBE_SEND_RELATIVE_ADDR: error %d\n", err));
+			PROBE_SET_ACTION(softc, PROBE_INVALID);
+                        break;
+                }
+
+                /* If memory is present, get CSD, otherwise select card */
+                if (mmcp->card_features & CARD_FEATURE_MEMORY)
+                        PROBE_SET_ACTION(softc, PROBE_GET_CSD);
+                else
+                        PROBE_SET_ACTION(softc, PROBE_SELECT_CARD);
+		break;
+        }
+	case PROBE_MMC_SET_RELATIVE_ADDR:
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+		if (err != MMC_ERR_NONE) {
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+			    ("PROBE_MMC_SET_RELATIVE_ADDR: error %d\n", err));
+			PROBE_SET_ACTION(softc, PROBE_INVALID);
+			break;
+		}
+		path->device->mmc_ident_data.card_rca = MMC_PROPOSED_RCA;
+		PROBE_SET_ACTION(softc, PROBE_GET_CSD);
+		break;
+        case PROBE_GET_CSD: {
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+
+		if (err != MMC_ERR_NONE) {
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+				  ("PROBE_GET_CSD: error %d\n", err));
+			PROBE_SET_ACTION(softc, PROBE_INVALID);
+                        break;
+                }
+
+                struct mmc_params *mmcp = &path->device->mmc_ident_data;
+                memcpy(mmcp->card_csd, mmcio->cmd.resp, 4 * sizeof(uint32_t));
+                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+                          ("CSD %08x%08x%08x%08x\n",
+                           mmcp->card_csd[0],
+                           mmcp->card_csd[1],
+                           mmcp->card_csd[2],
+                           mmcp->card_csd[3]));
+                PROBE_SET_ACTION(softc, PROBE_SELECT_CARD);
+                break;
+        }
+        case PROBE_SELECT_CARD: {
+		mmcio = &done_ccb->mmcio;
+		err = mmcio->cmd.error;
+		if (err != MMC_ERR_NONE) {
+			CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+				  ("PROBE_SEND_RELATIVE_ADDR: error %d\n", err));
+			PROBE_SET_ACTION(softc, PROBE_INVALID);
+                        break;
+                }
+
+		PROBE_SET_ACTION(softc, PROBE_DONE);
+                break;
+        }
+	default:
+		CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+			  ("mmc_probedone: invalid action state 0x%x\n", softc->action));
+		panic("default: case in mmc_probe_done()");
+	}
+
+        if (softc->action == PROBE_INVALID &&
+            (path->device->flags & CAM_DEV_UNCONFIGURED) == 0) {
+                CAM_DEBUG(done_ccb->ccb_h.path, CAM_DEBUG_PROBE,
+			  ("mmc_probedone: Should send AC_LOST_DEVICE but won't for now\n"));
+                //xpt_async(AC_LOST_DEVICE, path, NULL);
+        }
+
+	xpt_release_ccb(done_ccb);
+        if (softc->action != PROBE_INVALID)
+                xpt_schedule(periph, priority);
+	/* Drop freeze taken due to CAM_DEV_QFREEZE flag set. */
+	int frozen = cam_release_devq(path, 0, 0, 0, FALSE);
+        printf("mmc_probedone: remaining freezecnt %d\n", frozen);
+
+	if (softc->action == PROBE_DONE) {
+                /* Notify the system that the device is found! */
+		if (periph->path->device->flags & CAM_DEV_UNCONFIGURED) {
+			path->device->flags &= ~CAM_DEV_UNCONFIGURED;
+			xpt_acquire_device(path->device);
+			done_ccb->ccb_h.func_code = XPT_GDEV_TYPE;
+			xpt_action(done_ccb);
+			xpt_async(AC_FOUND_DEVICE, path, done_ccb);
+		}
+	}
+        if (softc->action == PROBE_DONE || softc->action == PROBE_INVALID) {
+                cam_periph_invalidate(periph);
+                cam_periph_release_locked(periph);
+        }
+}
diff --git a/freebsd/sys/cam/scsi/scsi_all.c b/freebsd/sys/cam/scsi/scsi_all.c
index 1c3a8c3..30dc4f9 100644
--- a/freebsd/sys/cam/scsi/scsi_all.c
+++ b/freebsd/sys/cam/scsi/scsi_all.c
@@ -38,7 +38,7 @@ __FBSDID("$FreeBSD$");
 
 #ifdef _KERNEL
 #ifndef __rtems__
-#include <opt_scsi.h>
+#include <rtems/bsd/local/opt_scsi.h>
 #else /* __rtems__ */
 #include <rtems/bsd/local/opt_scsi.h>
 #endif /* __rtems__ */
diff --git a/freebsd/sys/dev/mmc/bridge.h b/freebsd/sys/dev/mmc/bridge.h
index a780ffa..502f433 100644
--- a/freebsd/sys/dev/mmc/bridge.h
+++ b/freebsd/sys/dev/mmc/bridge.h
@@ -174,6 +174,7 @@ struct mmc_host {
 	struct mmc_ios ios;	/* Current state of the host */
 };
 
+#ifdef _KERNEL
 extern driver_t   mmc_driver;
 extern devclass_t mmc_devclass;
 
@@ -184,5 +185,6 @@ extern devclass_t mmc_devclass;
     MODULE_DEPEND(name, mmc, MMC_VERSION, MMC_VERSION, MMC_VERSION);
 #define	MMC_DEPEND(name)						\
     MODULE_DEPEND(name, mmc, MMC_VERSION, MMC_VERSION, MMC_VERSION);
+#endif /* _KERNEL */
 
 #endif /* DEV_MMC_BRIDGE_H */
diff --git a/freebsd/sys/dev/mmc/mmcbrvar.h b/freebsd/sys/dev/mmc/mmcbrvar.h
index 77c304b..03412ea 100644
--- a/freebsd/sys/dev/mmc/mmcbrvar.h
+++ b/freebsd/sys/dev/mmc/mmcbrvar.h
@@ -56,7 +56,6 @@
 #define	DEV_MMC_MMCBRVAR_H
 
 #include <dev/mmc/mmcreg.h>
-
 #include <rtems/bsd/local/mmcbr_if.h>
 
 enum mmcbr_device_ivars {
diff --git a/freebsd/sys/dev/mmc/mmcreg.h b/freebsd/sys/dev/mmc/mmcreg.h
index 39680ad..c416c19 100644
--- a/freebsd/sys/dev/mmc/mmcreg.h
+++ b/freebsd/sys/dev/mmc/mmcreg.h
@@ -1,6 +1,7 @@
 /*-
  * Copyright (c) 2006 M. Warner Losh.  All rights reserved.
  * Copyright (c) 2017 Marius Strobl <marius@FreeBSD.org>
+ * Copyright (c) 2015-2016 Ilya Bakulin <kibab@FreeBSD.org>
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions
@@ -159,6 +160,34 @@ struct mmc_command {
 #define	R1_STATE_PRG	7
 #define	R1_STATE_DIS	8
 
+/* R4 response (SDIO) */
+#define R4_IO_NUM_FUNCTIONS(ocr)	(((ocr) >> 28) & 0x3)
+#define R4_IO_MEM_PRESENT		(0x1<<27)
+#define R4_IO_OCR_MASK			0x00fffff0
+
+/*
+ * R5 responses
+ *
+ * Types (per SD 2.0 standard)
+ *e : error bit
+ *s : status bit
+ *r : detected and set for the actual command response
+ *x : Detected and set during command execution.  The host can get
+ *    the status by issuing a command with R1 response.
+ *
+ * Clear Condition (per SD 2.0 standard)
+ *a : according to the card current state.
+ *b : always related to the previous command.  reception of a valid
+ *    command will clear it (with a delay of one command).
+ *c : clear by read
+ */
+#define R5_COM_CRC_ERROR		(1u << 15)/* er, b */
+#define R5_ILLEGAL_COMMAND		(1u << 14)/* er, b */
+#define R5_IO_CURRENT_STATE_MASK	(3u << 12)/* s, b */
+#define R5_IO_CURRENT_STATE(x) 		(((x) & R5_IO_CURRENT_STATE_MASK) >> 12)
+#define R5_ERROR			(1u << 11)/* erx, c */
+#define R5_FUNCTION_NUMBER		(1u << 9)/* er, c */
+#define R5_OUT_OF_RANGE			(1u << 8)/* er, c */
 struct mmc_data {
 	size_t len;		/* size of the data */
 	size_t xfer_len;
@@ -194,6 +223,7 @@ struct mmc_request {
 #define	SD_SEND_RELATIVE_ADDR	3
 #define	MMC_SET_DSR		4
 #define	MMC_SLEEP_AWAKE		5
+#define IO_SEND_OP_COND		5
 #define	MMC_SWITCH_FUNC		6
 #define	 MMC_SWITCH_FUNC_CMDS	 0
 #define	 MMC_SWITCH_FUNC_SET	 1
@@ -276,7 +306,31 @@ struct mmc_request {
 
 /* Class 9: I/O cards (sd) */
 #define	SD_IO_RW_DIRECT		52
+/* CMD52 arguments */
+#define  SD_ARG_CMD52_READ		(0<<31)
+#define  SD_ARG_CMD52_WRITE		(1<<31)
+#define  SD_ARG_CMD52_FUNC_SHIFT		28
+#define  SD_ARG_CMD52_FUNC_MASK		0x7
+#define  SD_ARG_CMD52_EXCHANGE		(1<<27)
+#define  SD_ARG_CMD52_REG_SHIFT		9
+#define  SD_ARG_CMD52_REG_MASK		0x1ffff
+#define  SD_ARG_CMD52_DATA_SHIFT		0
+#define  SD_ARG_CMD52_DATA_MASK		0xff
+#define  SD_R5_DATA(resp)		((resp)[0] & 0xff)
+
 #define	SD_IO_RW_EXTENDED	53
+/* CMD53 arguments */
+#define  SD_ARG_CMD53_READ		(0<<31)
+#define  SD_ARG_CMD53_WRITE		(1<<31)
+#define  SD_ARG_CMD53_FUNC_SHIFT		28
+#define  SD_ARG_CMD53_FUNC_MASK		0x7
+#define  SD_ARG_CMD53_BLOCK_MODE		(1<<27)
+#define  SD_ARG_CMD53_INCREMENT		(1<<26)
+#define  SD_ARG_CMD53_REG_SHIFT		9
+#define  SD_ARG_CMD53_REG_MASK		0x1ffff
+#define  SD_ARG_CMD53_LENGTH_SHIFT	0
+#define  SD_ARG_CMD53_LENGTH_MASK	0x1ff
+#define  SD_ARG_CMD53_LENGTH_MAX		64 /* XXX should be 511? */
 
 /* Class 10: Switch function commands */
 #define	SD_SWITCH_FUNC		6
@@ -447,6 +501,54 @@ struct mmc_request {
 /* Specifications require 400 kHz max. during ID phase. */
 #define	SD_MMC_CARD_ID_FREQUENCY	400000
 
+/*
+ * SDIO Direct & Extended I/O
+ */
+#define SD_IO_RW_WR		(1u << 31)
+#define SD_IO_RW_FUNC(x)	(((x) & 0x7) << 28)
+#define SD_IO_RW_RAW		(1u << 27)
+#define SD_IO_RW_INCR		(1u << 26)
+#define SD_IO_RW_ADR(x)		(((x) & 0x1FFFF) << 9)
+#define SD_IO_RW_DAT(x)		(((x) & 0xFF) << 0)
+#define SD_IO_RW_LEN(x)		(((x) & 0xFF) << 0)
+
+#define SD_IOE_RW_LEN(x)	(((x) & 0x1FF) << 0)
+#define SD_IOE_RW_BLK		(1u << 27)
+
+/* Card Common Control Registers (CCCR) */
+#define SD_IO_CCCR_START		0x00000
+#define SD_IO_CCCR_SIZE			0x100
+#define SD_IO_CCCR_FN_ENABLE		0x02
+#define SD_IO_CCCR_FN_READY		0x03
+#define SD_IO_CCCR_INT_ENABLE		0x04
+#define SD_IO_CCCR_INT_PENDING		0x05
+#define SD_IO_CCCR_CTL			0x06
+#define  CCCR_CTL_RES			(1<<3)
+#define SD_IO_CCCR_BUS_WIDTH		0x07
+#define  CCCR_BUS_WIDTH_4		(1<<1)
+#define  CCCR_BUS_WIDTH_1		(1<<0)
+#define SD_IO_CCCR_CARDCAP		0x08
+#define SD_IO_CCCR_CISPTR		0x09 /* XXX 9-10, 10-11, or 9-12 */
+
+/* Function Basic Registers (FBR) */
+#define SD_IO_FBR_START			0x00100
+#define SD_IO_FBR_SIZE			0x00700
+
+/* Card Information Structure (CIS) */
+#define SD_IO_CIS_START			0x01000
+#define SD_IO_CIS_SIZE			0x17000
+
+/* CIS tuple codes (based on PC Card 16) */
+#define SD_IO_CISTPL_VERS_1		0x15
+#define SD_IO_CISTPL_MANFID		0x20
+#define SD_IO_CISTPL_FUNCID		0x21
+#define SD_IO_CISTPL_FUNCE		0x22
+#define SD_IO_CISTPL_END		0xff
+
+/* CISTPL_FUNCID codes */
+/* OpenBSD incorrectly defines 0x0c as FUNCTION_WLAN */
+/* #define SDMMC_FUNCTION_WLAN		0x0c */
+
 /* OCR bits */
 
 /*
diff --git a/freebsd/sys/dev/sdhci/fsl_sdhci.c b/freebsd/sys/dev/sdhci/fsl_sdhci.c
new file mode 100644
index 0000000..4a6b394
--- /dev/null
+++ b/freebsd/sys/dev/sdhci/fsl_sdhci.c
@@ -0,0 +1,998 @@
+#include <machine/rtems-bsd-kernel-space.h>
+
+/*-
+ * Copyright (c) 2013 Ian Lepore <ian@freebsd.org>
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ */
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+/*
+ * SDHCI driver glue for Freescale i.MX SoC and QorIQ families.
+ *
+ * This supports both eSDHC (earlier SoCs) and uSDHC (more recent SoCs).
+ */
+
+#include <rtems/bsd/local/opt_mmccam.h>
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/types.h>
+#include <sys/bus.h>
+#include <sys/callout.h>
+#include <sys/kernel.h>
+#include <sys/libkern.h>
+#include <sys/lock.h>
+#include <sys/malloc.h>
+#include <sys/module.h>
+#include <sys/mutex.h>
+#include <rtems/bsd/sys/resource.h>
+#include <sys/rman.h>
+#include <sys/sysctl.h>
+#include <sys/taskqueue.h>
+#include <sys/time.h>
+
+#include <machine/bus.h>
+#include <machine/resource.h>
+#ifdef __arm__
+#include <machine/intr.h>
+
+#include <arm/freescale/imx/imx_ccmvar.h>
+#endif
+
+#ifdef __powerpc__
+#include <powerpc/mpc85xx/mpc85xx.h>
+#endif
+
+#include <dev/gpio/gpiobusvar.h>
+
+#include <dev/ofw/ofw_bus.h>
+#include <dev/ofw/ofw_bus_subr.h>
+
+#include <dev/mmc/bridge.h>
+
+#include <dev/sdhci/sdhci.h>
+#include <dev/sdhci/sdhci_fdt_gpio.h>
+
+#include <rtems/bsd/local/mmcbr_if.h>
+#include <rtems/bsd/local/sdhci_if.h>
+
+struct fsl_sdhci_softc {
+	device_t		dev;
+	struct resource *	mem_res;
+	struct resource *	irq_res;
+	void *			intr_cookie;
+	struct sdhci_slot	slot;
+	struct callout		r1bfix_callout;
+	sbintime_t		r1bfix_timeout_at;
+	struct sdhci_fdt_gpio * gpio;
+	uint32_t		baseclk_hz;
+	uint32_t		cmd_and_mode;
+	uint32_t		r1bfix_intmask;
+	uint16_t		sdclockreg_freq_bits;
+	uint8_t			r1bfix_type;
+	uint8_t			hwtype;
+};
+
+#define	R1BFIX_NONE	0	/* No fix needed at next interrupt. */
+#define	R1BFIX_NODATA	1	/* Synthesize DATA_END for R1B w/o data. */
+#define	R1BFIX_AC12	2	/* Wait for busy after auto command 12. */
+
+#define	HWTYPE_NONE	0	/* Hardware not recognized/supported. */
+#define	HWTYPE_ESDHC	1	/* fsl5x and earlier. */
+#define	HWTYPE_USDHC	2	/* fsl6. */
+
+/*
+ * Freescale-specific registers, or in some cases the layout of bits within the
+ * sdhci-defined register is different on Freescale.  These names all begin with
+ * SDHC_ (not SDHCI_).
+ */
+
+#define	SDHC_WTMK_LVL		0x44	/* Watermark Level register. */
+#define	USDHC_MIX_CONTROL	0x48	/* Mix(ed) Control register. */
+#define	SDHC_VEND_SPEC		0xC0	/* Vendor-specific register. */
+#define	 SDHC_VEND_FRC_SDCLK_ON	(1 <<  8)
+#define	 SDHC_VEND_IPGEN	(1 << 11)
+#define	 SDHC_VEND_HCKEN	(1 << 12)
+#define	 SDHC_VEND_PEREN	(1 << 13)
+
+#define	SDHC_PRES_STATE		0x24
+#define	  SDHC_PRES_CIHB	  (1 <<  0)
+#define	  SDHC_PRES_CDIHB	  (1 <<  1)
+#define	  SDHC_PRES_DLA		  (1 <<  2)
+#define	  SDHC_PRES_SDSTB	  (1 <<  3)
+#define	  SDHC_PRES_IPGOFF	  (1 <<  4)
+#define	  SDHC_PRES_HCKOFF	  (1 <<  5)
+#define	  SDHC_PRES_PEROFF	  (1 <<  6)
+#define	  SDHC_PRES_SDOFF	  (1 <<  7)
+#define	  SDHC_PRES_WTA		  (1 <<  8)
+#define	  SDHC_PRES_RTA		  (1 <<  9)
+#define	  SDHC_PRES_BWEN	  (1 << 10)
+#define	  SDHC_PRES_BREN	  (1 << 11)
+#define	  SDHC_PRES_RTR		  (1 << 12)
+#define	  SDHC_PRES_CINST	  (1 << 16)
+#define	  SDHC_PRES_CDPL	  (1 << 18)
+#define	  SDHC_PRES_WPSPL	  (1 << 19)
+#define	  SDHC_PRES_CLSL	  (1 << 23)
+#define	  SDHC_PRES_DLSL_SHIFT	  24
+#define	  SDHC_PRES_DLSL_MASK	  (0xffU << SDHC_PRES_DLSL_SHIFT)
+
+#define	SDHC_PROT_CTRL		0x28
+#define	 SDHC_PROT_LED		(1 << 0)
+#define	 SDHC_PROT_WIDTH_1BIT	(0 << 1)
+#define	 SDHC_PROT_WIDTH_4BIT	(1 << 1)
+#define	 SDHC_PROT_WIDTH_8BIT	(2 << 1)
+#define	 SDHC_PROT_WIDTH_MASK	(3 << 1)
+#define	 SDHC_PROT_D3CD		(1 << 3)
+#define	 SDHC_PROT_EMODE_BIG	(0 << 4)
+#define	 SDHC_PROT_EMODE_HALF	(1 << 4)
+#define	 SDHC_PROT_EMODE_LITTLE	(2 << 4)
+#define	 SDHC_PROT_EMODE_MASK	(3 << 4)
+#define	 SDHC_PROT_SDMA		(0 << 8)
+#define	 SDHC_PROT_ADMA1	(1 << 8)
+#define	 SDHC_PROT_ADMA2	(2 << 8)
+#define	 SDHC_PROT_ADMA264	(3 << 8)
+#define	 SDHC_PROT_DMA_MASK	(3 << 8)
+#define	 SDHC_PROT_CDTL		(1 << 6)
+#define	 SDHC_PROT_CDSS		(1 << 7)
+
+#define	SDHC_SYS_CTRL		0x2c
+
+/*
+ * The clock enable bits exist in different registers for ESDHC vs USDHC, but
+ * they are the same bits in both cases.  The divisor values go into the
+ * standard sdhci clock register, but in different bit positions and meanings
+   than the sdhci spec values.
+ */
+#define	SDHC_CLK_IPGEN		(1 << 0)
+#define	SDHC_CLK_HCKEN		(1 << 1)
+#define	SDHC_CLK_PEREN		(1 << 2)
+#define	SDHC_CLK_SDCLKEN	(1 << 3)
+#define	SDHC_CLK_ENABLE_MASK	0x0000000f
+#define	SDHC_CLK_DIVISOR_MASK	0x000000f0
+#define	SDHC_CLK_DIVISOR_SHIFT	4
+#define	SDHC_CLK_PRESCALE_MASK	0x0000ff00
+#define	SDHC_CLK_PRESCALE_SHIFT	8
+
+static struct ofw_compat_data compat_data[] = {
+	{"fsl,imx6q-usdhc",	HWTYPE_USDHC},
+	{"fsl,imx6sl-usdhc",	HWTYPE_USDHC},
+	{"fsl,imx53-esdhc",	HWTYPE_ESDHC},
+	{"fsl,imx51-esdhc",	HWTYPE_ESDHC},
+	{"fsl,esdhc",		HWTYPE_ESDHC},
+	{NULL,			HWTYPE_NONE},
+};
+
+static uint16_t fsl_sdhc_get_clock(struct fsl_sdhci_softc *sc);
+static void fsl_sdhc_set_clock(struct fsl_sdhci_softc *sc, uint16_t val);
+static void fsl_sdhci_r1bfix_func(void *arg);
+
+static inline uint32_t
+RD4(struct fsl_sdhci_softc *sc, bus_size_t off)
+{
+
+	return (bus_read_4(sc->mem_res, off));
+}
+
+static inline void
+WR4(struct fsl_sdhci_softc *sc, bus_size_t off, uint32_t val)
+{
+
+	bus_write_4(sc->mem_res, off, val);
+}
+
+static uint8_t
+fsl_sdhci_read_1(device_t dev, struct sdhci_slot *slot, bus_size_t off)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(dev);
+	uint32_t val32, wrk32;
+
+	/*
+	 * Most of the things in the standard host control register are in the
+	 * hardware's wider protocol control register, but some of the bits are
+	 * moved around.
+	 */
+	if (off == SDHCI_HOST_CONTROL) {
+		wrk32 = RD4(sc, SDHC_PROT_CTRL);
+		val32 = wrk32 & (SDHCI_CTRL_LED | SDHCI_CTRL_CARD_DET |
+		    SDHCI_CTRL_FORCE_CARD);
+		switch (wrk32 & SDHC_PROT_WIDTH_MASK) {
+		case SDHC_PROT_WIDTH_1BIT:
+			/* Value is already 0. */
+			break;
+		case SDHC_PROT_WIDTH_4BIT:
+			val32 |= SDHCI_CTRL_4BITBUS;
+			break;
+		case SDHC_PROT_WIDTH_8BIT:
+			val32 |= SDHCI_CTRL_8BITBUS;
+			break;
+		}
+		switch (wrk32 & SDHC_PROT_DMA_MASK) {
+		case SDHC_PROT_SDMA:
+			/* Value is already 0. */
+			break;
+		case SDHC_PROT_ADMA1:
+			/* This value is deprecated, should never appear. */
+			break;
+		case SDHC_PROT_ADMA2:
+			val32 |= SDHCI_CTRL_ADMA2;
+			break;
+		case SDHC_PROT_ADMA264:
+			val32 |= SDHCI_CTRL_ADMA264;
+			break;
+		}
+		return val32;
+	}
+
+	/*
+	 * XXX can't find the bus power on/off knob.  For now we have to say the
+	 * power is always on and always set to the same voltage.
+	 */
+	if (off == SDHCI_POWER_CONTROL) {
+		return (SDHCI_POWER_ON | SDHCI_POWER_300);
+	}
+
+
+	return ((RD4(sc, off & ~3) >> (off & 3) * 8) & 0xff);
+}
+
+static uint16_t
+fsl_sdhci_read_2(device_t dev, struct sdhci_slot *slot, bus_size_t off)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(dev);
+	uint32_t val32;
+
+	if (sc->hwtype == HWTYPE_USDHC) {
+		/*
+		 * The USDHC hardware has nothing in the version register, but
+		 * it's v3 compatible with all our translation code.
+		 */
+		if (off == SDHCI_HOST_VERSION) {
+			return (SDHCI_SPEC_300 << SDHCI_SPEC_VER_SHIFT);
+		}
+		/*
+		 * The USDHC hardware moved the transfer mode bits to the mixed
+		 * control register, fetch them from there.
+		 */
+		if (off == SDHCI_TRANSFER_MODE)
+			return (RD4(sc, USDHC_MIX_CONTROL) & 0x37);
+
+	} else if (sc->hwtype == HWTYPE_ESDHC) {
+
+		/*
+		 * The ESDHC hardware has the typical 32-bit combined "command
+		 * and mode" register that we have to cache so that command
+		 * isn't written until after mode.  On a read, just retrieve the
+		 * cached values last written.
+		 */
+		if (off == SDHCI_TRANSFER_MODE) {
+			return (sc->cmd_and_mode & 0x0000ffff);
+		} else if (off == SDHCI_COMMAND_FLAGS) {
+			return (sc->cmd_and_mode >> 16);
+		}
+	}
+
+	/*
+	 * This hardware only manages one slot.  Synthesize a slot interrupt
+	 * status register... if there are any enabled interrupts active they
+	 * must be coming from our one and only slot.
+	 */
+	if (off == SDHCI_SLOT_INT_STATUS) {
+		val32  = RD4(sc, SDHCI_INT_STATUS);
+		val32 &= RD4(sc, SDHCI_SIGNAL_ENABLE);
+		return (val32 ? 1 : 0);
+	}
+
+	/*
+	 * Clock bits are scattered into various registers which differ by
+	 * hardware type, complex enough to have their own function.
+	 */
+	if (off == SDHCI_CLOCK_CONTROL) {
+		return (fsl_sdhc_get_clock(sc));
+	}
+
+	return ((RD4(sc, off & ~3) >> (off & 3) * 8) & 0xffff);
+}
+
+static uint32_t
+fsl_sdhci_read_4(device_t dev, struct sdhci_slot *slot, bus_size_t off)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(dev);
+	uint32_t val32, wrk32;
+
+	val32 = RD4(sc, off);
+
+	/*
+	 * The hardware leaves the base clock frequency out of the capabilities
+	 * register, but we filled it in by setting slot->max_clk at attach time
+	 * rather than here, because we can't represent frequencies above 63MHz
+	 * in an sdhci 2.0 capabliities register.  The timeout clock is the same
+	 * as the active output sdclock; we indicate that with a quirk setting
+	 * so don't populate the timeout frequency bits.
+	 *
+	 * XXX Turn off (for now) features the hardware can do but this driver
+	 * doesn't yet handle (1.8v, suspend/resume, etc).
+	 */
+	if (off == SDHCI_CAPABILITIES) {
+		val32 &= ~SDHCI_CAN_VDD_180;
+		val32 &= ~SDHCI_CAN_DO_SUSPEND;
+		val32 |= SDHCI_CAN_DO_8BITBUS;
+		return (val32);
+	}
+	
+	/*
+	 * The hardware moves bits around in the present state register to make
+	 * room for all 8 data line state bits.  To translate, mask out all the
+	 * bits which are not in the same position in both registers (this also
+	 * masks out some Freescale-specific bits in locations defined as
+	 * reserved by sdhci), then shift the data line and retune request bits
+	 * down to their standard locations.
+	 */
+	if (off == SDHCI_PRESENT_STATE) {
+		wrk32 = val32;
+		val32 &= 0x000F0F07;
+		val32 |= (wrk32 >> 4) & SDHCI_STATE_DAT_MASK;
+		val32 |= (wrk32 >> 9) & SDHCI_RETUNE_REQUEST;
+		return (val32);
+	}
+
+	/*
+	 * fsl_sdhci_intr() can synthesize a DATA_END interrupt following a
+	 * command with an R1B response, mix it into the hardware status.
+	 */
+	if (off == SDHCI_INT_STATUS) {
+		return (val32 | sc->r1bfix_intmask);
+	}
+
+	return val32;
+}
+
+static void
+fsl_sdhci_read_multi_4(device_t dev, struct sdhci_slot *slot, bus_size_t off,
+    uint32_t *data, bus_size_t count)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(dev);
+
+	bus_read_multi_4(sc->mem_res, off, data, count);
+}
+
+static void
+fsl_sdhci_write_1(device_t dev, struct sdhci_slot *slot, bus_size_t off, uint8_t val)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(dev);
+	uint32_t val32;
+
+	/*
+	 * Most of the things in the standard host control register are in the
+	 * hardware's wider protocol control register, but some of the bits are
+	 * moved around.
+	 */
+	if (off == SDHCI_HOST_CONTROL) {
+		val32 = RD4(sc, SDHC_PROT_CTRL);
+		val32 &= ~(SDHC_PROT_LED | SDHC_PROT_DMA_MASK |
+		    SDHC_PROT_WIDTH_MASK | SDHC_PROT_CDTL | SDHC_PROT_CDSS);
+		val32 |= (val & SDHCI_CTRL_LED);
+		if (val & SDHCI_CTRL_8BITBUS)
+			val32 |= SDHC_PROT_WIDTH_8BIT;
+		else
+			val32 |= (val & SDHCI_CTRL_4BITBUS);
+		val32 |= (val & (SDHCI_CTRL_SDMA | SDHCI_CTRL_ADMA2)) << 4;
+		val32 |= (val & (SDHCI_CTRL_CARD_DET | SDHCI_CTRL_FORCE_CARD));
+		WR4(sc, SDHC_PROT_CTRL, val32);
+		return;
+	}
+
+	/* XXX I can't find the bus power on/off knob; do nothing. */
+	if (off == SDHCI_POWER_CONTROL) {
+		return;
+	}
+#ifdef __powerpc__
+	/* XXX Reset doesn't seem to work as expected.  Do nothing for now. */
+	if (off == SDHCI_SOFTWARE_RESET)
+		return;
+#endif
+
+	val32 = RD4(sc, off & ~3);
+	val32 &= ~(0xff << (off & 3) * 8);
+	val32 |= (val << (off & 3) * 8);
+
+	WR4(sc, off & ~3, val32);
+}
+
+static void
+fsl_sdhci_write_2(device_t dev, struct sdhci_slot *slot, bus_size_t off, uint16_t val)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(dev);
+	uint32_t val32;
+
+	/*
+	 * The clock control stuff is complex enough to have its own function
+	 * that can handle the ESDHC versus USDHC differences.
+	 */
+	if (off == SDHCI_CLOCK_CONTROL) {
+		fsl_sdhc_set_clock(sc, val);
+		return;
+	}
+
+	/*
+	 * Figure out whether we need to check the DAT0 line for busy status at
+	 * interrupt time.  The controller should be doing this, but for some
+	 * reason it doesn't.  There are two cases:
+	 *  - R1B response with no data transfer should generate a DATA_END (aka
+	 *    TRANSFER_COMPLETE) interrupt after waiting for busy, but if
+	 *    there's no data transfer there's no DATA_END interrupt.  This is
+	 *    documented; they seem to think it's a feature.
+	 *  - R1B response after Auto-CMD12 appears to not work, even though
+	 *    there's a control bit for it (bit 3) in the vendor register.
+	 * When we're starting a command that needs a manual DAT0 line check at
+	 * interrupt time, we leave ourselves a note in r1bfix_type so that we
+	 * can do the extra work in fsl_sdhci_intr().
+	 */
+	if (off == SDHCI_COMMAND_FLAGS) {
+		if (val & SDHCI_CMD_DATA) {
+			const uint32_t MBAUTOCMD = SDHCI_TRNS_ACMD12 | SDHCI_TRNS_MULTI;
+			val32 = RD4(sc, USDHC_MIX_CONTROL);
+			if ((val32 & MBAUTOCMD) == MBAUTOCMD)
+				sc->r1bfix_type = R1BFIX_AC12;
+		} else {
+			if ((val & SDHCI_CMD_RESP_MASK) == SDHCI_CMD_RESP_SHORT_BUSY) {
+				WR4(sc, SDHCI_INT_ENABLE, slot->intmask | SDHCI_INT_RESPONSE);
+				WR4(sc, SDHCI_SIGNAL_ENABLE, slot->intmask | SDHCI_INT_RESPONSE);
+				sc->r1bfix_type = R1BFIX_NODATA;
+			}
+		}
+	}
+
+	/*
+	 * The USDHC hardware moved the transfer mode bits to mixed control; we
+	 * just write them there and we're done.  The ESDHC hardware has the
+	 * typical combined cmd-and-mode register that allows only 32-bit
+	 * access, so when writing the mode bits just save them, then later when
+	 * writing the command bits, add in the saved mode bits.
+	 */
+	if (sc->hwtype == HWTYPE_USDHC) {
+		if (off == SDHCI_TRANSFER_MODE) {
+			val32 = RD4(sc, USDHC_MIX_CONTROL);
+			val32 &= ~0x3f;
+			val32 |= val & 0x37;
+			// XXX acmd23 not supported here (or by sdhci driver)
+			WR4(sc, USDHC_MIX_CONTROL, val32);
+			return;
+		}
+	} else if (sc->hwtype == HWTYPE_ESDHC) {
+		if (off == SDHCI_TRANSFER_MODE) {
+			sc->cmd_and_mode =
+			    (sc->cmd_and_mode & 0xffff0000) | val;
+			return;
+		} else if (off == SDHCI_COMMAND_FLAGS) {
+			sc->cmd_and_mode =
+			    (sc->cmd_and_mode & 0xffff) | (val << 16);
+			WR4(sc, SDHCI_TRANSFER_MODE, sc->cmd_and_mode);
+			return;
+		}
+	}
+
+	val32 = RD4(sc, off & ~3);
+	val32 &= ~(0xffff << (off & 3) * 8);
+	val32 |= ((val & 0xffff) << (off & 3) * 8);
+	WR4(sc, off & ~3, val32);	
+}
+
+static void
+fsl_sdhci_write_4(device_t dev, struct sdhci_slot *slot, bus_size_t off, uint32_t val)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(dev);
+
+	/* Clear synthesized interrupts, then pass the value to the hardware. */
+	if (off == SDHCI_INT_STATUS) {
+		sc->r1bfix_intmask &= ~val;
+	}
+
+	WR4(sc, off, val);
+}
+
+static void
+fsl_sdhci_write_multi_4(device_t dev, struct sdhci_slot *slot, bus_size_t off,
+    uint32_t *data, bus_size_t count)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(dev);
+
+	bus_write_multi_4(sc->mem_res, off, data, count);
+}
+
+static uint16_t
+fsl_sdhc_get_clock(struct fsl_sdhci_softc *sc)
+{
+	uint16_t val;
+
+	/*
+	 * Whenever the sdhci driver writes the clock register we save a
+	 * snapshot of just the frequency bits, so that we can play them back
+	 * here on a register read without recalculating the frequency from the
+	 * prescalar and divisor bits in the real register.  We'll start with
+	 * those bits, and mix in the clock status and enable bits that come
+	 * from different places depending on which hardware we've got.
+	 */
+	val = sc->sdclockreg_freq_bits;
+
+	/*
+	 * The internal clock is always enabled (actually, the hardware manages
+	 * it).  Whether the internal clock is stable yet after a frequency
+	 * change comes from the present-state register on both hardware types.
+	 */
+	val |= SDHCI_CLOCK_INT_EN;
+	if (RD4(sc, SDHC_PRES_STATE) & SDHC_PRES_SDSTB)
+	    val |= SDHCI_CLOCK_INT_STABLE;
+
+	/*
+	 * On i.MX ESDHC hardware the card bus clock enable is in the usual
+	 * sdhci register but it's a different bit, so transcribe it (note the
+	 * difference between standard SDHCI_ and Freescale SDHC_ prefixes
+	 * here). On USDHC and QorIQ ESDHC hardware there is a force-on bit, but
+	 * no force-off for the card bus clock (the hardware runs the clock when
+	 * transfers are active no matter what), so we always say the clock is
+	 * on.
+	 * XXX Maybe we should say it's in whatever state the sdhci driver last
+	 * set it to.
+	 */
+	if (sc->hwtype == HWTYPE_ESDHC) {
+#ifdef __arm__
+		if (RD4(sc, SDHC_SYS_CTRL) & SDHC_CLK_SDCLKEN)
+#endif
+			val |= SDHCI_CLOCK_CARD_EN;
+	} else {
+		val |= SDHCI_CLOCK_CARD_EN;
+	}
+
+	return (val);
+}
+
+static void 
+fsl_sdhc_set_clock(struct fsl_sdhci_softc *sc, uint16_t val)
+{
+	uint32_t divisor, freq, prescale, val32;
+
+	val32 = RD4(sc, SDHCI_CLOCK_CONTROL);
+
+	/*
+	 * Save the frequency-setting bits in SDHCI format so that we can play
+	 * them back in get_clock without complex decoding of hardware regs,
+	 * then deal with the freqency part of the value based on hardware type.
+	 */
+	sc->sdclockreg_freq_bits = val & SDHCI_DIVIDERS_MASK;
+	if (sc->hwtype == HWTYPE_ESDHC) {
+		/*
+		 * The i.MX5 ESDHC hardware requires the driver to manually
+		 * start and stop the sd bus clock.  If the enable bit is not
+		 * set, turn off the clock in hardware and we're done, otherwise
+		 * decode the requested frequency.  ESDHC hardware is sdhci 2.0;
+		 * the sdhci driver will use the original 8-bit divisor field
+		 * and the "base / 2^N" divisor scheme.
+		 */
+		if ((val & SDHCI_CLOCK_CARD_EN) == 0) {
+#ifdef __arm__
+			/* On QorIQ, this is a reserved bit. */
+			WR4(sc, SDHCI_CLOCK_CONTROL, val32 & ~SDHC_CLK_SDCLKEN);
+#endif
+			return;
+
+		}
+		divisor = (val >> SDHCI_DIVIDER_SHIFT) & SDHCI_DIVIDER_MASK;
+		freq = sc->baseclk_hz >> ffs(divisor);
+	} else {
+		/*
+		 * The USDHC hardware provides only "force always on" control
+		 * over the sd bus clock, but no way to turn it off.  (If a cmd
+		 * or data transfer is in progress the clock is on, otherwise it
+		 * is off.)  If the clock is being disabled, we can just return
+		 * now, otherwise we decode the requested frequency.  USDHC
+		 * hardware is sdhci 3.0; the sdhci driver will use a 10-bit
+		 * divisor using the "base / 2*N" divisor scheme.
+		 */
+		if ((val & SDHCI_CLOCK_CARD_EN) == 0)
+			return;
+		divisor = ((val >> SDHCI_DIVIDER_SHIFT) & SDHCI_DIVIDER_MASK) |
+		    ((val >> SDHCI_DIVIDER_HI_SHIFT) & SDHCI_DIVIDER_HI_MASK) <<
+		    SDHCI_DIVIDER_MASK_LEN;
+		if (divisor == 0)
+			freq = sc->baseclk_hz;
+		else
+			freq = sc->baseclk_hz / (2 * divisor);
+	}
+
+	/*
+	 * Get a prescaler and final divisor to achieve the desired frequency.
+	 */
+	for (prescale = 2; freq < sc->baseclk_hz / (prescale * 16);)
+		prescale <<= 1;
+
+	for (divisor = 1; freq < sc->baseclk_hz / (prescale * divisor);)
+		++divisor;
+
+#ifdef DEBUG	
+	device_printf(sc->dev,
+	    "desired SD freq: %d, actual: %d; base %d prescale %d divisor %d\n",
+	    freq, sc->baseclk_hz / (prescale * divisor), sc->baseclk_hz, 
+	    prescale, divisor);
+#endif	
+
+	/*
+	 * Adjust to zero-based values, and store them to the hardware.
+	 */
+	prescale >>= 1;
+	divisor -= 1;
+
+	val32 &= ~(SDHC_CLK_DIVISOR_MASK | SDHC_CLK_PRESCALE_MASK);
+	val32 |= divisor << SDHC_CLK_DIVISOR_SHIFT;
+	val32 |= prescale << SDHC_CLK_PRESCALE_SHIFT;
+	val32 |= SDHC_CLK_IPGEN;
+	WR4(sc, SDHCI_CLOCK_CONTROL, val32);
+}
+
+static boolean_t
+fsl_sdhci_r1bfix_is_wait_done(struct fsl_sdhci_softc *sc)
+{
+	uint32_t inhibit;
+
+	mtx_assert(&sc->slot.mtx, MA_OWNED);
+
+	/*
+	 * Check the DAT0 line status using both the DLA (data line active) and
+	 * CDIHB (data inhibit) bits in the present state register.  In theory
+	 * just DLA should do the trick,  but in practice it takes both.  If the
+	 * DAT0 line is still being held and we're not yet beyond the timeout
+	 * point, just schedule another callout to check again later.
+	 */
+	inhibit = RD4(sc, SDHC_PRES_STATE) & (SDHC_PRES_DLA | SDHC_PRES_CDIHB);
+
+	if (inhibit && getsbinuptime() < sc->r1bfix_timeout_at) {
+		callout_reset_sbt(&sc->r1bfix_callout, SBT_1MS, 0, 
+		    fsl_sdhci_r1bfix_func, sc, 0);
+		return (false);
+	}
+
+	/*
+	 * If we reach this point with the inhibit bits still set, we've got a
+	 * timeout, synthesize a DATA_TIMEOUT interrupt.  Otherwise the DAT0
+	 * line has been released, and we synthesize a DATA_END, and if the type
+	 * of fix needed was on a command-without-data we also now add in the
+	 * original INT_RESPONSE that we suppressed earlier.
+	 */
+	if (inhibit)
+		sc->r1bfix_intmask |= SDHCI_INT_DATA_TIMEOUT;
+	else {
+		sc->r1bfix_intmask |= SDHCI_INT_DATA_END;
+		if (sc->r1bfix_type == R1BFIX_NODATA)
+			sc->r1bfix_intmask |= SDHCI_INT_RESPONSE;
+	}
+
+	sc->r1bfix_type = R1BFIX_NONE;
+	return (true);
+}
+
+static void
+fsl_sdhci_r1bfix_func(void * arg)
+{
+	struct fsl_sdhci_softc *sc = arg;
+	boolean_t r1bwait_done;
+
+	mtx_lock(&sc->slot.mtx);
+	r1bwait_done = fsl_sdhci_r1bfix_is_wait_done(sc);
+	mtx_unlock(&sc->slot.mtx);
+	if (r1bwait_done)
+		sdhci_generic_intr(&sc->slot);
+}
+
+static void
+fsl_sdhci_intr(void *arg)
+{
+	struct fsl_sdhci_softc *sc = arg;
+	uint32_t intmask;
+
+	mtx_lock(&sc->slot.mtx);
+
+	/*
+	 * Manually check the DAT0 line for R1B response types that the
+	 * controller fails to handle properly.  The controller asserts the done
+	 * interrupt while the card is still asserting busy with the DAT0 line.
+	 *
+	 * We check DAT0 immediately because most of the time, especially on a
+	 * read, the card will actually be done by time we get here.  If it's
+	 * not, then the wait_done routine will schedule a callout to re-check
+	 * periodically until it is done.  In that case we clear the interrupt
+	 * out of the hardware now so that we can present it later when the DAT0
+	 * line is released.
+	 *
+	 * If we need to wait for the DAT0 line to be released, we set up a
+	 * timeout point 250ms in the future.  This number comes from the SD
+	 * spec, which allows a command to take that long.  In the real world,
+	 * cards tend to take 10-20ms for a long-running command such as a write
+	 * or erase that spans two pages.
+	 */
+	switch (sc->r1bfix_type) {
+	case R1BFIX_NODATA:
+		intmask = RD4(sc, SDHCI_INT_STATUS) & SDHCI_INT_RESPONSE;
+		break;
+	case R1BFIX_AC12:
+		intmask = RD4(sc, SDHCI_INT_STATUS) & SDHCI_INT_DATA_END;
+		break;
+	default:
+		intmask = 0;
+		break;
+	}
+	if (intmask) {
+		sc->r1bfix_timeout_at = getsbinuptime() + 250 * SBT_1MS;
+		if (!fsl_sdhci_r1bfix_is_wait_done(sc)) {
+			WR4(sc, SDHCI_INT_STATUS, intmask);
+			bus_barrier(sc->mem_res, SDHCI_INT_STATUS, 4, 
+			    BUS_SPACE_BARRIER_WRITE);
+		}
+	}
+
+	mtx_unlock(&sc->slot.mtx);
+	sdhci_generic_intr(&sc->slot);
+}
+
+static int
+fsl_sdhci_get_ro(device_t bus, device_t child)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(bus);
+
+	return (sdhci_fdt_gpio_get_readonly(sc->gpio));
+}
+
+static bool
+fsl_sdhci_get_card_present(device_t dev, struct sdhci_slot *slot)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(dev);
+
+	return (sdhci_fdt_gpio_get_present(sc->gpio));
+}
+
+#ifdef __powerpc__
+static uint32_t
+fsl_sdhci_get_platform_clock(device_t dev)
+{
+	phandle_t node;
+	uint32_t clock;
+
+	node = ofw_bus_get_node(dev);
+
+	/* Get sdhci node properties */
+	if((OF_getprop(node, "clock-frequency", (void *)&clock,
+	    sizeof(clock)) <= 0) || (clock == 0)) {
+
+		clock = mpc85xx_get_system_clock();
+
+		if (clock == 0) {
+			device_printf(dev,"Cannot acquire correct sdhci "
+			    "frequency from DTS.\n");
+
+			return (0);
+		}
+	}
+
+	if (bootverbose)
+		device_printf(dev, "Acquired clock: %d from DTS\n", clock);
+
+	return (clock);
+}
+#endif
+
+
+static int
+fsl_sdhci_detach(device_t dev)
+{
+
+	/* sdhci_fdt_gpio_teardown(sc->gpio); */
+	return (EBUSY);
+}
+
+static int
+fsl_sdhci_attach(device_t dev)
+{
+	struct fsl_sdhci_softc *sc = device_get_softc(dev);
+	int rid, err;
+#ifdef __powerpc__
+	phandle_t node;
+	uint32_t protctl;
+#endif
+
+	sc->dev = dev;
+
+	sc->hwtype = ofw_bus_search_compatible(dev, compat_data)->ocd_data;
+	if (sc->hwtype == HWTYPE_NONE)
+		panic("Impossible: not compatible in fsl_sdhci_attach()");
+
+	rid = 0;
+	sc->mem_res = bus_alloc_resource_any(dev, SYS_RES_MEMORY, &rid,
+	    RF_ACTIVE);
+	if (!sc->mem_res) {
+		device_printf(dev, "cannot allocate memory window\n");
+		err = ENXIO;
+		goto fail;
+	}
+
+	rid = 0;
+	sc->irq_res = bus_alloc_resource_any(dev, SYS_RES_IRQ, &rid,
+	    RF_ACTIVE);
+	if (!sc->irq_res) {
+		device_printf(dev, "cannot allocate interrupt\n");
+		err = ENXIO;
+		goto fail;
+	}
+
+	if (bus_setup_intr(dev, sc->irq_res, INTR_TYPE_BIO | INTR_MPSAFE,
+	    NULL, fsl_sdhci_intr, sc, &sc->intr_cookie)) {
+		device_printf(dev, "cannot setup interrupt handler\n");
+		err = ENXIO;
+		goto fail;
+	}
+
+	sc->slot.quirks |= SDHCI_QUIRK_DATA_TIMEOUT_USES_SDCLK;
+
+	/*
+	 * DMA is not really broken, I just haven't implemented it yet.
+	 */
+	sc->slot.quirks |= SDHCI_QUIRK_BROKEN_DMA;
+
+	/*
+	 * Set the buffer watermark level to 128 words (512 bytes) for both read
+	 * and write.  The hardware has a restriction that when the read or
+	 * write ready status is asserted, that means you can read exactly the
+	 * number of words set in the watermark register before you have to
+	 * re-check the status and potentially wait for more data.  The main
+	 * sdhci driver provides no hook for doing status checking on less than
+	 * a full block boundary, so we set the watermark level to be a full
+	 * block.  Reads and writes where the block size is less than the
+	 * watermark size will work correctly too, no need to change the
+	 * watermark for different size blocks.  However, 128 is the maximum
+	 * allowed for the watermark, so PIO is limitted to 512 byte blocks
+	 * (which works fine for SD cards, may be a problem for SDIO some day).
+	 *
+	 * XXX need named constants for this stuff.
+	 */
+	/* P1022 has the '*_BRST_LEN' fields as reserved, always reading 0x10 */
+	if (ofw_bus_is_compatible(dev, "fsl,p1022-esdhc"))
+		WR4(sc, SDHC_WTMK_LVL, 0x10801080);
+	else
+		WR4(sc, SDHC_WTMK_LVL, 0x08800880);
+
+	/*
+	 * We read in native byte order in the main driver, but the register
+	 * defaults to little endian.
+	 */
+#ifdef __powerpc__
+	sc->baseclk_hz = fsl_sdhci_get_platform_clock(dev);
+#else
+	sc->baseclk_hz = imx_ccm_sdhci_hz();
+#endif
+	sc->slot.max_clk = sc->baseclk_hz;
+
+	/*
+	 * Set up any gpio pin handling described in the FDT data. This cannot
+	 * fail; see comments in sdhci_fdt_gpio.h for details.
+	 */
+	sc->gpio = sdhci_fdt_gpio_setup(dev, &sc->slot);
+
+#ifdef __powerpc__
+	node = ofw_bus_get_node(dev);
+	/* Default to big-endian on powerpc */
+	protctl = RD4(sc, SDHC_PROT_CTRL);
+	protctl &= ~SDHC_PROT_EMODE_MASK;
+	if (OF_hasprop(node, "little-endian"))
+		protctl |= SDHC_PROT_EMODE_LITTLE;
+	else
+		protctl |= SDHC_PROT_EMODE_BIG;
+	WR4(sc, SDHC_PROT_CTRL, protctl);
+#endif
+
+	callout_init(&sc->r1bfix_callout, 1);
+	sdhci_init_slot(dev, &sc->slot, 0);
+
+	bus_generic_probe(dev);
+	bus_generic_attach(dev);
+
+	sdhci_start_slot(&sc->slot);
+
+	return (0);
+
+fail:
+	if (sc->intr_cookie)
+		bus_teardown_intr(dev, sc->irq_res, sc->intr_cookie);
+	if (sc->irq_res)
+		bus_release_resource(dev, SYS_RES_IRQ, 0, sc->irq_res);
+	if (sc->mem_res)
+		bus_release_resource(dev, SYS_RES_MEMORY, 0, sc->mem_res);
+
+	return (err);
+}
+
+static int
+fsl_sdhci_probe(device_t dev)
+{
+
+        if (!ofw_bus_status_okay(dev))
+		return (ENXIO);
+
+	switch (ofw_bus_search_compatible(dev, compat_data)->ocd_data) {
+	case HWTYPE_ESDHC:
+		device_set_desc(dev, "Freescale eSDHC controller");
+		return (BUS_PROBE_DEFAULT);
+	case HWTYPE_USDHC:
+		device_set_desc(dev, "Freescale uSDHC controller");
+		return (BUS_PROBE_DEFAULT);
+	default:
+		break;
+	}
+	return (ENXIO);
+}
+
+static device_method_t fsl_sdhci_methods[] = {
+	/* Device interface */
+	DEVMETHOD(device_probe,		fsl_sdhci_probe),
+	DEVMETHOD(device_attach,	fsl_sdhci_attach),
+	DEVMETHOD(device_detach,	fsl_sdhci_detach),
+
+	/* Bus interface */
+	DEVMETHOD(bus_read_ivar,	sdhci_generic_read_ivar),
+	DEVMETHOD(bus_write_ivar,	sdhci_generic_write_ivar),
+
+	/* MMC bridge interface */
+	DEVMETHOD(mmcbr_update_ios,	sdhci_generic_update_ios),
+	DEVMETHOD(mmcbr_request,	sdhci_generic_request),
+	DEVMETHOD(mmcbr_get_ro,		fsl_sdhci_get_ro),
+	DEVMETHOD(mmcbr_acquire_host,	sdhci_generic_acquire_host),
+	DEVMETHOD(mmcbr_release_host,	sdhci_generic_release_host),
+
+	/* SDHCI accessors */
+	DEVMETHOD(sdhci_read_1,		fsl_sdhci_read_1),
+	DEVMETHOD(sdhci_read_2,		fsl_sdhci_read_2),
+	DEVMETHOD(sdhci_read_4,		fsl_sdhci_read_4),
+	DEVMETHOD(sdhci_read_multi_4,	fsl_sdhci_read_multi_4),
+	DEVMETHOD(sdhci_write_1,	fsl_sdhci_write_1),
+	DEVMETHOD(sdhci_write_2,	fsl_sdhci_write_2),
+	DEVMETHOD(sdhci_write_4,	fsl_sdhci_write_4),
+	DEVMETHOD(sdhci_write_multi_4,	fsl_sdhci_write_multi_4),
+	DEVMETHOD(sdhci_get_card_present,fsl_sdhci_get_card_present),
+
+	DEVMETHOD_END
+};
+
+static devclass_t fsl_sdhci_devclass;
+
+static driver_t fsl_sdhci_driver = {
+	"sdhci_fsl",
+	fsl_sdhci_methods,
+	sizeof(struct fsl_sdhci_softc),
+};
+
+DRIVER_MODULE(sdhci_fsl, simplebus, fsl_sdhci_driver, fsl_sdhci_devclass,
+    NULL, NULL);
+MODULE_DEPEND(sdhci_fsl, sdhci, 1, 1, 1);
+
+#ifndef MMCCAM
+MMC_DECLARE_BRIDGE(sdhci_fsl);
+#endif
diff --git a/freebsd/sys/dev/sdhci/sdhci.c b/freebsd/sys/dev/sdhci/sdhci.c
index 8c1be21..0d66d49 100644
--- a/freebsd/sys/dev/sdhci/sdhci.c
+++ b/freebsd/sys/dev/sdhci/sdhci.c
@@ -50,13 +50,21 @@ __FBSDID("$FreeBSD$");
 #include <dev/mmc/mmcreg.h>
 #include <dev/mmc/mmcbrvar.h>
 
+#include <cam/cam.h>
+#include <cam/cam_ccb.h>
+#include <cam/cam_debug.h>
+#include <cam/cam_sim.h>
+#include <cam/cam_xpt_sim.h>
+
 #include <rtems/bsd/local/mmcbr_if.h>
 #include "sdhci.h"
 #include <rtems/bsd/local/sdhci_if.h>
 
+#include <rtems/bsd/local/opt_mmccam.h>
+
 SYSCTL_NODE(_hw, OID_AUTO, sdhci, CTLFLAG_RD, 0, "sdhci driver");
 
-static int sdhci_debug;
+static int sdhci_debug = 0;
 SYSCTL_INT(_hw_sdhci, OID_AUTO, debug, CTLFLAG_RWTUN, &sdhci_debug, 0,
     "Debug level");
 u_int sdhci_quirk_clear = 0;
@@ -85,6 +93,16 @@ static void sdhci_start_data(struct sdhci_slot *slot, struct mmc_data *data);
 static void sdhci_card_poll(void *);
 static void sdhci_card_task(void *, int);
 
+#ifdef MMCCAM
+/* CAM-related */
+int sdhci_cam_get_possible_host_clock(struct sdhci_slot *slot, int proposed_clock);
+static int sdhci_cam_update_ios(struct sdhci_slot *slot);
+static int sdhci_cam_request(struct sdhci_slot *slot, union ccb *ccb);
+static void sdhci_cam_action(struct cam_sim *sim, union ccb *ccb);
+static void sdhci_cam_poll(struct cam_sim *sim);
+static int sdhci_cam_settran_settings(struct sdhci_slot *slot, union ccb *ccb);
+#endif
+
 /* helper routines */
 static void sdhci_dumpregs(struct sdhci_slot *slot);
 static int slot_printf(struct sdhci_slot *slot, const char * fmt, ...)
@@ -254,7 +272,7 @@ sdhci_init(struct sdhci_slot *slot)
 	    SDHCI_INT_END_BIT | SDHCI_INT_CRC | SDHCI_INT_TIMEOUT |
 	    SDHCI_INT_DATA_AVAIL | SDHCI_INT_SPACE_AVAIL |
 	    SDHCI_INT_DMA_END | SDHCI_INT_DATA_END | SDHCI_INT_RESPONSE |
-	    SDHCI_INT_ACMD12ERR;
+	    SDHCI_INT_ACMD12ERR | SDHCI_INT_CARD_INT;
 
 	if (!(slot->quirks & SDHCI_QUIRK_POLL_CARD_PRESENT) &&
 	    !(slot->opt & SDHCI_NON_REMOVABLE)) {
@@ -368,6 +386,7 @@ sdhci_set_clock(struct sdhci_slot *slot, uint32_t clock)
 static void
 sdhci_set_power(struct sdhci_slot *slot, u_char power)
 {
+	int i;
 	uint8_t pwr;
 
 	if (slot->power == power)
@@ -396,9 +415,20 @@ sdhci_set_power(struct sdhci_slot *slot, u_char power)
 		break;
 	}
 	WR1(slot, SDHCI_POWER_CONTROL, pwr);
-	/* Turn on the power. */
+	/*
+	 * Turn on VDD1 power.  Note that at least some Intel controllers can
+	 * fail to enable bus power on the first try after transiting from D3
+	 * to D0, so we give them up to 2 ms.
+	 */
 	pwr |= SDHCI_POWER_ON;
-	WR1(slot, SDHCI_POWER_CONTROL, pwr);
+	for (i = 0; i < 20; i++) {
+		WR1(slot, SDHCI_POWER_CONTROL, pwr);
+		if (RD1(slot, SDHCI_POWER_CONTROL) & SDHCI_POWER_ON)
+			break;
+		DELAY(100);
+	}
+	if (!(RD1(slot, SDHCI_POWER_CONTROL) & SDHCI_POWER_ON))
+		slot_printf(slot, "Bus power failed to enable");
 
 	if (slot->quirks & SDHCI_QUIRK_INTEL_POWER_UP_RESET) {
 		WR1(slot, SDHCI_POWER_CONTROL, pwr | 0x10);
@@ -521,25 +551,89 @@ sdhci_card_task(void *arg, int pending __unused)
 
 	SDHCI_LOCK(slot);
 	if (SDHCI_GET_CARD_PRESENT(slot->bus, slot)) {
+#ifdef MMCCAM
+		if (slot->card_present == 0) {
+#else
 		if (slot->dev == NULL) {
+#endif
 			/* If card is present - attach mmc bus. */
 			if (bootverbose || sdhci_debug)
 				slot_printf(slot, "Card inserted\n");
+#ifdef MMCCAM
+			slot->card_present = 1;
+			union ccb *ccb;
+			uint32_t pathid;
+			pathid = cam_sim_path(slot->sim);
+			printf("**Calling cam_sim_ccb_nowait and pathid is %d \n",pathid);
+			ccb = xpt_alloc_ccb_nowait();
+			printf("**Calling xpt_alloc_ccb_nowait and result of ccb==NULL is %d\n",(ccb == NULL));
+			if (ccb == NULL) {
+				slot_printf(slot, "Unable to alloc CCB for rescan\n");
+				SDHCI_UNLOCK(slot);
+				return;
+			}
+
+			/*
+			 * We create a rescan request for BUS:0:0, since the card
+			 * will be at lun 0.
+			 */
+			if (xpt_create_path(&ccb->ccb_h.path, NULL, pathid,
+					    /* target */ 0, /* lun */ 0) != CAM_REQ_CMP) {
+				slot_printf(slot, "Unable to create path for rescan\n");
+				SDHCI_UNLOCK(slot);
+				xpt_free_ccb(ccb);
+				return;
+			}
+			SDHCI_UNLOCK(slot);
+			xpt_rescan(ccb);
+#else
 			slot->dev = device_add_child(slot->bus, "mmc", -1);
 			device_set_ivars(slot->dev, slot);
 			SDHCI_UNLOCK(slot);
 			device_probe_and_attach(slot->dev);
+#endif
 		} else
 			SDHCI_UNLOCK(slot);
 	} else {
+#ifdef MMCCAM
+		if (slot->card_present == 1) {
+#else
 		if (slot->dev != NULL) {
+#endif
 			/* If no card present - detach mmc bus. */
 			if (bootverbose || sdhci_debug)
 				slot_printf(slot, "Card removed\n");
 			d = slot->dev;
 			slot->dev = NULL;
+#ifdef MMCCAM
+			slot->card_present = 0;
+			union ccb *ccb;
+			uint32_t pathid;
+			pathid = cam_sim_path(slot->sim);
+			ccb = xpt_alloc_ccb_nowait();
+			if (ccb == NULL) {
+				slot_printf(slot, "Unable to alloc CCB for rescan\n");
+				SDHCI_UNLOCK(slot);
+				return;
+			}
+
+			/*
+			 * We create a rescan request for BUS:0:0, since the card
+			 * will be at lun 0.
+			 */
+			if (xpt_create_path(&ccb->ccb_h.path, NULL, pathid,
+					    /* target */ 0, /* lun */ 0) != CAM_REQ_CMP) {
+				slot_printf(slot, "Unable to create path for rescan\n");
+				SDHCI_UNLOCK(slot);
+				xpt_free_ccb(ccb);
+				return;
+			}
+			SDHCI_UNLOCK(slot);
+			xpt_rescan(ccb);
+#else
 			SDHCI_UNLOCK(slot);
 			device_delete_child(slot->bus, d);
+#endif
 		} else
 			SDHCI_UNLOCK(slot);
 	}
@@ -561,7 +655,11 @@ sdhci_handle_card_present_locked(struct sdhci_slot *slot, bool is_present)
 	 * because once power is removed, a full card re-init is needed, and
 	 * that happens by deleting and recreating the child device.
 	 */
+#ifdef MMCCAM
+	was_present = slot->card_present;
+#else
 	was_present = slot->dev != NULL;
+#endif
 	if (!was_present && is_present) {
 		taskqueue_enqueue_timeout(taskqueue_swi_giant,
 		    &slot->card_delayed_task, -SDHCI_INSERT_DELAY_TICKS);
@@ -593,10 +691,12 @@ sdhci_card_poll(void *arg)
 int
 sdhci_init_slot(device_t dev, struct sdhci_slot *slot, int num)
 {
+
 	uint32_t caps, caps2, freq, host_caps;
 	int err;
 
 	SDHCI_LOCK_INIT(slot);
+
 	slot->num = num;
 	slot->bus = dev;
 
@@ -733,11 +833,11 @@ sdhci_init_slot(device_t dev, struct sdhci_slot *slot, int num)
 	    MMC_CAP_MMC_DDR52_180 | MMC_CAP_MMC_HS200_180 |
 	    MMC_CAP_MMC_HS400_180))
 		host_caps |= MMC_CAP_SIGNALING_180;
-	if (caps & SDHCI_CTRL2_DRIVER_TYPE_A)
+	if (caps2 & SDHCI_CAN_DRIVE_TYPE_A)
 		host_caps |= MMC_CAP_DRIVER_TYPE_A;
-	if (caps & SDHCI_CTRL2_DRIVER_TYPE_C)
+	if (caps2 & SDHCI_CAN_DRIVE_TYPE_C)
 		host_caps |= MMC_CAP_DRIVER_TYPE_C;
-	if (caps & SDHCI_CTRL2_DRIVER_TYPE_D)
+	if (caps2 & SDHCI_CAN_DRIVE_TYPE_D)
 		host_caps |= MMC_CAP_DRIVER_TYPE_D;
 	slot->host.caps = host_caps;
 
@@ -771,9 +871,9 @@ sdhci_init_slot(device_t dev, struct sdhci_slot *slot, int num)
 		    (caps & SDHCI_CAN_VDD_180) ? " 1.8V" : "",
 		    (host_caps & MMC_CAP_SIGNALING_180) ? " 1.8V" : "",
 		    (host_caps & MMC_CAP_SIGNALING_120) ? " 1.2V" : "",
-		    (caps & SDHCI_CTRL2_DRIVER_TYPE_A) ? "A" : "",
-		    (caps & SDHCI_CTRL2_DRIVER_TYPE_C) ? "C" : "",
-		    (caps & SDHCI_CTRL2_DRIVER_TYPE_D) ? "D" : "",
+		    (caps2 & SDHCI_CAN_DRIVE_TYPE_A) ? "A" : "",
+		    (caps2 & SDHCI_CAN_DRIVE_TYPE_C) ? "C" : "",
+		    (caps2 & SDHCI_CAN_DRIVE_TYPE_D) ? "D" : "",
 		    (slot->opt & SDHCI_HAVE_DMA) ? "DMA" : "PIO");
 		if (host_caps & (MMC_CAP_MMC_DDR52 | MMC_CAP_MMC_HS200 |
 		    MMC_CAP_MMC_HS400 | MMC_CAP_MMC_ENH_STROBE))
@@ -816,12 +916,14 @@ sdhci_init_slot(device_t dev, struct sdhci_slot *slot, int num)
 	return (0);
 }
 
+#ifndef MMCCAM
 void
 sdhci_start_slot(struct sdhci_slot *slot)
 {
 
 	sdhci_card_task(slot, 0);
 }
+#endif
 
 int
 sdhci_cleanup_slot(struct sdhci_slot *slot)
@@ -1019,6 +1121,30 @@ done:
 	return (err);
 }
 
+#ifdef MMCCAM
+static void
+sdhci_req_done(struct sdhci_slot *slot)
+{
+        union ccb *ccb;
+
+	if (sdhci_debug > 1)
+		slot_printf(slot, "%s\n", __func__);
+	if (slot->ccb != NULL && slot->curcmd != NULL) {
+		callout_stop(&slot->timeout_callout);
+                ccb = slot->ccb;
+                slot->ccb = NULL;
+		slot->curcmd = NULL;
+
+                /* Tell CAM the request is finished */
+                struct ccb_mmcio *mmcio;
+                mmcio = &ccb->mmcio;
+
+                ccb->ccb_h.status =
+                        (mmcio->cmd.error == 0 ? CAM_REQ_CMP : CAM_REQ_CMP_ERR);
+                xpt_done(ccb);
+	}
+}
+#else
 static void
 sdhci_req_done(struct sdhci_slot *slot)
 {
@@ -1032,6 +1158,7 @@ sdhci_req_done(struct sdhci_slot *slot)
 		req->done(req);
 	}
 }
+#endif
 
 static void
 sdhci_timeout(void *arg)
@@ -1062,8 +1189,16 @@ sdhci_set_transfer_mode(struct sdhci_slot *slot, struct mmc_data *data)
 		mode |= SDHCI_TRNS_MULTI;
 	if (data->flags & MMC_DATA_READ)
 		mode |= SDHCI_TRNS_READ;
-	if (slot->req->stop)
+#ifdef MMCCAM
+	struct ccb_mmcio *mmcio;
+	mmcio = &slot->ccb->mmcio;
+	if (mmcio->stop.opcode == MMC_STOP_TRANSMISSION
+	    && !(slot->quirks & SDHCI_QUIRK_BROKEN_AUTO_STOP))
+		mode |= SDHCI_TRNS_ACMD12;
+#else
+	if (slot->req->stop && !(slot->quirks & SDHCI_QUIRK_BROKEN_AUTO_STOP))
 		mode |= SDHCI_TRNS_ACMD12;
+#endif
 	if (slot->flags & SDHCI_USE_DMA)
 		mode |= SDHCI_TRNS_DMA;
 
@@ -1096,6 +1231,9 @@ sdhci_start_command(struct sdhci_slot *slot, struct mmc_command *cmd)
 	if (!SDHCI_GET_CARD_PRESENT(slot->bus, slot) ||
 	    slot->power == 0 ||
 	    slot->clock == 0) {
+		slot_printf(slot,
+			    "Cannot issue a command (power=%d clock=%d)",
+			    slot->power, slot->clock);
 		cmd->error = MMC_ERR_FAILED;
 		sdhci_req_done(slot);
 		return;
@@ -1103,11 +1241,17 @@ sdhci_start_command(struct sdhci_slot *slot, struct mmc_command *cmd)
 	/* Always wait for free CMD bus. */
 	mask = SDHCI_CMD_INHIBIT;
 	/* Wait for free DAT if we have data or busy signal. */
-	if (cmd->data || (cmd->flags & MMC_RSP_BUSY))
+	if (cmd->data != NULL || (cmd->flags & MMC_RSP_BUSY))
 		mask |= SDHCI_DAT_INHIBIT;
 	/* We shouldn't wait for DAT for stop commands. */
+#ifdef MMCCAM
+	struct ccb_mmcio *mmcio = &slot->ccb->mmcio;
+	if (cmd == &mmcio->stop)
+		mask &= ~SDHCI_DAT_INHIBIT;
+#else
 	if (cmd == slot->req->stop)
 		mask &= ~SDHCI_DAT_INHIBIT;
+#endif
 	/*
 	 *  Wait for bus no more then 250 ms.  Typically there will be no wait
 	 *  here at all, but when writing a crash dump we may be bypassing the
@@ -1145,7 +1289,7 @@ sdhci_start_command(struct sdhci_slot *slot, struct mmc_command *cmd)
 		flags |= SDHCI_CMD_CRC;
 	if (cmd->flags & MMC_RSP_OPCODE)
 		flags |= SDHCI_CMD_INDEX;
-	if (cmd->data)
+	if (cmd->data != NULL)
 		flags |= SDHCI_CMD_DATA;
 	if (cmd->opcode == MMC_STOP_TRANSMISSION)
 		flags |= SDHCI_CMD_TYPE_ABORT;
@@ -1164,6 +1308,8 @@ sdhci_start_command(struct sdhci_slot *slot, struct mmc_command *cmd)
 	WR4(slot, SDHCI_ARGUMENT, cmd->arg);
 	/* Set data transfer mode. */
 	sdhci_set_transfer_mode(slot, cmd->data);
+	if (sdhci_debug > 1)
+		slot_printf(slot, "Starting command!\n");
 	/* Start command. */
 	WR2(slot, SDHCI_COMMAND_FLAGS, (cmd->opcode << 8) | (flags & 0xff));
 	/* Start timeout callout. */
@@ -1178,6 +1324,9 @@ sdhci_finish_command(struct sdhci_slot *slot)
 	uint32_t val;
 	uint8_t extra;
 
+	if (sdhci_debug > 1)
+		slot_printf(slot, "%s: called, err %d flags %d\n",
+			    __func__, slot->curcmd->error, slot->curcmd->flags);
 	slot->cmd_done = 1;
 	/*
 	 * Interrupt aggregation: Restore command interrupt.
@@ -1211,6 +1360,11 @@ sdhci_finish_command(struct sdhci_slot *slot)
 		} else
 			slot->curcmd->resp[0] = RD4(slot, SDHCI_RESPONSE);
 	}
+	if (sdhci_debug > 1)
+		printf("Resp: %02x %02x %02x %02x\n",
+		       slot->curcmd->resp[0], slot->curcmd->resp[1],
+		       slot->curcmd->resp[2], slot->curcmd->resp[3]);
+
 	/* If data ready - finish. */
 	if (slot->data_done)
 		sdhci_start(slot);
@@ -1291,6 +1445,11 @@ sdhci_start_data(struct sdhci_slot *slot, struct mmc_data *data)
 	    (data->len < 512) ? data->len : 512));
 	/* Set block count. */
 	WR2(slot, SDHCI_BLOCK_COUNT, (data->len + 511) / 512);
+
+	if (sdhci_debug > 1)
+		slot_printf(slot, "Block size: %02x, count %lu\n",
+		    (unsigned int)SDHCI_MAKE_BLKSZ(DMA_BOUNDARY, (data->len < 512) ? data->len : 512),
+		    (unsigned long)(data->len + 511) / 512);
 }
 
 void
@@ -1307,7 +1466,8 @@ sdhci_finish_data(struct sdhci_slot *slot)
 		    slot->intmask |= SDHCI_INT_RESPONSE);
 	}
 	/* Unload rest of data from DMA buffer. */
-	if (!slot->data_done && (slot->flags & SDHCI_USE_DMA)) {
+	if (!slot->data_done && (slot->flags & SDHCI_USE_DMA) &&
+	    slot->curcmd->data != NULL) {
 		if (data->flags & MMC_DATA_READ) {
 			left = data->len - slot->offset;
 			bus_dmamap_sync(slot->dmatag, slot->dmamap,
@@ -1331,6 +1491,47 @@ sdhci_finish_data(struct sdhci_slot *slot)
 		sdhci_start(slot);
 }
 
+#ifdef MMCCAM
+static void
+sdhci_start(struct sdhci_slot *slot)
+{
+        union ccb *ccb;
+
+	ccb = slot->ccb;
+	if (ccb == NULL)
+		return;
+
+        struct ccb_mmcio *mmcio;
+	mmcio = &ccb->mmcio;
+
+	if (!(slot->flags & CMD_STARTED)) {
+		slot->flags |= CMD_STARTED;
+		sdhci_start_command(slot, &mmcio->cmd);
+		return;
+	}
+
+	/*
+	 * Old stack doesn't use this!
+	 * Enabling this code causes significant performance degradation
+	 * and IRQ storms on BBB, Wandboard behaves fine.
+	 * Not using this code does no harm...
+	if (!(slot->flags & STOP_STARTED) && mmcio->stop.opcode != 0) {
+		slot->flags |= STOP_STARTED;
+		sdhci_start_command(slot, &mmcio->stop);
+		return;
+	}
+	*/
+	if (sdhci_debug > 1)
+		slot_printf(slot, "result: %d\n", mmcio->cmd.error);
+	if (mmcio->cmd.error == 0 &&
+	    (slot->quirks & SDHCI_QUIRK_RESET_AFTER_REQUEST)) {
+		sdhci_reset(slot, SDHCI_RESET_CMD);
+		sdhci_reset(slot, SDHCI_RESET_DATA);
+	}
+
+	sdhci_req_done(slot);
+}
+#else
 static void
 sdhci_start(struct sdhci_slot *slot)
 {
@@ -1345,23 +1546,25 @@ sdhci_start(struct sdhci_slot *slot)
 		sdhci_start_command(slot, req->cmd);
 		return;
 	}
-/* 	We don't need this until using Auto-CMD12 feature
-	if (!(slot->flags & STOP_STARTED) && req->stop) {
+	if ((slot->quirks & SDHCI_QUIRK_BROKEN_AUTO_STOP) &&
+	    !(slot->flags & STOP_STARTED) && req->stop) {
 		slot->flags |= STOP_STARTED;
 		sdhci_start_command(slot, req->stop);
 		return;
 	}
-*/
 	if (sdhci_debug > 1)
 		slot_printf(slot, "result: %d\n", req->cmd->error);
 	if (!req->cmd->error &&
-	    (slot->quirks & SDHCI_QUIRK_RESET_AFTER_REQUEST)) {
+	    ((slot->curcmd == req->stop &&
+	     (slot->quirks & SDHCI_QUIRK_BROKEN_AUTO_STOP)) ||
+	     (slot->quirks & SDHCI_QUIRK_RESET_AFTER_REQUEST))) {
 		sdhci_reset(slot, SDHCI_RESET_CMD);
 		sdhci_reset(slot, SDHCI_RESET_DATA);
 	}
 
 	sdhci_req_done(slot);
 }
+#endif
 
 int
 sdhci_generic_request(device_t brdev __unused, device_t reqdev,
@@ -1629,6 +1832,10 @@ sdhci_generic_intr(struct sdhci_slot *slot)
 		    "Card is consuming too much power!\n");
 		intmask &= ~SDHCI_INT_BUS_POWER;
 	}
+	/* Handle card interrupt. */
+	if (intmask & SDHCI_INT_CARD_INT) {
+
+	}
 	/* The rest is unknown. */
 	if (intmask) {
 		WR4(slot, SDHCI_INT_STATUS, intmask);
@@ -1712,6 +1919,8 @@ sdhci_generic_write_ivar(device_t bus, device_t child, int which,
 	uint32_t clock, max_clock;
 	int i;
 
+	if (sdhci_debug > 1)
+		slot_printf(slot, "%s: var=%d\n", __func__, which);
 	switch (which) {
 	default:
 		return (EINVAL);
@@ -1777,4 +1986,325 @@ sdhci_generic_write_ivar(device_t bus, device_t child, int which,
 	return (0);
 }
 
+#ifdef MMCCAM
+/* CAM-related functions */
+#include <cam/cam.h>
+#include <cam/cam_ccb.h>
+#include <cam/cam_debug.h>
+#include <cam/cam_sim.h>
+#include <cam/cam_xpt_sim.h>
+
+void
+sdhci_start_slot(struct sdhci_slot *slot)
+{
+        if ((slot->devq = cam_simq_alloc(1)) == NULL) {
+                goto fail;
+        }
+
+        mtx_init(&slot->sim_mtx, "sdhcisim", NULL, MTX_DEF);
+        slot->sim = cam_sim_alloc(sdhci_cam_action, sdhci_cam_poll,
+                                  "sdhci_slot", slot, device_get_unit(slot->bus),
+                                  &slot->sim_mtx, 1, 1, slot->devq);
+
+        if (slot->sim == NULL) {
+                cam_simq_free(slot->devq);
+                slot_printf(slot, "cannot allocate CAM SIM\n");
+                goto fail;
+        }
+
+        mtx_lock(&slot->sim_mtx);
+        if (xpt_bus_register(slot->sim, slot->bus, 0) != 0) {
+                slot_printf(slot,
+                              "cannot register SCSI pass-through bus\n");
+                cam_sim_free(slot->sim, FALSE);
+                cam_simq_free(slot->devq);
+                mtx_unlock(&slot->sim_mtx);
+                goto fail;
+        }
+
+        mtx_unlock(&slot->sim_mtx);
+        /* End CAM-specific init */
+	slot->card_present = 0;
+	sdhci_card_task(slot, 0);
+        return;
+
+fail:
+        if (slot->sim != NULL) {
+                mtx_lock(&slot->sim_mtx);
+                xpt_bus_deregister(cam_sim_path(slot->sim));
+                cam_sim_free(slot->sim, FALSE);
+                mtx_unlock(&slot->sim_mtx);
+        }
+
+        if (slot->devq != NULL)
+                cam_simq_free(slot->devq);
+}
+
+static void
+sdhci_cam_handle_mmcio(struct cam_sim *sim, union ccb *ccb)
+{
+	struct sdhci_slot *slot;
+
+	slot = cam_sim_softc(sim);
+
+	sdhci_cam_request(slot, ccb);
+}
+
+void
+sdhci_cam_action(struct cam_sim *sim, union ccb *ccb)
+{
+	struct sdhci_slot *slot;
+
+	slot = cam_sim_softc(sim);
+	if (slot == NULL) {
+		ccb->ccb_h.status = CAM_SEL_TIMEOUT;
+		xpt_done(ccb);
+		return;
+	}
+
+	mtx_assert(&slot->sim_mtx, MA_OWNED);
+
+	switch (ccb->ccb_h.func_code) {
+	case XPT_PATH_INQ:
+	{
+		struct ccb_pathinq *cpi;
+
+		cpi = &ccb->cpi;
+		cpi->version_num = 1;
+		cpi->hba_inquiry = 0;
+		cpi->target_sprt = 0;
+		cpi->hba_misc = PIM_NOBUSRESET | PIM_SEQSCAN;
+		cpi->hba_eng_cnt = 0;
+		cpi->max_target = 0;
+		cpi->max_lun = 0;
+		cpi->initiator_id = 1;
+		cpi->maxio = MAXPHYS;
+		strncpy(cpi->sim_vid, "FreeBSD", SIM_IDLEN);
+		strncpy(cpi->hba_vid, "Deglitch Networks", HBA_IDLEN);
+		strncpy(cpi->dev_name, cam_sim_name(sim), DEV_IDLEN);
+		cpi->unit_number = cam_sim_unit(sim);
+		cpi->bus_id = cam_sim_bus(sim);
+		cpi->base_transfer_speed = 100; /* XXX WTF? */
+		cpi->protocol = PROTO_MMCSD;
+		cpi->protocol_version = SCSI_REV_0;
+		cpi->transport = XPORT_MMCSD;
+		cpi->transport_version = 0;
+
+		cpi->ccb_h.status = CAM_REQ_CMP;
+		break;
+	}
+	case XPT_GET_TRAN_SETTINGS:
+	{
+		struct ccb_trans_settings *cts = &ccb->cts;
+
+		if (sdhci_debug > 1)
+			slot_printf(slot, "Got XPT_GET_TRAN_SETTINGS\n");
+
+		cts->protocol = PROTO_MMCSD;
+		cts->protocol_version = 1;
+		cts->transport = XPORT_MMCSD;
+		cts->transport_version = 1;
+		cts->xport_specific.valid = 0;
+		cts->proto_specific.mmc.host_ocr = slot->host.host_ocr;
+		cts->proto_specific.mmc.host_f_min = slot->host.f_min;
+		cts->proto_specific.mmc.host_f_max = slot->host.f_max;
+		cts->proto_specific.mmc.host_caps = slot->host.caps;
+		memcpy(&cts->proto_specific.mmc.ios, &slot->host.ios, sizeof(struct mmc_ios));
+		ccb->ccb_h.status = CAM_REQ_CMP;
+		break;
+	}
+	case XPT_SET_TRAN_SETTINGS:
+	{
+		if (sdhci_debug > 1)
+			slot_printf(slot, "Got XPT_SET_TRAN_SETTINGS\n");
+		sdhci_cam_settran_settings(slot, ccb);
+		ccb->ccb_h.status = CAM_REQ_CMP;
+		break;
+	}
+	case XPT_RESET_BUS:
+		if (sdhci_debug > 1)
+			slot_printf(slot, "Got XPT_RESET_BUS, ACK it...\n");
+		ccb->ccb_h.status = CAM_REQ_CMP;
+		break;
+	case XPT_MMC_IO:
+		/*
+		 * Here is the HW-dependent part of
+		 * sending the command to the underlying h/w
+		 * At some point in the future an interrupt comes.
+		 * Then the request will be marked as completed.
+		 */
+		if (sdhci_debug > 1)
+			slot_printf(slot, "Got XPT_MMC_IO\n");
+		ccb->ccb_h.status = CAM_REQ_INPROG;
+
+		sdhci_cam_handle_mmcio(sim, ccb);
+		return;
+		/* NOTREACHED */
+		break;
+	default:
+		ccb->ccb_h.status = CAM_REQ_INVALID;
+		break;
+	}
+	xpt_done(ccb);
+	return;
+}
+
+void
+sdhci_cam_poll(struct cam_sim *sim)
+{
+	return;
+}
+
+int sdhci_cam_get_possible_host_clock(struct sdhci_slot *slot, int proposed_clock) {
+	int max_clock, clock, i;
+
+	if (proposed_clock == 0)
+		return 0;
+	max_clock = slot->max_clk;
+	clock = max_clock;
+
+	if (slot->version < SDHCI_SPEC_300) {
+		for (i = 0; i < SDHCI_200_MAX_DIVIDER;
+		     i <<= 1) {
+			if (clock <= proposed_clock)
+				break;
+			clock >>= 1;
+		}
+	} else {
+		for (i = 0; i < SDHCI_300_MAX_DIVIDER;
+		     i += 2) {
+			if (clock <= proposed_clock)
+				break;
+			clock = max_clock / (i + 2);
+		}
+	}
+	return clock;
+}
+
+int
+sdhci_cam_settran_settings(struct sdhci_slot *slot, union ccb *ccb)
+{
+	struct mmc_ios *ios;
+	struct mmc_ios *new_ios;
+	struct ccb_trans_settings_mmc *cts;
+
+	ios = &slot->host.ios;
+
+	cts = &ccb->cts.proto_specific.mmc;
+	new_ios = &cts->ios;
+
+	/* Update only requested fields */
+	if (cts->ios_valid & MMC_CLK) {
+		ios->clock = sdhci_cam_get_possible_host_clock(slot, new_ios->clock);
+		slot_printf(slot, "Clock => %d\n", ios->clock);
+	}
+	if (cts->ios_valid & MMC_VDD) {
+		ios->vdd = new_ios->vdd;
+		slot_printf(slot, "VDD => %d\n", ios->vdd);
+	}
+	if (cts->ios_valid & MMC_CS) {
+		ios->chip_select = new_ios->chip_select;
+		slot_printf(slot, "CS => %d\n", ios->chip_select);
+	}
+	if (cts->ios_valid & MMC_BW) {
+		ios->bus_width = new_ios->bus_width;
+		slot_printf(slot, "Bus width => %d\n", ios->bus_width);
+	}
+	if (cts->ios_valid & MMC_PM) {
+		ios->power_mode = new_ios->power_mode;
+		slot_printf(slot, "Power mode => %d\n", ios->power_mode);
+	}
+	if (cts->ios_valid & MMC_BT) {
+		ios->timing = new_ios->timing;
+		slot_printf(slot, "Timing => %d\n", ios->timing);
+	}
+	if (cts->ios_valid & MMC_BM) {
+		ios->bus_mode = new_ios->bus_mode;
+		slot_printf(slot, "Bus mode => %d\n", ios->bus_mode);
+	}
+
+        /* XXX Provide a way to call a chip-specific IOS update, required for TI */
+	return (sdhci_cam_update_ios(slot));
+}
+
+int
+sdhci_cam_update_ios(struct sdhci_slot *slot)
+{
+	struct mmc_ios *ios = &slot->host.ios;
+
+	slot_printf(slot, "%s: power_mode=%d, clk=%d, bus_width=%d, timing=%d\n",
+		    __func__, ios->power_mode, ios->clock, ios->bus_width, ios->timing);
+	SDHCI_LOCK(slot);
+	/* Do full reset on bus power down to clear from any state. */
+	if (ios->power_mode == power_off) {
+		WR4(slot, SDHCI_SIGNAL_ENABLE, 0);
+		sdhci_init(slot);
+	}
+	/* Configure the bus. */
+	sdhci_set_clock(slot, ios->clock);
+	sdhci_set_power(slot, (ios->power_mode == power_off) ? 0 : ios->vdd);
+	if (ios->bus_width == bus_width_8) {
+		slot->hostctrl |= SDHCI_CTRL_8BITBUS;
+		slot->hostctrl &= ~SDHCI_CTRL_4BITBUS;
+	} else if (ios->bus_width == bus_width_4) {
+		slot->hostctrl &= ~SDHCI_CTRL_8BITBUS;
+		slot->hostctrl |= SDHCI_CTRL_4BITBUS;
+	} else if (ios->bus_width == bus_width_1) {
+		slot->hostctrl &= ~SDHCI_CTRL_8BITBUS;
+		slot->hostctrl &= ~SDHCI_CTRL_4BITBUS;
+	} else {
+		panic("Invalid bus width: %d", ios->bus_width);
+	}
+	if (ios->timing == bus_timing_hs &&
+	    !(slot->quirks & SDHCI_QUIRK_DONT_SET_HISPD_BIT))
+		slot->hostctrl |= SDHCI_CTRL_HISPD;
+	else
+		slot->hostctrl &= ~SDHCI_CTRL_HISPD;
+	WR1(slot, SDHCI_HOST_CONTROL, slot->hostctrl);
+	/* Some controllers like reset after bus changes. */
+	if(slot->quirks & SDHCI_QUIRK_RESET_ON_IOS)
+		sdhci_reset(slot, SDHCI_RESET_CMD | SDHCI_RESET_DATA);
+
+	SDHCI_UNLOCK(slot);
+	return (0);
+}
+
+int
+sdhci_cam_request(struct sdhci_slot *slot, union ccb *ccb)
+{
+	struct ccb_mmcio *mmcio;
+
+	mmcio = &ccb->mmcio;
+
+	SDHCI_LOCK(slot);
+/*	if (slot->req != NULL) {
+		SDHCI_UNLOCK(slot);
+		return (EBUSY);
+	}
+*/
+	if (sdhci_debug > 1) {
+		slot_printf(slot, "CMD%u arg %#x flags %#x dlen %u dflags %#x\n",
+			    mmcio->cmd.opcode, mmcio->cmd.arg, mmcio->cmd.flags,
+			    mmcio->cmd.data != NULL ? (unsigned int) mmcio->cmd.data->len : 0,
+			    mmcio->cmd.data != NULL ? mmcio->cmd.data->flags: 0);
+	}
+	if (mmcio->cmd.data != NULL) {
+		if (mmcio->cmd.data->len == 0 || mmcio->cmd.data->flags == 0)
+			panic("data->len = %d, data->flags = %d -- something is b0rked",
+			      (int)mmcio->cmd.data->len, mmcio->cmd.data->flags);
+	}
+	slot->ccb = ccb;
+	slot->flags = 0;
+	sdhci_start(slot);
+	SDHCI_UNLOCK(slot);
+	if (dumping) {
+		while (slot->ccb != NULL) {
+			sdhci_generic_intr(slot);
+			DELAY(10);
+		}
+	}
+	return (0);
+}
+#endif /* MMCCAM */
+
 MODULE_VERSION(sdhci, 1);
diff --git a/freebsd/sys/dev/sdhci/sdhci.h b/freebsd/sys/dev/sdhci/sdhci.h
index 0b29915..a2e221f 100644
--- a/freebsd/sys/dev/sdhci/sdhci.h
+++ b/freebsd/sys/dev/sdhci/sdhci.h
@@ -28,6 +28,8 @@
 #ifndef	__SDHCI_H__
 #define	__SDHCI_H__
 
+#include <rtems/bsd/local/opt_mmccam.h>
+
 #define	DMA_BLOCK_SIZE	4096
 #define	DMA_BOUNDARY	0	/* DMA reload every 4K */
 
@@ -87,6 +89,8 @@
 #define	SDHCI_QUIRK_CAPS_BIT63_FOR_MMC_HS400		(1 << 26)
 /* Controller support for SDHCI_CTRL2_PRESET_VALUE is broken. */
 #define	SDHCI_QUIRK_PRESET_VALUE_BROKEN			(1 << 27)
+/* Controller does not support or the support for ACMD12 is broken. */
+#define	SDHCI_QUIRK_BROKEN_AUTO_STOP			(1 << 28)
 
 /*
  * Controller registers
@@ -365,6 +369,15 @@ struct sdhci_slot {
 #define	SDHCI_USE_DMA		4	/* Use DMA for this req. */
 #define	PLATFORM_DATA_STARTED	8	/* Data xfer is handled by platform */
 	struct mtx	mtx;		/* Slot mutex */
+
+#ifdef MMCCAM
+	/* CAM stuff */
+	union ccb	*ccb;
+	struct cam_devq		*devq;
+	struct cam_sim		*sim;
+	struct mtx		sim_mtx;
+	u_char			card_present; /* XXX Maybe derive this from elsewhere? */
+#endif
 };
 
 int sdhci_generic_read_ivar(device_t bus, device_t child, int which,
diff --git a/freebsd/sys/sys/devicestat.h b/freebsd/sys/sys/devicestat.h
new file mode 100644
index 0000000..bce0570
--- /dev/null
+++ b/freebsd/sys/sys/devicestat.h
@@ -0,0 +1,206 @@
+/*-
+ * Copyright (c) 1997, 1998, 1999 Kenneth D. Merry.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _DEVICESTAT_H
+#define _DEVICESTAT_H
+
+#include <sys/queue.h>
+#include <sys/time.h>
+
+/*
+ * XXX: Should really be SPECNAMELEN
+ */
+#define DEVSTAT_NAME_LEN  16
+
+/*
+ * device name for the mmap device
+ */
+#define DEVSTAT_DEVICE_NAME "devstat"
+
+/*
+ * ATTENTION:  The devstat version below should be incremented any time a
+ * change is made in struct devstat, or any time a change is made in the
+ * enumerated types that struct devstat uses.  (Only if those changes
+ * would require a recompile -- i.e. re-arranging the order of an
+ * enumerated type or something like that.)  This version number is used by
+ * userland utilities to determine whether or not they are in sync with the
+ * kernel.
+ */
+#define DEVSTAT_VERSION	   6
+
+/*
+ * These flags specify which statistics features are supported or not
+ * supported by a particular device.  The default is all statistics are
+ * supported.
+ */
+typedef enum {
+	DEVSTAT_ALL_SUPPORTED	= 0x00,
+	DEVSTAT_NO_BLOCKSIZE	= 0x01,
+	DEVSTAT_NO_ORDERED_TAGS	= 0x02,
+	DEVSTAT_BS_UNAVAILABLE	= 0x04
+} devstat_support_flags;
+
+typedef enum {
+	DEVSTAT_NO_DATA	= 0x00,
+	DEVSTAT_READ	= 0x01,
+	DEVSTAT_WRITE	= 0x02,
+	DEVSTAT_FREE	= 0x03
+} devstat_trans_flags;
+#define DEVSTAT_N_TRANS_FLAGS	4
+
+typedef enum {
+	DEVSTAT_TAG_SIMPLE	= 0x00,
+	DEVSTAT_TAG_HEAD	= 0x01,
+	DEVSTAT_TAG_ORDERED	= 0x02,
+	DEVSTAT_TAG_NONE	= 0x03
+} devstat_tag_type;
+
+typedef enum {
+	DEVSTAT_PRIORITY_MIN	= 0x000,
+	DEVSTAT_PRIORITY_OTHER	= 0x020,
+	DEVSTAT_PRIORITY_PASS	= 0x030,
+	DEVSTAT_PRIORITY_FD	= 0x040,
+	DEVSTAT_PRIORITY_WFD	= 0x050,
+	DEVSTAT_PRIORITY_TAPE	= 0x060,
+	DEVSTAT_PRIORITY_CD	= 0x090,
+	DEVSTAT_PRIORITY_DISK	= 0x110,
+	DEVSTAT_PRIORITY_ARRAY	= 0x120,
+	DEVSTAT_PRIORITY_MAX	= 0xfff
+} devstat_priority;
+
+/*
+ * These types are intended to aid statistics gathering/display programs.
+ * The first 13 types (up to the 'target' flag) are identical numerically
+ * to the SCSI device type numbers.  The next 3 types designate the device
+ * interface.  Currently the choices are IDE, SCSI, and 'other'.  The last
+ * flag specifies whether or not the given device is a passthrough device
+ * or not.  If it is a passthrough device, the lower 4 bits specify which
+ * type of physical device lies under the passthrough device, and the next
+ * 4 bits specify the interface.
+ */
+typedef enum {
+	DEVSTAT_TYPE_DIRECT	= 0x000,
+	DEVSTAT_TYPE_SEQUENTIAL	= 0x001,
+	DEVSTAT_TYPE_PRINTER	= 0x002,
+	DEVSTAT_TYPE_PROCESSOR	= 0x003,
+	DEVSTAT_TYPE_WORM	= 0x004,
+	DEVSTAT_TYPE_CDROM	= 0x005,
+	DEVSTAT_TYPE_SCANNER	= 0x006,
+	DEVSTAT_TYPE_OPTICAL	= 0x007,
+	DEVSTAT_TYPE_CHANGER	= 0x008,
+	DEVSTAT_TYPE_COMM	= 0x009,
+	DEVSTAT_TYPE_ASC0	= 0x00a,
+	DEVSTAT_TYPE_ASC1	= 0x00b,
+	DEVSTAT_TYPE_STORARRAY	= 0x00c,
+	DEVSTAT_TYPE_ENCLOSURE	= 0x00d,
+	DEVSTAT_TYPE_FLOPPY	= 0x00e,
+	DEVSTAT_TYPE_MASK	= 0x00f,
+	DEVSTAT_TYPE_IF_SCSI	= 0x010,
+	DEVSTAT_TYPE_IF_IDE	= 0x020,
+	DEVSTAT_TYPE_IF_OTHER	= 0x030,
+	DEVSTAT_TYPE_IF_MASK	= 0x0f0,
+	DEVSTAT_TYPE_PASS	= 0x100
+} devstat_type_flags;
+
+/*
+ * XXX: Next revision should add
+ *	off_t		offset[DEVSTAT_N_TRANS_FLAGS];
+ * XXX: which should contain the offset of the last completed transfer.
+ */
+struct devstat {
+	/* Internal house-keeping fields */
+	u_int			sequence0;	     /* Update sequence# */
+	int			allocated;	     /* Allocated entry */
+	u_int			start_count;	     /* started ops */
+	u_int			end_count;	     /* completed ops */
+	struct bintime		busy_from;	     /*
+						      * busy time unaccounted
+						      * for since this time
+						      */
+	STAILQ_ENTRY(devstat) 	dev_links;
+	u_int32_t		device_number;	     /*
+						      * Devstat device
+						      * number.
+						      */
+	char			device_name[DEVSTAT_NAME_LEN];
+	int			unit_number;
+	u_int64_t		bytes[DEVSTAT_N_TRANS_FLAGS];
+	u_int64_t		operations[DEVSTAT_N_TRANS_FLAGS];
+	struct bintime		duration[DEVSTAT_N_TRANS_FLAGS];
+	struct bintime		busy_time;
+	struct bintime          creation_time;       /* 
+						      * Time the device was
+						      * created.
+						      */
+	u_int32_t		block_size;	     /* Block size, bytes */
+	u_int64_t		tag_types[3];	     /*
+						      * The number of
+						      * simple, ordered, 
+						      * and head of queue 
+						      * tags sent.
+						      */
+	devstat_support_flags	flags;		     /*
+						      * Which statistics
+						      * are supported by a 
+						      * given device.
+						      */
+	devstat_type_flags	device_type;	     /* Device type */
+	devstat_priority	priority;	     /* Controls list pos. */
+	const void		*id;		     /*
+						      * Identification for
+						      * GEOM nodes
+						      */
+	u_int			sequence1;	     /* Update sequence# */
+};
+
+STAILQ_HEAD(devstatlist, devstat);
+
+#ifdef _KERNEL
+struct bio;
+
+struct devstat *devstat_new_entry(const void *dev_name, int unit_number,
+				  u_int32_t block_size,
+				  devstat_support_flags flags,
+				  devstat_type_flags device_type,
+				  devstat_priority priority);
+
+void devstat_remove_entry(struct devstat *ds);
+void devstat_start_transaction(struct devstat *ds, struct bintime *now);
+void devstat_start_transaction_bio(struct devstat *ds, struct bio *bp);
+void devstat_end_transaction(struct devstat *ds, u_int32_t bytes, 
+			     devstat_tag_type tag_type,
+			     devstat_trans_flags flags,
+			     struct bintime *now, struct bintime *then);
+void devstat_end_transaction_bio(struct devstat *ds, struct bio *bp);
+void devstat_end_transaction_bio_bt(struct devstat *ds, struct bio *bp,
+			     struct bintime *now);
+#endif
+
+#endif /* _DEVICESTAT_H */
diff --git a/libbsd.py b/libbsd.py
index 05e9dd5..2a3abcc 100644
--- a/libbsd.py
+++ b/libbsd.py
@@ -699,6 +699,50 @@ class mmc_ti(builder.Module):
         )
 
 #
+# MMCCAM
+#
+class mmccam(builder.Module):
+
+   def __init__(self, manager):
+       super(mmccam, self).__init__(manager, type(self).__name__)
+
+   def generate(self):
+       mm = self.manager
+       self.addKernelSpaceHeaderFiles(
+           [
+               'sys/cam/cam.h',
+               'sys/cam/cam_ccb.h',
+               'sys/cam/cam_periph.h',
+               'sys/cam/cam_xpt.h',
+               'sys/cam/cam_compat.h',
+               'sys/cam/cam_debug.h',
+               'sys/cam/cam_queue.h',
+               'sys/cam/mmc/mmc.h',
+               'sys/cam/mmc/mmc_all.h',
+               'sys/cam/mmc/mmc_bus.h',
+               'sys/dev/mmc/bridge.h',
+               'sys/dev/mmc/mmcbrvar.h',
+               'sys/dev/mmc/mmcreg.h',
+               'sys/dev/sdhci/sdhci.h',
+               'sys/arm/include/md_var.h',
+               'sys/sys/devicestat.h',
+           ]
+       )
+       self.addKernelSpaceSourceFiles(
+           [
+               'sys/arm/ti/ti_sdhci.c',
+               'sys/cam/cam_periph.c',
+               'sys/cam/cam_xpt.c',
+               'sys/cam/cam_compat.c',
+               'sys/cam/cam_queue.c',
+               'sys/cam/mmc/mmc_da.c',
+               'sys/cam/mmc/mmc_xpt.c',
+               'sys/dev/sdhci/sdhci.c',
+           ],
+           mm.generator['source']()
+       )
+
+#
 # Input
 #
 class dev_input(builder.Module):
@@ -4791,6 +4835,7 @@ def load(mm):
     mm.addModule(tty(mm))
     mm.addModule(mmc(mm))
     mm.addModule(mmc_ti(mm))
+    mm.addModule(mmccam(mm))
     mm.addModule(dev_input(mm))
     mm.addModule(evdev(mm))
 
diff --git a/rtemsbsd/include/bsp/nexus-devices.h b/rtemsbsd/include/bsp/nexus-devices.h
index f700392..cd6682d 100644
--- a/rtemsbsd/include/bsp/nexus-devices.h
+++ b/rtemsbsd/include/bsp/nexus-devices.h
@@ -69,7 +69,11 @@ SYSINIT_MODULE_REFERENCE(wlan_tkip);
 SYSINIT_MODULE_REFERENCE(wlan_ccmp);
 SYSINIT_REFERENCE(rtwn_rtl8188eufw);
 #endif /* RTEMS_BSD_MODULE_IEEE80211 */
-
+#ifdef RTEMS_BSD_MODULE_MMCCAM
+SYSINIT_MODULE_REFERENCE(cam);
+SYSINIT_MODULE_REFERENCE(mmcprobe);
+SYSINIT_MODULE_REFERENCE(sdda);
+#endif /* RTEMS_BSD_MODULE_MMCCAM */
 RTEMS_BSD_DRIVER_USB;
 RTEMS_BSD_DRIVER_USB_MASS;
 
diff --git a/rtemsbsd/include/cam/cam_queue.h b/rtemsbsd/include/cam/cam_queue.h
index 936ffd8..ea1ef79 100644
--- a/rtemsbsd/include/cam/cam_queue.h
+++ b/rtemsbsd/include/cam/cam_queue.h
@@ -1 +1,293 @@
-/* EMPTY */
+/*-
+ * CAM request queue management definitions.
+ *
+ * Copyright (c) 1997 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _CAM_CAM_QUEUE_H
+#define _CAM_CAM_QUEUE_H 1
+
+#ifdef _KERNEL
+
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/queue.h>
+#include <cam/cam.h>
+
+/*
+ * This structure implements a heap based priority queue.  The queue
+ * assumes that the objects stored in it begin with a cam_qentry
+ * structure holding the priority information used to sort the objects.
+ * This structure is opaque to clients (outside of the XPT layer) to allow
+ * the implementation to change without affecting them.
+ */
+struct camq {
+	cam_pinfo **queue_array;
+	int	   array_size;
+	int	   entries;
+	u_int32_t  generation;
+	u_int32_t  qfrozen_cnt;
+};
+
+TAILQ_HEAD(ccb_hdr_tailq, ccb_hdr);
+LIST_HEAD(ccb_hdr_list, ccb_hdr);
+SLIST_HEAD(ccb_hdr_slist, ccb_hdr);
+
+struct cam_ccbq {
+	struct	camq queue;
+	struct ccb_hdr_tailq	queue_extra_head;
+	int	queue_extra_entries;
+	int	total_openings;
+	int	allocated;
+	int	dev_openings;
+	int	dev_active;
+};
+
+struct cam_ed;
+
+struct cam_devq {
+	struct mtx	 send_mtx;
+	struct camq	 send_queue;
+	int		 send_openings;
+	int		 send_active;
+};
+
+
+struct cam_devq *cam_devq_alloc(int devices, int openings);
+
+int		 cam_devq_init(struct cam_devq *devq, int devices,
+			       int openings);
+
+void		 cam_devq_free(struct cam_devq *devq);
+
+u_int32_t	 cam_devq_resize(struct cam_devq *camq, int openings);
+	
+/*
+ * Allocate a cam_ccb_queue structure and initialize it.
+ */
+struct cam_ccbq	*cam_ccbq_alloc(int openings);
+
+u_int32_t	cam_ccbq_resize(struct cam_ccbq *ccbq, int devices);
+
+int		cam_ccbq_init(struct cam_ccbq *ccbq, int openings);
+
+void		cam_ccbq_free(struct cam_ccbq *ccbq);
+
+void		cam_ccbq_fini(struct cam_ccbq *ccbq);
+
+/*
+ * Allocate and initialize a cam_queue structure.
+ */
+struct camq	*camq_alloc(int size);
+
+/*
+ * Resize a cam queue
+ */
+u_int32_t	camq_resize(struct camq *queue, int new_size);
+
+/* 
+ * Initialize a camq structure.  Return 0 on success, 1 on failure.
+ */
+int		camq_init(struct camq *camq, int size);
+
+/*
+ * Free a cam_queue structure.  This should only be called if a controller
+ * driver failes somehow during its attach routine or is unloaded and has
+ * obtained a cam_queue structure.
+ */
+void		camq_free(struct camq *queue);
+
+/*
+ * Finialize any internal storage or state of a cam_queue.
+ */
+void		camq_fini(struct camq *queue);
+
+/*
+ * cam_queue_insert: Given a CAM queue with at least one open spot,
+ * insert the new entry maintaining order.
+ */
+void		camq_insert(struct camq *queue, cam_pinfo *new_entry);
+
+/*
+ * camq_remove: Remove and arbitrary entry from the queue maintaining
+ * queue order.
+ */
+cam_pinfo	*camq_remove(struct camq *queue, int index);
+#define CAMQ_HEAD 1	/* Head of queue index */
+
+/* Index the first element in the heap */
+#define CAMQ_GET_HEAD(camq) ((camq)->queue_array[CAMQ_HEAD])
+
+/* Get the first element priority. */
+#define CAMQ_GET_PRIO(camq) (((camq)->entries > 0) ?			\
+			    ((camq)->queue_array[CAMQ_HEAD]->priority) : 0)
+
+/*
+ * camq_change_priority: Raise or lower the priority of an entry
+ * maintaining queue order.
+ */
+void		camq_change_priority(struct camq *queue, int index,
+				     u_int32_t new_priority);
+
+static __inline int
+cam_ccbq_pending_ccb_count(struct cam_ccbq *ccbq);
+
+static __inline void
+cam_ccbq_take_opening(struct cam_ccbq *ccbq);
+
+static __inline void
+cam_ccbq_insert_ccb(struct cam_ccbq *ccbq, union ccb *new_ccb);
+
+static __inline void
+cam_ccbq_remove_ccb(struct cam_ccbq *ccbq, union ccb *ccb);
+
+static __inline union ccb *
+cam_ccbq_peek_ccb(struct cam_ccbq *ccbq, int index);
+
+static __inline void
+cam_ccbq_send_ccb(struct cam_ccbq *queue, union ccb *send_ccb);
+
+static __inline void
+cam_ccbq_ccb_done(struct cam_ccbq *ccbq, union ccb *done_ccb);
+
+static __inline void
+cam_ccbq_release_opening(struct cam_ccbq *ccbq);
+
+
+static __inline int
+cam_ccbq_pending_ccb_count(struct cam_ccbq *ccbq)
+{
+	return (ccbq->queue.entries + ccbq->queue_extra_entries);
+}
+
+static __inline void
+cam_ccbq_take_opening(struct cam_ccbq *ccbq)
+{
+	printk("chkpnt#4\n");
+	ccbq->allocated++;
+	printk("chkpnt#\n");
+}
+
+static __inline void
+cam_ccbq_insert_ccb(struct cam_ccbq *ccbq, union ccb *new_ccb)
+{
+	struct ccb_hdr *old_ccb;
+	struct camq *queue = &ccbq->queue;
+
+	KASSERT((new_ccb->ccb_h.func_code & XPT_FC_QUEUED) != 0 &&
+	    (new_ccb->ccb_h.func_code & XPT_FC_USER_CCB) == 0,
+	    ("%s: Cannot queue ccb %p func_code %#x", __func__, new_ccb,
+	     new_ccb->ccb_h.func_code));
+
+	/*
+	 * If queue is already full, try to resize.
+	 * If resize fail, push CCB with lowest priority out to the TAILQ.
+	 */
+	if (queue->entries == queue->array_size &&
+	    camq_resize(&ccbq->queue, queue->array_size * 2) != CAM_REQ_CMP) {
+		old_ccb = (struct ccb_hdr *)camq_remove(queue, queue->entries);
+		TAILQ_INSERT_HEAD(&ccbq->queue_extra_head, old_ccb,
+		    xpt_links.tqe);
+		old_ccb->pinfo.index = CAM_EXTRAQ_INDEX;
+		ccbq->queue_extra_entries++;
+	}
+
+	camq_insert(queue, &new_ccb->ccb_h.pinfo);
+}
+
+static __inline void
+cam_ccbq_remove_ccb(struct cam_ccbq *ccbq, union ccb *ccb)
+{
+	struct ccb_hdr *cccb, *bccb;
+	struct camq *queue = &ccbq->queue;
+	cam_pinfo *removed_entry __unused;
+
+	/* If the CCB is on the TAILQ, remove it from there. */
+	if (ccb->ccb_h.pinfo.index == CAM_EXTRAQ_INDEX) {
+		TAILQ_REMOVE(&ccbq->queue_extra_head, &ccb->ccb_h,
+		    xpt_links.tqe);
+		ccb->ccb_h.pinfo.index = CAM_UNQUEUED_INDEX;
+		ccbq->queue_extra_entries--;
+		return;
+	}
+
+	removed_entry = camq_remove(queue, ccb->ccb_h.pinfo.index);
+	KASSERT(removed_entry == &ccb->ccb_h.pinfo,
+	    ("%s: Removed wrong entry from queue (%p != %p)", __func__,
+	     removed_entry, &ccb->ccb_h.pinfo));
+
+	/*
+	 * If there are some CCBs on TAILQ, find the best one and move it
+	 * to the emptied space in the queue.
+	 */
+	bccb = TAILQ_FIRST(&ccbq->queue_extra_head);
+	if (bccb == NULL)
+		return;
+	TAILQ_FOREACH(cccb, &ccbq->queue_extra_head, xpt_links.tqe) {
+		if (bccb->pinfo.priority > cccb->pinfo.priority ||
+		    (bccb->pinfo.priority == cccb->pinfo.priority &&
+		     GENERATIONCMP(bccb->pinfo.generation, >,
+		      cccb->pinfo.generation)))
+		        bccb = cccb;
+	}
+	TAILQ_REMOVE(&ccbq->queue_extra_head, bccb, xpt_links.tqe);
+	ccbq->queue_extra_entries--;
+	camq_insert(queue, &bccb->pinfo);
+}
+
+static __inline union ccb *
+cam_ccbq_peek_ccb(struct cam_ccbq *ccbq, int index)
+{
+	return((union ccb *)ccbq->queue.queue_array[index]);
+}
+
+static __inline void
+cam_ccbq_send_ccb(struct cam_ccbq *ccbq, union ccb *send_ccb)
+{
+
+	send_ccb->ccb_h.pinfo.index = CAM_ACTIVE_INDEX;
+	ccbq->dev_active++;
+	ccbq->dev_openings--;
+}
+
+static __inline void
+cam_ccbq_ccb_done(struct cam_ccbq *ccbq, union ccb *done_ccb)
+{
+
+	ccbq->dev_active--;
+	ccbq->dev_openings++;
+}
+
+static __inline void
+cam_ccbq_release_opening(struct cam_ccbq *ccbq)
+{
+
+	ccbq->allocated--;
+}
+
+#endif /* _KERNEL */
+#endif  /* _CAM_CAM_QUEUE_H */
diff --git a/rtemsbsd/include/cam/cam_xpt_internal.h b/rtemsbsd/include/cam/cam_xpt_internal.h
index 936ffd8..931727d 100644
--- a/rtemsbsd/include/cam/cam_xpt_internal.h
+++ b/rtemsbsd/include/cam/cam_xpt_internal.h
@@ -1 +1,211 @@
-/* EMPTY */
+/*-
+ * Copyright 2009 Scott Long
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _CAM_CAM_XPT_INTERNAL_H
+#define _CAM_CAM_XPT_INTERNAL_H 1
+
+#include <sys/taskqueue.h>
+
+/* Forward Declarations */
+struct cam_eb;
+struct cam_et;
+struct cam_ed;
+
+typedef struct cam_ed * (*xpt_alloc_device_func)(struct cam_eb *bus,
+					         struct cam_et *target,
+					         lun_id_t lun_id);
+typedef void (*xpt_release_device_func)(struct cam_ed *device);
+typedef void (*xpt_action_func)(union ccb *start_ccb);
+typedef void (*xpt_dev_async_func)(u_int32_t async_code,
+				   struct cam_eb *bus,
+				   struct cam_et *target,
+				   struct cam_ed *device,
+				   void *async_arg);
+typedef void (*xpt_announce_periph_func)(struct cam_periph *periph);
+
+struct xpt_xport_ops {
+	xpt_alloc_device_func	alloc_device;
+	xpt_release_device_func	reldev;
+	xpt_action_func		action;
+	xpt_dev_async_func	async;
+	xpt_announce_periph_func announce;
+};
+
+struct xpt_xport {
+	cam_xport		xport;
+	const char		*name;
+	struct xpt_xport_ops	*ops;
+};
+
+RTEMS_BSD_DECLARE_SET(cam_xpt_xport_set, struct xpt_xport);
+#define CAM_XPT_XPORT(data) 				\
+	DATA_SET(cam_xpt_xport_set, data)
+
+typedef void (*xpt_proto_announce_func)(struct cam_ed *);
+typedef void (*xpt_proto_debug_out_func)(union ccb *);
+
+struct xpt_proto_ops {
+	xpt_proto_announce_func	announce;
+	xpt_proto_announce_func	denounce;
+	xpt_proto_debug_out_func debug_out;
+};
+
+struct xpt_proto {
+	cam_proto		proto;
+	const char		*name;
+	struct xpt_proto_ops	*ops;
+};
+
+RTEMS_BSD_DECLARE_SET(cam_xpt_proto_set, struct xpt_proto);
+#define CAM_XPT_PROTO(data) 				\
+	DATA_SET(cam_xpt_proto_set, data)
+
+/*
+ * The CAM EDT (Existing Device Table) contains the device information for
+ * all devices for all buses in the system.  The table contains a
+ * cam_ed structure for each device on the bus.
+ */
+struct cam_ed {
+	cam_pinfo	 devq_entry;
+	TAILQ_ENTRY(cam_ed) links;
+	struct	cam_et	 *target;
+	struct	cam_sim  *sim;
+	lun_id_t	 lun_id;
+	struct	cam_ccbq ccbq;		/* Queue of pending ccbs */
+	struct	async_list asyncs;	/* Async callback info for this B/T/L */
+	struct	periph_list periphs;	/* All attached devices */
+	u_int	generation;		/* Generation number */
+	void		 *quirk;	/* Oddities about this device */
+	u_int		 maxtags;
+	u_int		 mintags;
+	cam_proto	 protocol;
+	u_int		 protocol_version;
+	cam_xport	 transport;
+	u_int		 transport_version;
+	struct		 scsi_inquiry_data inq_data;
+	uint8_t		 *supported_vpds;
+	uint8_t		 supported_vpds_len;
+	uint32_t	 device_id_len;
+	uint8_t		 *device_id;
+	uint32_t	 ext_inq_len;
+	uint8_t		 *ext_inq;
+	uint8_t		 physpath_len;
+	uint8_t		 *physpath;	/* physical path string form */
+	uint32_t	 rcap_len;
+	uint8_t		 *rcap_buf;
+	struct		 ata_params ident_data;
+        struct		 mmc_params mmc_ident_data;
+	u_int8_t	 inq_flags;	/*
+					 * Current settings for inquiry flags.
+					 * This allows us to override settings
+					 * like disconnection and tagged
+					 * queuing for a device.
+					 */
+	u_int8_t	 queue_flags;	/* Queue flags from the control page */
+	u_int8_t	 serial_num_len;
+	u_int8_t	*serial_num;
+	u_int32_t	 flags;
+#define CAM_DEV_UNCONFIGURED	 	0x01
+#define CAM_DEV_REL_TIMEOUT_PENDING	0x02
+#define CAM_DEV_REL_ON_COMPLETE		0x04
+#define CAM_DEV_REL_ON_QUEUE_EMPTY	0x08
+#define CAM_DEV_TAG_AFTER_COUNT		0x20
+#define CAM_DEV_INQUIRY_DATA_VALID	0x40
+#define	CAM_DEV_IN_DV			0x80
+#define	CAM_DEV_DV_HIT_BOTTOM		0x100
+#define CAM_DEV_IDENTIFY_DATA_VALID	0x200
+	u_int32_t	 tag_delay_count;
+#define	CAM_TAG_DELAY_COUNT		5
+	u_int32_t	 tag_saved_openings;
+	u_int32_t	 refcount;
+	struct callout	 callout;
+	STAILQ_ENTRY(cam_ed) highpowerq_entry;
+	struct mtx	 device_mtx;
+	struct task	 device_destroy_task;
+	const struct	 nvme_controller_data *nvme_cdata;
+	const struct	 nvme_namespace_data *nvme_data;
+};
+
+/*
+ * Each target is represented by an ET (Existing Target).  These
+ * entries are created when a target is successfully probed with an
+ * identify, and removed when a device fails to respond after a number
+ * of retries, or a bus rescan finds the device missing.
+ */
+struct cam_et {
+	TAILQ_HEAD(, cam_ed) ed_entries;
+	TAILQ_ENTRY(cam_et) links;
+	struct	cam_eb	*bus;
+	target_id_t	target_id;
+	u_int32_t	refcount;
+	u_int		generation;
+	struct		timeval last_reset;
+	u_int		rpl_size;
+	struct scsi_report_luns_data *luns;
+	struct mtx	luns_mtx;	/* Protection for luns field. */
+};
+
+/*
+ * Each bus is represented by an EB (Existing Bus).  These entries
+ * are created by calls to xpt_bus_register and deleted by calls to
+ * xpt_bus_deregister.
+ */
+struct cam_eb {
+	TAILQ_HEAD(, cam_et) et_entries;
+	TAILQ_ENTRY(cam_eb)  links;
+	path_id_t	     path_id;
+	struct cam_sim	     *sim;
+	struct timeval	     last_reset;
+	u_int32_t	     flags;
+#define	CAM_EB_RUNQ_SCHEDULED	0x01
+	u_int32_t	     refcount;
+	u_int		     generation;
+	device_t	     parent_dev;
+	struct xpt_xport     *xport;
+	struct mtx	     eb_mtx;	/* Bus topology mutex. */
+};
+
+struct cam_path {
+	struct cam_periph *periph;
+	struct cam_eb	  *bus;
+	struct cam_et	  *target;
+	struct cam_ed	  *device;
+};
+
+struct cam_ed *		xpt_alloc_device(struct cam_eb *bus,
+					 struct cam_et *target,
+					 lun_id_t lun_id);
+void			xpt_acquire_device(struct cam_ed *device);
+void			xpt_release_device(struct cam_ed *device);
+u_int32_t		xpt_dev_ccbq_resize(struct cam_path *path, int newopenings);
+void			xpt_start_tags(struct cam_path *path);
+void			xpt_stop_tags(struct cam_path *path);
+
+MALLOC_DECLARE(M_CAMXPT);
+
+#endif
diff --git a/rtemsbsd/include/cam/cam_xpt_periph.h b/rtemsbsd/include/cam/cam_xpt_periph.h
index 936ffd8..e20a777 100644
--- a/rtemsbsd/include/cam/cam_xpt_periph.h
+++ b/rtemsbsd/include/cam/cam_xpt_periph.h
@@ -1 +1,259 @@
-/* EMPTY */
+/*-
+ * Data structures and definitions for CAM peripheral ("type") drivers.
+ *
+ * Copyright (c) 1997, 1998 Justin T. Gibbs.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions, and the following disclaimer,
+ *    without modification, immediately at the beginning of the file.
+ * 2. The name of the author may not be used to endorse or promote products
+ *    derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR
+ * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _CAM_CAM_PERIPH_H
+#define _CAM_CAM_PERIPH_H 1
+
+#include <sys/queue.h>
+#include <cam/cam_sim.h>
+
+#ifdef _KERNEL
+#include <sys/taskqueue.h>
+
+#include <cam/cam_xpt.h>
+
+struct devstat;
+
+extern struct cam_periph *xpt_periph;
+
+extern struct periph_driver **periph_drivers;
+void periphdriver_register(void *);
+int periphdriver_unregister(void *);
+void periphdriver_init(int level);
+
+#include <sys/module.h>
+#define PERIPHDRIVER_DECLARE(name, driver) \
+	static int name ## _modevent(module_t mod, int type, void *data) \
+	{ \
+		switch (type) { \
+		case MOD_LOAD: \
+			periphdriver_register(data); \
+			break; \
+		case MOD_UNLOAD: \
+			return (periphdriver_unregister(data)); \
+		default: \
+			return EOPNOTSUPP; \
+		} \
+		return 0; \
+	} \
+	static moduledata_t name ## _mod = { \
+		#name, \
+		name ## _modevent, \
+		(void *)&driver \
+	}; \
+	DECLARE_MODULE(name, name ## _mod, SI_SUB_DRIVERS, SI_ORDER_ANY); \
+	MODULE_DEPEND(name, cam, 1, 1, 1)
+
+/*
+ * Callback informing the peripheral driver it can perform it's
+ * initialization since the XPT is now fully initialized.
+ */
+typedef void (periph_init_t)(void);
+
+/*
+ * Callback requesting the peripheral driver to remove its instances
+ * and shutdown, if possible.
+ */
+typedef int (periph_deinit_t)(void);
+
+struct periph_driver {
+	periph_init_t		*init;
+	char			*driver_name;
+	TAILQ_HEAD(,cam_periph)	 units;
+	u_int			 generation;
+	u_int			 flags;
+#define CAM_PERIPH_DRV_EARLY		0x01
+	periph_deinit_t		*deinit;
+};
+
+typedef enum {
+	CAM_PERIPH_BIO
+} cam_periph_type;
+
+/* Generically useful offsets into the peripheral private area */
+#define ppriv_ptr0 periph_priv.entries[0].ptr
+#define ppriv_ptr1 periph_priv.entries[1].ptr
+#define ppriv_field0 periph_priv.entries[0].field
+#define ppriv_field1 periph_priv.entries[1].field
+
+typedef void		periph_start_t (struct cam_periph *periph,
+					union ccb *start_ccb);
+typedef cam_status	periph_ctor_t (struct cam_periph *periph,
+				       void *arg);
+typedef void		periph_oninv_t (struct cam_periph *periph);
+typedef void		periph_dtor_t (struct cam_periph *periph);
+struct cam_periph {
+	periph_start_t		*periph_start;
+	periph_oninv_t		*periph_oninval;
+	periph_dtor_t		*periph_dtor;
+	char			*periph_name;
+	struct cam_path		*path;	/* Compiled path to device */
+	void			*softc;
+	struct cam_sim		*sim;
+	u_int32_t		 unit_number;
+	cam_periph_type		 type;
+	u_int32_t		 flags;
+#define CAM_PERIPH_RUNNING		0x01
+#define CAM_PERIPH_LOCKED		0x02
+#define CAM_PERIPH_LOCK_WANTED		0x04
+#define CAM_PERIPH_INVALID		0x08
+#define CAM_PERIPH_NEW_DEV_FOUND	0x10
+#define CAM_PERIPH_RECOVERY_INPROG	0x20
+#define CAM_PERIPH_RUN_TASK		0x40
+#define CAM_PERIPH_FREE			0x80
+#define CAM_PERIPH_ANNOUNCED		0x100
+	uint32_t		 scheduled_priority;
+	uint32_t		 immediate_priority;
+	int			 periph_allocating;
+	int			 periph_allocated;
+	u_int32_t		 refcount;
+	SLIST_HEAD(, ccb_hdr)	 ccb_list;	/* For "immediate" requests */
+	SLIST_ENTRY(cam_periph)  periph_links;
+	TAILQ_ENTRY(cam_periph)  unit_links;
+	ac_callback_t		*deferred_callback; 
+	ac_code			 deferred_ac;
+	struct task		 periph_run_task;
+};
+
+#define CAM_PERIPH_MAXMAPS	2
+
+struct cam_periph_map_info {
+	int		num_bufs_used;
+	struct buf	*bp[CAM_PERIPH_MAXMAPS];
+};
+
+cam_status cam_periph_alloc(periph_ctor_t *periph_ctor,
+			    periph_oninv_t *periph_oninvalidate,
+			    periph_dtor_t *periph_dtor,
+			    periph_start_t *periph_start,
+			    char *name, cam_periph_type type, struct cam_path *,
+			    ac_callback_t *, ac_code, void *arg);
+struct cam_periph *cam_periph_find(struct cam_path *path, char *name);
+int		cam_periph_list(struct cam_path *, struct sbuf *);
+cam_status	cam_periph_acquire(struct cam_periph *periph);
+void		cam_periph_doacquire(struct cam_periph *periph);
+void		cam_periph_release(struct cam_periph *periph);
+void		cam_periph_release_locked(struct cam_periph *periph);
+void		cam_periph_release_locked_buses(struct cam_periph *periph);
+int		cam_periph_hold(struct cam_periph *periph, int priority);
+void		cam_periph_unhold(struct cam_periph *periph);
+void		cam_periph_invalidate(struct cam_periph *periph);
+int		cam_periph_mapmem(union ccb *ccb,
+				  struct cam_periph_map_info *mapinfo,
+				  u_int maxmap);
+void		cam_periph_unmapmem(union ccb *ccb,
+				    struct cam_periph_map_info *mapinfo);
+union ccb	*cam_periph_getccb(struct cam_periph *periph,
+				   u_int32_t priority);
+int		cam_periph_runccb(union ccb *ccb,
+				  int (*error_routine)(union ccb *ccb,
+						       cam_flags camflags,
+						       u_int32_t sense_flags),
+				  cam_flags camflags, u_int32_t sense_flags,
+				  struct devstat *ds);
+int		cam_periph_ioctl(struct cam_periph *periph, u_long cmd, 
+				 caddr_t addr,
+				 int (*error_routine)(union ccb *ccb,
+						      cam_flags camflags,
+						      u_int32_t sense_flags));
+void		cam_freeze_devq(struct cam_path *path);
+u_int32_t	cam_release_devq(struct cam_path *path, u_int32_t relsim_flags,
+				 u_int32_t opening_reduction, u_int32_t arg,
+				 int getcount_only);
+void		cam_periph_async(struct cam_periph *periph, u_int32_t code,
+		 		 struct cam_path *path, void *arg);
+void		cam_periph_bus_settle(struct cam_periph *periph,
+				      u_int bus_settle_ms);
+void		cam_periph_freeze_after_event(struct cam_periph *periph,
+					      struct timeval* event_time,
+					      u_int duration_ms);
+int		cam_periph_error(union ccb *ccb, cam_flags camflags,
+				 u_int32_t sense_flags);
+
+static __inline struct mtx *
+cam_periph_mtx(struct cam_periph *periph)
+{
+	return (xpt_path_mtx(periph->path));
+}
+
+#define cam_periph_owned(periph)					\
+	mtx_owned(xpt_path_mtx((periph)->path))
+
+#define cam_periph_lock(periph)						\
+	mtx_lock(xpt_path_mtx((periph)->path))
+
+#define cam_periph_unlock(periph)					\
+	mtx_unlock(xpt_path_mtx((periph)->path))
+
+#define cam_periph_assert(periph, what)					\
+	mtx_assert(xpt_path_mtx((periph)->path), (what))
+
+#define cam_periph_sleep(periph, chan, priority, wmesg, timo)		\
+	xpt_path_sleep((periph)->path, (chan), (priority), (wmesg), (timo))
+
+static inline struct cam_periph *
+cam_periph_acquire_first(struct periph_driver *driver)
+{
+	struct cam_periph *periph;
+
+	xpt_lock_buses();
+	periph = TAILQ_FIRST(&driver->units);
+	while (periph != NULL && (periph->flags & CAM_PERIPH_INVALID) != 0)
+		periph = TAILQ_NEXT(periph, unit_links);
+	if (periph != NULL)
+		periph->refcount++;
+	xpt_unlock_buses();
+	return (periph);
+}
+
+static inline struct cam_periph *
+cam_periph_acquire_next(struct cam_periph *pperiph)
+{
+	struct cam_periph *periph = pperiph;
+
+	cam_periph_assert(pperiph, MA_NOTOWNED);
+	xpt_lock_buses();
+	do {
+		periph = TAILQ_NEXT(periph, unit_links);
+	} while (periph != NULL && (periph->flags & CAM_PERIPH_INVALID) != 0);
+	if (periph != NULL)
+		periph->refcount++;
+	xpt_unlock_buses();
+	cam_periph_release(pperiph);
+	return (periph);
+}
+
+#define CAM_PERIPH_FOREACH(periph, driver)				\
+	for ((periph) = cam_periph_acquire_first(driver);		\
+	    (periph) != NULL;						\
+	    (periph) = cam_periph_acquire_next(periph))
+
+#endif /* _KERNEL */
+#endif /* _CAM_CAM_PERIPH_H */
diff --git a/rtemsbsd/include/machine/rtems-bsd-kernel-namespace.h b/rtemsbsd/include/machine/rtems-bsd-kernel-namespace.h
index 0a9c6bb..115d4f8 100644
--- a/rtemsbsd/include/machine/rtems-bsd-kernel-namespace.h
+++ b/rtemsbsd/include/machine/rtems-bsd-kernel-namespace.h
@@ -5029,9 +5029,6 @@
 #define	X_ip6_mrouter_get _bsd_X_ip6_mrouter_get
 #define	X_ip6_mrouter_set _bsd_X_ip6_mrouter_set
 #define	X_mrt6_ioctl _bsd_X_mrt6_ioctl
-#define	xpt_bus_deregister _bsd_xpt_bus_deregister
-#define	xpt_bus_register _bsd_xpt_bus_register
-#define	xpt_done _bsd_xpt_done
 #define	z_alloc _bsd_z_alloc
 #define	zd1211b_firmware _bsd_zd1211b_firmware
 #define	zd1211_firmware _bsd_zd1211_firmware
diff --git a/rtemsbsd/include/rtems/bsd/local/opt_cam.h b/rtemsbsd/include/rtems/bsd/local/opt_cam.h
index e69de29..3d6ff47 100644
--- a/rtemsbsd/include/rtems/bsd/local/opt_cam.h
+++ b/rtemsbsd/include/rtems/bsd/local/opt_cam.h
@@ -0,0 +1,6 @@
+#define CAMDEBUG 1
+#define CAM_DEBUG_FLAGS (CAM_DEBUG_INFO|CAM_DEBUG_PROBE|CAM_DEBUG_PERIPH)
+#include <rtems/bsd/modules.h>
+#ifdef RTEMS_BSD_MODULE_MMCCAM
+	#define MMCCAM 1
+#endif
diff --git a/rtemsbsd/include/rtems/bsd/local/opt_mmccam.h b/rtemsbsd/include/rtems/bsd/local/opt_mmccam.h
new file mode 100644
index 0000000..324451f
--- /dev/null
+++ b/rtemsbsd/include/rtems/bsd/local/opt_mmccam.h
@@ -0,0 +1,4 @@
+#include <rtems/bsd/modules.h>
+#ifdef RTEMS_BSD_MODULE_MMCCAM
+	#define MMCCAM 1
+#endif
diff --git a/rtemsbsd/rtems/rtems-kernel-cam.c b/rtemsbsd/rtems/rtems-kernel-cam.c
index b2abff3..ba0dcdf 100644
--- a/rtemsbsd/rtems/rtems-kernel-cam.c
+++ b/rtemsbsd/rtems/rtems-kernel-cam.c
@@ -72,7 +72,10 @@
 
 #define BSD_SCSI_MIN_COMMAND_SIZE 10
 
+#define CAM_PATH_ANY (u_int32_t)-1
 MALLOC_DEFINE(M_CAMSIM, "CAM SIM", "CAM SIM buffers");
+static struct mtx cam_sim_free_mtx;
+MTX_SYSINIT(cam_sim_free_init, &cam_sim_free_mtx, "CAM SIM free lock", MTX_DEF);
 
 static void
 rtems_bsd_sim_set_state(struct cam_sim *sim, enum bsd_sim_state state)
@@ -409,6 +412,8 @@ cam_sim_alloc(
 	struct cam_devq *queue
 )
 {
+    printf("**cam_sim_alloc\n");
+#ifndef MMCCAM
 	rtems_status_code sc = RTEMS_SUCCESSFUL;
 	struct cam_sim *sim = NULL;
 
@@ -435,11 +440,44 @@ cam_sim_alloc(
 	BSD_ASSERT_SC(sc);
 
 	return sim;
+#else
+    struct cam_sim *sim;
+
+	sim = (struct cam_sim *)malloc(sizeof(struct cam_sim),
+	    M_CAMSIM, M_ZERO | M_NOWAIT);
+
+	if (sim == NULL)
+		return (NULL);
+
+	sim->sim_action = sim_action;
+	sim->sim_poll = sim_poll;
+	sim->sim_name = sim_name;
+	sim->softc = softc;
+	sim->path_id = CAM_PATH_ANY;
+	sim->unit_number = unit;
+	sim->bus_id = 0;	/* set in xpt_bus_register */
+	sim->max_tagged_dev_openings = max_tagged_dev_transactions;
+	sim->max_dev_openings = max_dev_transactions;
+	sim->flags = 0;
+	sim->refcount = 1;
+	sim->devq = queue;
+	sim->mtx = mtx;
+	if (mtx == &Giant) {
+		sim->flags |= 0;
+		callout_init(&sim->callout, 0);
+	} else {
+		sim->flags |= CAM_SIM_MPSAFE;
+		callout_init(&sim->callout, 1);
+	}
+    return (sim);
+#endif
 }
 
 void
 cam_sim_free(struct cam_sim *sim, int free_devq)
 {
+    printf("**cam_sim_free\n");
+#ifndef MMCCAM
 	rtems_status_code sc = RTEMS_SUCCESSFUL;
 
 	/*
@@ -461,26 +499,61 @@ cam_sim_free(struct cam_sim *sim, int free_devq)
 
 	cv_destroy(&sim->state_changed);
 	free(sim, M_CAMSIM);
+#else
+    struct mtx *mtx = sim->mtx;
+	int error;
+
+	if (mtx) {
+		mtx_assert(mtx, MA_OWNED);
+	} else {
+		mtx = &cam_sim_free_mtx;
+		mtx_lock(mtx);
+	}
+	sim->refcount--;
+	if (sim->refcount > 0) {
+		error = msleep(sim, mtx, PRIBIO, "simfree", 0);
+		KASSERT(error == 0, ("invalid error value for msleep(9)"));
+	}
+	KASSERT(sim->refcount == 0, ("sim->refcount == 0"));
+	if (sim->mtx == NULL)
+		mtx_unlock(mtx);
+
+	if (free_devq)
+		cam_simq_free(sim->devq);
+    free(sim, M_CAMSIM);
+#endif
 }
 
 struct cam_devq *
 cam_simq_alloc(u_int32_t max_sim_transactions)
 {
+printf("**cam_simq_alloc\n");
+#ifndef MMCCAM
 	return BSD_CAM_DEVQ_DUMMY;
+#else
+    return (cam_devq_alloc(/*size*/0, max_sim_transactions));
+#endif
 }
 
 void
 cam_simq_free(struct cam_devq *devq)
 {
+printf("**cam_simq_free\n");
+#ifndef MMCCAM
 	BSD_ASSERT(devq == BSD_CAM_DEVQ_DUMMY);
+#else
+    cam_devq_free(devq);
+#endif
 }
-
+#ifndef __rtems__
 void
 xpt_done(union ccb *done_ccb)
 {
 	(*done_ccb->ccb_h.cbfcnp)(NULL, done_ccb);
 }
+#endif
 
+#ifndef __rtems__
 int32_t
 xpt_bus_register(struct cam_sim *sim, device_t parent, u_int32_t bus)
 {
@@ -491,10 +564,12 @@ xpt_bus_register(struct cam_sim *sim, device_t parent, u_int32_t bus)
 
 	return CAM_SUCCESS;
 }
+#endif
 
 int32_t
 xpt_bus_deregister(path_id_t pathid)
 {
+    printf("**xpt_bus_deregister\n");
 	/*
 	 * We ignore this bus stuff completely.  This is easier than removing
 	 * the calls from "umass.c".
@@ -502,3 +577,45 @@ xpt_bus_deregister(path_id_t pathid)
 
 	return CAM_REQ_CMP;
 }
+
+void
+cam_sim_hold(struct cam_sim *sim)
+{
+	struct mtx *mtx = sim->mtx;
+    printf("**cam_sim_hold\n");
+	if (mtx) {
+		if (!mtx_owned(mtx))
+			mtx_lock(mtx);
+		else
+			mtx = NULL;
+	} else {
+		mtx = &cam_sim_free_mtx;
+		mtx_lock(mtx);
+	}
+	KASSERT(sim->refcount >= 1, ("sim->refcount >= 1"));
+	sim->refcount++;
+	if (mtx)
+		mtx_unlock(mtx);
+}
+
+void
+cam_sim_release(struct cam_sim *sim)
+{
+	struct mtx *mtx = sim->mtx;
+    printf("**cam_sim_release\n");
+	if (mtx) {
+		if (!mtx_owned(mtx))
+			mtx_lock(mtx);
+		else
+			mtx = NULL;
+	} else {
+		mtx = &cam_sim_free_mtx;
+		mtx_lock(mtx);
+	}
+	KASSERT(sim->refcount >= 1, ("sim->refcount >= 1"));
+	sim->refcount--;
+	if (sim->refcount == 0)
+		wakeup(sim);
+	if (mtx)
+		mtx_unlock(mtx);
+}
-- 
1.9.1

